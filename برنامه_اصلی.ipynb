{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahdyabdnia/kalabama/blob/master/%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87_%D8%A7%D8%B5%D9%84%DB%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# آماده‌سازی داده‌ها\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "# توابع انتخاب ویژگی\n",
        "def select_features_by_rfecv(data, target_name='is_long_parameters_list', estimator=DecisionTreeClassifier(), n_features_to_select=None):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    if n_features_to_select:\n",
        "        selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    else:\n",
        "        selector = RFECV(estimator, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', estimator=DecisionTreeClassifier(), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select=n_features_to_select, direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', estimator=DecisionTreeClassifier(), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01, n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    # انتخاب n ویژگی برتر\n",
        "    top_indices = np.argsort(best_solution)[-n_features_to_select:]\n",
        "    best_solution = np.zeros_like(best_solution)\n",
        "    best_solution[top_indices] = 1\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "# توابع الگوریتم ژنتیک\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "# تابع ارزیابی دقت\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "# تابع برای محاسبه اهمیت ویژگی‌ها\n",
        "def calculate_feature_importance(X, y):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X, y)\n",
        "    return model.feature_importances_\n",
        "\n",
        "# تابع برای انتخاب بهترین n ویژگی بر اساس اهمیت\n",
        "def select_top_n_features(feature_importance, feature_names, n):\n",
        "    top_indices = np.argsort(feature_importance)[-n:]\n",
        "    selected_features = [feature_names[i] for i in top_indices]\n",
        "    return selected_features\n",
        "\n",
        "# تابع جدید برای انتخاب ویژگی‌ها بر اساس اهمیت\n",
        "def select_features_by_importance(data, target_name='is_long_parameters_list', n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    feature_importance = calculate_feature_importance(X, y)\n",
        "    selected_features = select_top_n_features(feature_importance, X.columns, n_features_to_select)\n",
        "    return {feature: feature in selected_features for feature in X.columns}\n",
        "\n",
        "# توابع برای اجتماع، اشتراک و رأی‌گیری حداکثری\n",
        "def union_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update({feature for feature, selected in results.items() if selected})\n",
        "    return list(all_features)\n",
        "\n",
        "def intersection_features(results_dicts):\n",
        "    intersection_features = set(results_dicts[0].keys())\n",
        "    for results in results_dicts:\n",
        "        intersection_features.intersection_update({feature for feature, selected in results.items() if selected})\n",
        "    return list(intersection_features)\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    feature_count = {}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] = feature_count.get(feature, 0) + 1\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "# تابع اصلی برای کنترل تعداد ویژگی‌ها و بررسی دقت\n",
        "def evaluate_feature_selection(data, target_name='is_long_parameters_list', feature_counts=[5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]):\n",
        "    results = {}\n",
        "    methods = {\n",
        "        \"RFECV\": select_features_by_rfecv,\n",
        "        \"SFS\": select_features_by_sfs,\n",
        "        \"RFE\": select_features_by_rfe,\n",
        "        \"Genetic\": select_features_by_custom_genetic,\n",
        "        \"Importance\": select_features_by_importance  # اضافه کردن روش جدید\n",
        "    }\n",
        "\n",
        "    for method_name, method_func in methods.items():\n",
        "        print(f\"\\nارزیابی روش: {method_name}\")\n",
        "        best_accuracy = 0\n",
        "        best_feature_count = 0\n",
        "        best_features_dict = None\n",
        "\n",
        "        for n_features in feature_counts:\n",
        "            print(f\"تعداد ویژگی‌ها: {n_features}\")\n",
        "            selected_features_dict = method_func(data, target_name=target_name, n_features_to_select=n_features)\n",
        "            selected_features = [feature for feature, selected in selected_features_dict.items() if selected]\n",
        "\n",
        "            # محاسبه دقت\n",
        "            solution = np.array([1 if feature in selected_features else 0 for feature in data.drop(target_name, axis=1).columns])\n",
        "            accuracy = fitness(solution, data.drop(target_name, axis=1).values, data[target_name].values)\n",
        "            print(f\"دقت: {accuracy}\")\n",
        "\n",
        "            # به‌روزرسانی بهترین دقت و تعداد ویژگی‌ها\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_feature_count = n_features\n",
        "                best_features_dict = selected_features_dict\n",
        "\n",
        "        results[method_name] = {\n",
        "            \"best_accuracy\": best_accuracy,\n",
        "            \"best_feature_count\": best_feature_count,\n",
        "            \"best_features_dict\": best_features_dict\n",
        "        }\n",
        "\n",
        "    # محاسبه اجتماع، اشتراک و رأی‌گیری حداکثری\n",
        "    print(\"\\nمحاسبه اجتماع، اشتراک و رأی‌گیری حداکثری:\")\n",
        "    all_results_dicts = [results[method][\"best_features_dict\"] for method in results]\n",
        "    union = union_features(all_results_dicts)\n",
        "    intersection = intersection_features(all_results_dicts)\n",
        "    majority = majority_voting_features(all_results_dicts)\n",
        "\n",
        "    print(f\"اجتماع: {union}\")\n",
        "    print(f\"اشتراک: {intersection}\")\n",
        "    print(f\"رأی‌گیری حداکثری: {majority}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# اجرای تابع اصلی\n",
        "results = evaluate_feature_selection(data, target_name='is_long_parameters_list')\n",
        "\n",
        "# چاپ نتایج\n",
        "for method_name, result in results.items():\n",
        "    print(f\"\\nنتایج برای روش {method_name}:\")\n",
        "    print(f\"بهترین تعداد ویژگی‌ها: {result['best_feature_count']}\")\n",
        "    print(f\"بهترین دقت: {result['best_accuracy']}\")\n",
        "    print(f\"ویژگی‌های انتخاب‌شده: {[feature for feature, selected in result['best_features_dict'].items() if selected]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTzgV7uyp3g2",
        "outputId": "a1524e4c-282d-4e8a-cb72-fbafac2683d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "ارزیابی روش: RFECV\n",
            "تعداد ویژگی‌ها: 5\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 11\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 12\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 13\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 14\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 16\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 17\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 18\n",
            "دقت: 0.9047619047619048\n",
            "تعداد ویژگی‌ها: 19\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 21\n",
            "دقت: 0.9047619047619048\n",
            "تعداد ویژگی‌ها: 22\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 23\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 24\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.8809523809523809\n",
            "تعداد ویژگی‌ها: 26\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 27\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 28\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 29\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 30\n",
            "دقت: 0.9206349206349206\n",
            "\n",
            "ارزیابی روش: SFS\n",
            "تعداد ویژگی‌ها: 5\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 11\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 12\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 13\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 14\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 16\n",
            "دقت: 0.8809523809523809\n",
            "تعداد ویژگی‌ها: 17\n",
            "دقت: 0.8809523809523809\n",
            "تعداد ویژگی‌ها: 18\n",
            "دقت: 0.8650793650793651\n",
            "تعداد ویژگی‌ها: 19\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.8809523809523809\n",
            "تعداد ویژگی‌ها: 21\n",
            "دقت: 0.8650793650793651\n",
            "تعداد ویژگی‌ها: 22\n",
            "دقت: 0.873015873015873\n",
            "تعداد ویژگی‌ها: 23\n",
            "دقت: 0.873015873015873\n",
            "تعداد ویژگی‌ها: 24\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.8571428571428571\n",
            "تعداد ویژگی‌ها: 26\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 27\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 28\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 29\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 30\n",
            "دقت: 0.9126984126984127\n",
            "\n",
            "ارزیابی روش: RFE\n",
            "تعداد ویژگی‌ها: 5\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 11\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 12\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 13\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 14\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 16\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 17\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 18\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 19\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 21\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 22\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 23\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 24\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.9047619047619048\n",
            "تعداد ویژگی‌ها: 26\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 27\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 28\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 29\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 30\n",
            "دقت: 0.9047619047619048\n",
            "\n",
            "ارزیابی روش: Genetic\n",
            "تعداد ویژگی‌ها: 5\n",
            "دقت: 0.6904761904761905\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.6984126984126984\n",
            "تعداد ویژگی‌ها: 11\n",
            "دقت: 0.7380952380952381\n",
            "تعداد ویژگی‌ها: 12\n",
            "دقت: 0.6746031746031746\n",
            "تعداد ویژگی‌ها: 13\n",
            "دقت: 0.6587301587301587\n",
            "تعداد ویژگی‌ها: 14\n",
            "دقت: 0.6746031746031746\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.7063492063492064\n",
            "تعداد ویژگی‌ها: 16\n",
            "دقت: 0.7936507936507936\n",
            "تعداد ویژگی‌ها: 17\n",
            "دقت: 0.8174603174603174\n",
            "تعداد ویژگی‌ها: 18\n",
            "دقت: 0.7857142857142857\n",
            "تعداد ویژگی‌ها: 19\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 21\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 22\n",
            "دقت: 0.9047619047619048\n",
            "تعداد ویژگی‌ها: 23\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 24\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 26\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 27\n",
            "دقت: 0.7936507936507936\n",
            "تعداد ویژگی‌ها: 28\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 29\n",
            "دقت: 0.9444444444444444\n",
            "تعداد ویژگی‌ها: 30\n",
            "دقت: 0.9365079365079365\n",
            "\n",
            "ارزیابی روش: Importance\n",
            "تعداد ویژگی‌ها: 5\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.8809523809523809\n",
            "تعداد ویژگی‌ها: 11\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 12\n",
            "دقت: 0.9047619047619048\n",
            "تعداد ویژگی‌ها: 13\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 14\n",
            "دقت: 0.8968253968253969\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 16\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 17\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 18\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 19\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 21\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 22\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 23\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 24\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 26\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 27\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 28\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 29\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 30\n",
            "دقت: 0.9206349206349206\n",
            "\n",
            "محاسبه اجتماع، اشتراک و رأی‌گیری حداکثری:\n",
            "اجتماع: ['NOAV_method', 'CM_method', 'ATLD_method', 'MaMCL_method', 'FDP_method', 'LOC_package', 'CBO_type', 'WOC_type', 'CC_method', 'NOMNAMM_type', 'NOC_type', 'NOAM_type', 'WMCNAMM_type', 'LOC_project', 'ATFD_method', 'NOPA_type', 'NMCS_method', 'LCOM5_type', 'NOI_package', 'NOP_method', 'NOLV_method', 'NOM_package', 'LOCNAMM_type', 'FANOUT_type', 'NOMNAMM_project', 'NOM_project', 'ATFD_type', 'NMO_type', 'NOM_type', 'CYCLO_method', 'NOA_type', 'NOI_project', 'NOII_type', 'NOCS_package', 'NOCS_project', 'MeMCL_method']\n",
            "اشتراک: ['NOP_method']\n",
            "رأی‌گیری حداکثری: ['NOP_method', 'CYCLO_method', 'NOAM_type', 'NOPA_type', 'CBO_type', 'LCOM5_type', 'WOC_type', 'NOCS_package', 'LOC_package', 'NOM_package', 'NOM_project', 'NOMNAMM_project', 'LOC_project', 'MaMCL_method', 'MeMCL_method', 'NOCS_project']\n",
            "\n",
            "نتایج برای روش RFECV:\n",
            "بهترین تعداد ویژگی‌ها: 13\n",
            "بهترین دقت: 0.9444444444444444\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'CYCLO_method', 'NOAM_type', 'NOPA_type', 'CBO_type', 'LCOM5_type', 'WOC_type', 'NOCS_package', 'LOC_package', 'NOM_package', 'NOM_project', 'NOMNAMM_project', 'LOC_project']\n",
            "\n",
            "نتایج برای روش SFS:\n",
            "بهترین تعداد ویژگی‌ها: 10\n",
            "بهترین دقت: 0.9365079365079365\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'CC_method', 'CM_method', 'CYCLO_method', 'NMCS_method', 'NOLV_method', 'MaMCL_method', 'ATLD_method', 'MeMCL_method', 'NOC_type']\n",
            "\n",
            "نتایج برای روش RFE:\n",
            "بهترین تعداد ویژگی‌ها: 10\n",
            "بهترین دقت: 0.9444444444444444\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'CYCLO_method', 'NOAM_type', 'NOPA_type', 'CBO_type', 'LCOM5_type', 'WOC_type', 'NOM_package', 'NOMNAMM_project', 'LOC_project']\n",
            "\n",
            "نتایج برای روش Genetic:\n",
            "بهترین تعداد ویژگی‌ها: 20\n",
            "بهترین دقت: 0.9444444444444444\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'ATFD_method', 'FDP_method', 'MaMCL_method', 'NOAV_method', 'MeMCL_method', 'NOII_type', 'NOM_type', 'NMO_type', 'ATFD_type', 'FANOUT_type', 'NOMNAMM_type', 'CBO_type', 'WOC_type', 'NOI_package', 'LOC_package', 'NOCS_project', 'NOI_project', 'NOM_project', 'LOC_project']\n",
            "\n",
            "نتایج برای روش Importance:\n",
            "بهترین تعداد ویژگی‌ها: 11\n",
            "بهترین دقت: 0.9365079365079365\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'CYCLO_method', 'NOA_type', 'LOCNAMM_type', 'CBO_type', 'WOC_type', 'WMCNAMM_type', 'NOCS_package', 'NOM_package', 'NOCS_project', 'NOMNAMM_project']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/SwitchStatements.csv')\n",
        "\n",
        "# آماده‌سازی داده‌ها\n",
        "X = data.drop('is_switch_statements', axis=1)\n",
        "y = data['is_switch_statements']\n",
        "\n",
        "# توابع انتخاب ویژگی\n",
        "def select_features_by_rfecv(data, target_name='is_switch_statements', estimator=DecisionTreeClassifier(), n_features_to_select=None):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    if n_features_to_select:\n",
        "        selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    else:\n",
        "        selector = RFECV(estimator, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_switch_statements', estimator=DecisionTreeClassifier(), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select=n_features_to_select, direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_switch_statements', estimator=DecisionTreeClassifier(), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_switch_statements', pop_size=20, n_generations=50, mutation_rate=0.01, n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    # انتخاب n ویژگی برتر\n",
        "    top_indices = np.argsort(best_solution)[-n_features_to_select:]\n",
        "    best_solution = np.zeros_like(best_solution)\n",
        "    best_solution[top_indices] = 1\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "# توابع الگوریتم ژنتیک\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "# تابع ارزیابی دقت\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "# تابع برای محاسبه اهمیت ویژگی‌ها\n",
        "def calculate_feature_importance(X, y):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X, y)\n",
        "    return model.feature_importances_\n",
        "\n",
        "# تابع برای انتخاب بهترین n ویژگی بر اساس اهمیت\n",
        "def select_top_n_features(feature_importance, feature_names, n):\n",
        "    top_indices = np.argsort(feature_importance)[-n:]\n",
        "    selected_features = [feature_names[i] for i in top_indices]\n",
        "    return selected_features\n",
        "\n",
        "# تابع جدید برای انتخاب ویژگی‌ها بر اساس اهمیت\n",
        "def select_features_by_importance(data, target_name='is_switch_statements', n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    feature_importance = calculate_feature_importance(X, y)\n",
        "    selected_features = select_top_n_features(feature_importance, X.columns, n_features_to_select)\n",
        "    return {feature: feature in selected_features for feature in X.columns}\n",
        "\n",
        "# توابع برای اجتماع، اشتراک و رأی‌گیری حداکثری\n",
        "def union_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update({feature for feature, selected in results.items() if selected})\n",
        "    return list(all_features)\n",
        "\n",
        "def intersection_features(results_dicts):\n",
        "    intersection_features = set(results_dicts[0].keys())\n",
        "    for results in results_dicts:\n",
        "        intersection_features.intersection_update({feature for feature, selected in results.items() if selected})\n",
        "    return list(intersection_features)\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    feature_count = {}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] = feature_count.get(feature, 0) + 1\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "# تابع اصلی برای کنترل تعداد ویژگی‌ها و بررسی دقت\n",
        "def evaluate_feature_selection(data, target_name='is_switch_statements', feature_counts=[10, 15, 20, 25]):\n",
        "    results = {}\n",
        "    methods = {\n",
        "        \"RFECV\": select_features_by_rfecv,\n",
        "        \"SFS\": select_features_by_sfs,\n",
        "        \"RFE\": select_features_by_rfe,\n",
        "        \"Genetic\": select_features_by_custom_genetic,\n",
        "        \"Importance\": select_features_by_importance  # اضافه کردن روش جدید\n",
        "    }\n",
        "\n",
        "    for method_name, method_func in methods.items():\n",
        "        print(f\"\\nارزیابی روش: {method_name}\")\n",
        "        best_accuracy = 0\n",
        "        best_feature_count = 0\n",
        "        best_features_dict = None\n",
        "\n",
        "        for n_features in feature_counts:\n",
        "            print(f\"تعداد ویژگی‌ها: {n_features}\")\n",
        "            selected_features_dict = method_func(data, target_name=target_name, n_features_to_select=n_features)\n",
        "            selected_features = [feature for feature, selected in selected_features_dict.items() if selected]\n",
        "\n",
        "            # محاسبه دقت\n",
        "            solution = np.array([1 if feature in selected_features else 0 for feature in data.drop(target_name, axis=1).columns])\n",
        "            accuracy = fitness(solution, data.drop(target_name, axis=1).values, data[target_name].values)\n",
        "            print(f\"دقت: {accuracy}\")\n",
        "\n",
        "            # بررسی اگر دقت کاهش یافت\n",
        "            if accuracy < best_accuracy:\n",
        "                print(f\"دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\")\n",
        "                break\n",
        "\n",
        "            # به‌روزرسانی بهترین دقت و تعداد ویژگی‌ها\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_feature_count = n_features\n",
        "                best_features_dict = selected_features_dict\n",
        "\n",
        "        results[method_name] = {\n",
        "            \"best_accuracy\": best_accuracy,\n",
        "            \"best_feature_count\": best_feature_count,\n",
        "            \"best_features_dict\": best_features_dict\n",
        "        }\n",
        "\n",
        "    # محاسبه اجتماع، اشتراک و رأی‌گیری حداکثری\n",
        "    print(\"\\nمحاسبه اجتماع، اشتراک و رأی‌گیری حداکثری:\")\n",
        "    all_results_dicts = [results[method][\"best_features_dict\"] for method in results]\n",
        "    union = union_features(all_results_dicts)\n",
        "    intersection = intersection_features(all_results_dicts)\n",
        "    majority = majority_voting_features(all_results_dicts)\n",
        "\n",
        "    print(f\"اجتماع: {union}\")\n",
        "    print(f\"اشتراک: {intersection}\")\n",
        "    print(f\"رأی‌گیری حداکثری: {majority}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# اجرای تابع اصلی\n",
        "results = evaluate_feature_selection(data, target_name='is_switch_statements')\n",
        "\n",
        "# چاپ نتایج\n",
        "for method_name, result in results.items():\n",
        "    print(f\"\\nنتایج برای روش {method_name}:\")\n",
        "    print(f\"بهترین تعداد ویژگی‌ها: {result['best_feature_count']}\")\n",
        "    print(f\"بهترین دقت: {result['best_accuracy']}\")\n",
        "    print(f\"ویژگی‌های انتخاب‌شده: {[feature for feature, selected in result['best_features_dict'].items() if selected]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hNznEW5oIO4",
        "outputId": "ea8a0d37-f1aa-400b-cd56-fbb77ed4d714"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "ارزیابی روش: RFECV\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.8968253968253969\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: SFS\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.8650793650793651\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.873015873015873\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.8571428571428571\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: RFE\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.9206349206349206\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.8888888888888888\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: Genetic\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.7857142857142857\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9365079365079365\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.9126984126984127\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: Importance\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9047619047619048\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.8650793650793651\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "محاسبه اجتماع، اشتراک و رأی‌گیری حداکثری:\n",
            "اجتماع: ['NOAV_method', 'FANOUT_method', 'CM_method', 'ATLD_method', 'FDP_method', 'MaMCL_method', 'AMWNAMM_type', 'NOMNAMM_package', 'LOC_package', 'CBO_type', 'NOC_type', 'NOAM_type', 'WMCNAMM_type', 'LOC_project', 'WMC_type', 'DIT_type', 'LAA_method', 'NMCS_method', 'LCOM5_type', 'NOP_method', 'NOCS_type', 'NOLV_method', 'NOM_package', 'LOCNAMM_type', 'TCC_type', 'CINT_method', 'LOC_method', 'NOMNAMM_project', 'RFC_type', 'CFNAMM_type', 'NOM_type', 'CYCLO_method', 'CDISP_method', 'AMW_type', 'CFNAMM_method', 'MAXNESTING_method', 'NOCS_package', 'NOPK_project', 'MeMCL_method']\n",
            "اشتراک: ['NOP_method', 'CYCLO_method']\n",
            "رأی‌گیری حداکثری: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'TCC_type', 'CBO_type', 'RFC_type', 'AMWNAMM_type', 'NOCS_package', 'NOPK_project', 'FDP_method', 'NMCS_method', 'MaMCL_method', 'LAA_method', 'CFNAMM_method', 'ATLD_method', 'CDISP_method', 'NOLV_method', 'NOM_type', 'LOCNAMM_type', 'NOC_type', 'LCOM5_type']\n",
            "\n",
            "نتایج برای روش RFECV:\n",
            "بهترین تعداد ویژگی‌ها: 10\n",
            "بهترین دقت: 0.9126984126984127\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'TCC_type', 'CBO_type', 'RFC_type', 'AMWNAMM_type', 'NOCS_package', 'NOPK_project']\n",
            "\n",
            "نتایج برای روش SFS:\n",
            "بهترین تعداد ویژگی‌ها: 15\n",
            "بهترین دقت: 0.873015873015873\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'FDP_method', 'CYCLO_method', 'NMCS_method', 'MaMCL_method', 'LAA_method', 'FANOUT_method', 'CFNAMM_method', 'ATLD_method', 'CINT_method', 'MeMCL_method', 'CDISP_method', 'NOAM_type', 'NOCS_type', 'DIT_type']\n",
            "\n",
            "نتایج برای روش RFE:\n",
            "بهترین تعداد ویژگی‌ها: 20\n",
            "بهترین دقت: 0.9285714285714286\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'NMCS_method', 'NOLV_method', 'NOM_type', 'LOCNAMM_type', 'CFNAMM_type', 'TCC_type', 'CBO_type', 'RFC_type', 'NOC_type', 'WMC_type', 'LCOM5_type', 'AMWNAMM_type', 'NOCS_package', 'NOMNAMM_package', 'NOM_package', 'NOPK_project']\n",
            "\n",
            "نتایج برای روش Genetic:\n",
            "بهترین تعداد ویژگی‌ها: 20\n",
            "بهترین دقت: 0.9365079365079365\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'FDP_method', 'CM_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'CFNAMM_method', 'ATLD_method', 'NOM_type', 'LOCNAMM_type', 'TCC_type', 'RFC_type', 'NOC_type', 'LCOM5_type', 'NOPK_project', 'NOMNAMM_project', 'LOC_project']\n",
            "\n",
            "نتایج برای روش Importance:\n",
            "بهترین تعداد ویژگی‌ها: 15\n",
            "بهترین دقت: 0.9285714285714286\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'LAA_method', 'CDISP_method', 'NOM_type', 'LOCNAMM_type', 'TCC_type', 'WMCNAMM_type', 'AMW_type', 'AMWNAMM_type', 'NOCS_package', 'LOC_package', 'NOPK_project']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# توابع برای اجتماع، اشتراک و رأی‌گیری حداکثری\n",
        "def union_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update({feature for feature, selected in results.items() if selected})\n",
        "    return list(all_features)\n",
        "\n",
        "def intersection_features(results_dicts):\n",
        "    intersection_features = set(results_dicts[0].keys())\n",
        "    for results in results_dicts:\n",
        "        intersection_features.intersection_update({feature for feature, selected in results.items() if selected})\n",
        "    return list(intersection_features)\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    feature_count = {}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] = feature_count.get(feature, 0) + 1\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "# تابع اصلی برای کنترل تعداد ویژگی‌ها و بررسی دقت\n",
        "def evaluate_feature_selection(data, target_name='is_switch_statements', feature_counts=[10, 15, 20, 25]):\n",
        "    results = {}\n",
        "    methods = {\n",
        "        \"RFECV\": select_features_by_rfecv,\n",
        "        \"SFS\": select_features_by_sfs,\n",
        "        \"RFE\": select_features_by_rfe,\n",
        "        \"Genetic\": select_features_by_custom_genetic\n",
        "    }\n",
        "\n",
        "    for method_name, method_func in methods.items():\n",
        "        print(f\"\\nارزیابی روش: {method_name}\")\n",
        "        best_accuracy = 0\n",
        "        best_feature_count = 0\n",
        "        best_features_dict = None\n",
        "\n",
        "        for n_features in feature_counts:\n",
        "            print(f\"تعداد ویژگی‌ها: {n_features}\")\n",
        "            selected_features_dict = method_func(data, target_name=target_name, n_features_to_select=n_features)\n",
        "            selected_features = [feature for feature, selected in selected_features_dict.items() if selected]\n",
        "\n",
        "            # محاسبه دقت\n",
        "            solution = np.array([1 if feature in selected_features else 0 for feature in data.drop(target_name, axis=1).columns])\n",
        "            accuracy = fitness(solution, data.drop(target_name, axis=1).values, data[target_name].values)\n",
        "            print(f\"دقت: {accuracy}\")\n",
        "\n",
        "            # بررسی اگر دقت کاهش یافت\n",
        "            if accuracy < best_accuracy:\n",
        "                print(f\"دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\")\n",
        "                break\n",
        "\n",
        "            # به‌روزرسانی بهترین دقت و تعداد ویژگی‌ها\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_feature_count = n_features\n",
        "                best_features_dict = selected_features_dict\n",
        "\n",
        "        results[method_name] = {\n",
        "            \"best_accuracy\": best_accuracy,\n",
        "            \"best_feature_count\": best_feature_count,\n",
        "            \"best_features_dict\": best_features_dict\n",
        "        }\n",
        "\n",
        "    # محاسبه اجتماع، اشتراک و رأی‌گیری حداکثری\n",
        "    print(\"\\nمحاسبه اجتماع، اشتراک و رأی‌گیری حداکثری:\")\n",
        "    all_results_dicts = [results[method][\"best_features_dict\"] for method in results]\n",
        "    union = union_features(all_results_dicts)\n",
        "    intersection = intersection_features(all_results_dicts)\n",
        "    majority = majority_voting_features(all_results_dicts)\n",
        "\n",
        "    print(f\"اجتماع: {union}\")\n",
        "    print(f\"اشتراک: {intersection}\")\n",
        "    print(f\"رأی‌گیری حداکثری: {majority}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# اجرای تابع اصلی\n",
        "results = evaluate_feature_selection(data, target_name='is_switch_statements')\n",
        "\n",
        "# چاپ نتایج\n",
        "for method_name, result in results.items():\n",
        "    print(f\"\\nنتایج برای روش {method_name}:\")\n",
        "    print(f\"بهترین تعداد ویژگی‌ها: {result['best_feature_count']}\")\n",
        "    print(f\"بهترین دقت: {result['best_accuracy']}\")\n",
        "    print(f\"ویژگی‌های انتخاب‌شده: {[feature for feature, selected in result['best_features_dict'].items() if selected]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoNZ46-4HITO",
        "outputId": "0bccab42-3399-4254-d1ec-30107678ddb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ارزیابی روش: RFECV\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.9285714285714286\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.9206349206349206\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: SFS\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.873015873015873\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.8650793650793651\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: RFE\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.8888888888888888\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.9126984126984127\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.9047619047619048\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "ارزیابی روش: Genetic\n",
            "تعداد ویژگی‌ها: 10\n",
            "دقت: 0.8095238095238095\n",
            "تعداد ویژگی‌ها: 15\n",
            "دقت: 0.8095238095238095\n",
            "تعداد ویژگی‌ها: 20\n",
            "دقت: 0.8809523809523809\n",
            "تعداد ویژگی‌ها: 25\n",
            "دقت: 0.873015873015873\n",
            "دقت کاهش یافت. متوقف کردن فرآیند برای این روش.\n",
            "\n",
            "محاسبه اجتماع، اشتراک و رأی‌گیری حداکثری:\n",
            "اجتماع: ['NOAV_method', 'FANOUT_method', 'MaMCL_method', 'AMWNAMM_type', 'LOC_type', 'CBO_type', 'CC_method', 'NOC_type', 'NOAM_type', 'WMCNAMM_type', 'WMC_type', 'DIT_type', 'NOPA_type', 'NMCS_method', 'LCOM5_type', 'NOP_method', 'NOCS_type', 'NOLV_method', 'LOCNAMM_type', 'TCC_type', 'CINT_method', 'LOC_method', 'NOMNAMM_project', 'RFC_type', 'NMO_type', 'NOM_type', 'CYCLO_method', 'CDISP_method', 'NOCS_package', 'MAXNESTING_method', 'NOPK_project', 'NIM_type', 'MeMCL_method']\n",
            "اشتراک: []\n",
            "رأی‌گیری حداکثری: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'CBO_type', 'RFC_type', 'WMCNAMM_type', 'AMWNAMM_type', 'NOCS_package', 'NOPK_project', 'CC_method', 'NMCS_method', 'MaMCL_method', 'FANOUT_method', 'MeMCL_method', 'CDISP_method', 'TCC_type', 'NOPA_type']\n",
            "\n",
            "نتایج برای روش RFECV:\n",
            "بهترین تعداد ویژگی‌ها: 10\n",
            "بهترین دقت: 0.9285714285714286\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'CBO_type', 'RFC_type', 'WMCNAMM_type', 'AMWNAMM_type', 'NOCS_package', 'NOPK_project']\n",
            "\n",
            "نتایج برای روش SFS:\n",
            "بهترین تعداد ویژگی‌ها: 10\n",
            "بهترین دقت: 0.873015873015873\n",
            "ویژگی‌های انتخاب‌شده: ['CC_method', 'CYCLO_method', 'NMCS_method', 'MaMCL_method', 'FANOUT_method', 'CINT_method', 'MeMCL_method', 'CDISP_method', 'NMO_type', 'NOC_type']\n",
            "\n",
            "نتایج برای روش RFE:\n",
            "بهترین تعداد ویژگی‌ها: 15\n",
            "بهترین دقت: 0.9126984126984127\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'NMCS_method', 'CDISP_method', 'NOCS_type', 'NOM_type', 'LOCNAMM_type', 'TCC_type', 'NOPA_type', 'CBO_type', 'AMWNAMM_type', 'NOCS_package', 'NOPK_project']\n",
            "\n",
            "نتایج برای روش Genetic:\n",
            "بهترین تعداد ویژگی‌ها: 20\n",
            "بهترین دقت: 0.8809523809523809\n",
            "ویژگی‌های انتخاب‌شده: ['NOP_method', 'CC_method', 'LOC_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'FANOUT_method', 'MeMCL_method', 'NOAM_type', 'NIM_type', 'DIT_type', 'LOC_type', 'TCC_type', 'NOPA_type', 'RFC_type', 'WMC_type', 'LCOM5_type', 'WMCNAMM_type', 'NOPK_project', 'NOMNAMM_project']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_7TeHEmhMrf",
        "outputId": "94f338ff-b44a-4519-9d43-c82cc582a780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ویژگی‌های انتخاب‌شده با روش RFECV: ['NOP_method', 'LCOM5_type']\n",
            "ویژگی‌های انتخاب‌شده با روش SFS: ['NOP_method', 'CC_method', 'ATFD_method', 'CM_method', 'LOC_method', 'CYCLO_method', 'NMCS_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'ATLD_method', 'MeMCL_method', 'CDISP_method', 'NOAM_type', 'NOM_type', 'NMO_type', 'ATFD_type', 'FANOUT_type', 'NOA_type', 'CFNAMM_type', 'NOPA_type', 'CBO_type', 'RFC_type', 'NOC_type', 'WMC_type', 'NOMNAMM_package', 'NOM_package']\n",
            "ویژگی‌های انتخاب‌شده با روش RFE: ['NOP_method', 'MAXNESTING_method', 'MaMCL_method', 'LAA_method', 'ATLD_method', 'MeMCL_method', 'NOII_type', 'NOCS_type', 'TCC_type', 'LCOM5_type']\n",
            "ویژگی‌های انتخاب‌شده با روش الگوریتم ژنتیک: ['NOP_method', 'ATFD_method', 'CM_method', 'MAXNESTING_method', 'NMCS_method', 'NOLV_method', 'MaMCL_method', 'LAA_method', 'FANOUT_method', 'CINT_method', 'CDISP_method', 'NOII_type', 'ATFD_type', 'DIT_type', 'CFNAMM_type', 'TCC_type', 'WMC_type', 'AMWNAMM_type', 'LOC_package', 'NOPK_project', 'NOI_project', 'NOM_project']\n",
            "ویژگی‌های انتخاب‌شده با رای‌گیری حداکثری: ['CFNAMM_type', 'NOII_type', 'ATFD_method', 'MeMCL_method', 'CDISP_method', 'ATFD_type', 'CM_method', 'WMC_type', 'LAA_method', 'LCOM5_type', 'NOP_method', 'MaMCL_method', 'NMCS_method', 'MAXNESTING_method', 'TCC_type', 'ATLD_method', 'NOLV_method']\n",
            "ویژگی‌های اجتماع: ['NIM_type', 'FANOUT_method', 'NOM_type', 'LOC_method', 'NOAV_method', 'CFNAMM_type', 'DIT_type', 'NOII_type', 'WOC_type', 'FANOUT_type', 'LOCNAMM_type', 'ATFD_method', 'CBO_type', 'LOC_type', 'LOC_project', 'MeMCL_method', 'CDISP_method', 'NOPA_type', 'NOI_package', 'AMW_type', 'NOMNAMM_type', 'NOM_project', 'WMCNAMM_type', 'AMWNAMM_type', 'ATFD_type', 'FDP_method', 'CM_method', 'NOAM_type', 'NOCS_project', 'WMC_type', 'CFNAMM_method', 'LAA_method', 'NOMNAMM_project', 'LCOM5_type', 'NOC_type', 'NOP_method', 'MaMCL_method', 'NOCS_package', 'NMCS_method', 'MAXNESTING_method', 'RFC_type', 'CYCLO_method', 'NOA_type', 'TCC_type', 'NOMNAMM_package', 'LOC_package', 'ATLD_method', 'NOPK_project', 'NOCS_type', 'NMO_type', 'NOI_project', 'CINT_method', 'NOM_package', 'CC_method', 'NOLV_method']\n",
            "ویژگی‌های اشتراک: ['NOP_method']\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# آماده‌سازی داده‌ها\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "# انتخاب ویژگی‌ها\n",
        "def select_features_by_rfecv(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "# اجرای روش‌های انتخاب ویژگی\n",
        "rfecv_results = select_features_by_rfecv(data, target_name='is_long_parameters_list')\n",
        "sfs_results = select_features_by_sfs(data, target_name='is_long_parameters_list')\n",
        "rfe_results = select_features_by_rfe(data, target_name='is_long_parameters_list', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data, target_name='is_long_parameters_list')\n",
        "\n",
        "# چاپ ویژگی‌های انتخاب‌شده توسط هر الگوریتم\n",
        "def print_selected_features(method_name, results):\n",
        "    selected_features = [feature for feature, selected in results.items() if selected]\n",
        "    print(f\"ویژگی‌های انتخاب‌شده با روش {method_name}: {selected_features}\")\n",
        "\n",
        "print_selected_features(\"RFECV\", rfecv_results)\n",
        "print_selected_features(\"SFS\", sfs_results)\n",
        "print_selected_features(\"RFE\", rfe_results)\n",
        "print_selected_features(\"الگوریتم ژنتیک\", genetic_results)\n",
        "\n",
        "# ترکیب نتایج\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "\n",
        "# رای‌گیری حداکثری\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "# عملیات اجتماع و اشتراک\n",
        "def union_and_intersection_features(results_dicts):\n",
        "    all_features = set()\n",
        "    intersection_features = set(results_dicts[0].keys())\n",
        "\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "        intersection_features.intersection_update({feature for feature, selected in results.items() if selected})\n",
        "\n",
        "    return list(all_features), list(intersection_features)\n",
        "\n",
        "# چاپ ویژگی‌های انتخاب‌شده\n",
        "selected_features = majority_voting_features(results_dicts)\n",
        "print(f\"ویژگی‌های انتخاب‌شده با رای‌گیری حداکثری: {selected_features}\")\n",
        "\n",
        "# چاپ اجتماع و اشتراک\n",
        "union_features, intersection_features = union_and_intersection_features(results_dicts)\n",
        "print(f\"ویژگی‌های اجتماع: {union_features}\")\n",
        "print(f\"ویژگی‌های اشتراک: {intersection_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0FqnRriqKm2",
        "outputId": "58f1947e-bc59-483e-f212-4eaab6fe26a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9690, F1=0.9538, AUC=0.9679\n",
            "SVM: Accuracy=0.9667, F1=0.9475, AUC=0.9954\n",
            "Logistic Regression: Accuracy=0.9643, F1=0.9432, AUC=0.9957\n",
            "Random Forest: Accuracy=0.9762, F1=0.9648, AUC=0.9974\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9175, AUC=0.9936\n",
            "Gaussian Process: Accuracy=0.9690, F1=0.9534, AUC=0.9926\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9571, AUC=0.9959\n",
            "----------------------------------------\n",
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.9619, F1=0.9437, AUC=0.9607\n",
            "SVM: Accuracy=0.9643, F1=0.9438, AUC=0.9954\n",
            "Logistic Regression: Accuracy=0.9643, F1=0.9432, AUC=0.9957\n",
            "Random Forest: Accuracy=0.9738, F1=0.9613, AUC=0.9982\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9186, AUC=0.9941\n",
            "Gaussian Process: Accuracy=0.9714, F1=0.9569, AUC=0.9934\n",
            "Stacking Classifier: Accuracy=0.9762, F1=0.9637, AUC=0.9911\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9476, F1=0.9211, AUC=0.9411\n",
            "SVM: Accuracy=0.9690, F1=0.9515, AUC=0.9949\n",
            "Logistic Regression: Accuracy=0.9619, F1=0.9389, AUC=0.9946\n",
            "Random Forest: Accuracy=0.9738, F1=0.9608, AUC=0.9967\n",
            "Linear Discriminant Analysis: Accuracy=0.9452, F1=0.9087, AUC=0.9959\n",
            "Gaussian Process: Accuracy=0.9714, F1=0.9568, AUC=0.9944\n",
            "Stacking Classifier: Accuracy=0.9786, F1=0.9677, AUC=0.9977\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9548, F1=0.9324, AUC=0.9518\n",
            "SVM: Accuracy=0.9643, F1=0.9432, AUC=0.9952\n",
            "Logistic Regression: Accuracy=0.9548, F1=0.9262, AUC=0.9954\n",
            "Random Forest: Accuracy=0.9786, F1=0.9682, AUC=0.9949\n",
            "Linear Discriminant Analysis: Accuracy=0.9357, F1=0.8933, AUC=0.9946\n",
            "Gaussian Process: Accuracy=0.9690, F1=0.9531, AUC=0.9954\n",
            "Stacking Classifier: Accuracy=0.9786, F1=0.9677, AUC=0.9982\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9643, F1=0.9471, AUC=0.9643\n",
            "SVM: Accuracy=0.9643, F1=0.9438, AUC=0.9954\n",
            "Logistic Regression: Accuracy=0.9619, F1=0.9392, AUC=0.9944\n",
            "Random Forest: Accuracy=0.9762, F1=0.9648, AUC=0.9967\n",
            "Linear Discriminant Analysis: Accuracy=0.9476, F1=0.9151, AUC=0.9906\n",
            "Gaussian Process: Accuracy=0.9667, F1=0.9502, AUC=0.9906\n",
            "Stacking Classifier: Accuracy=0.9762, F1=0.9643, AUC=0.9974\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9548, F1=0.9328, AUC=0.9518\n",
            "SVM: Accuracy=0.9667, F1=0.9475, AUC=0.9954\n",
            "Logistic Regression: Accuracy=0.9643, F1=0.9432, AUC=0.9957\n",
            "Random Forest: Accuracy=0.9786, F1=0.9680, AUC=0.9964\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9175, AUC=0.9936\n",
            "Gaussian Process: Accuracy=0.9690, F1=0.9534, AUC=0.9926\n",
            "Stacking Classifier: Accuracy=0.9690, F1=0.9531, AUC=0.9911\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9690, F1=0.9533, AUC=0.9661\n",
            "SVM: Accuracy=0.9667, F1=0.9475, AUC=0.9954\n",
            "Logistic Regression: Accuracy=0.9643, F1=0.9432, AUC=0.9957\n",
            "Random Forest: Accuracy=0.9762, F1=0.9645, AUC=0.9977\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9175, AUC=0.9936\n",
            "Gaussian Process: Accuracy=0.9690, F1=0.9534, AUC=0.9926\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9568, AUC=0.9908\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9595, F1=0.9401, AUC=0.9571\n",
            "SVM: Accuracy=0.9643, F1=0.9438, AUC=0.9952\n",
            "Logistic Regression: Accuracy=0.9643, F1=0.9432, AUC=0.9954\n",
            "Random Forest: Accuracy=0.9786, F1=0.9680, AUC=0.9980\n",
            "Linear Discriminant Analysis: Accuracy=0.9452, F1=0.9109, AUC=0.9903\n",
            "Gaussian Process: Accuracy=0.9690, F1=0.9529, AUC=0.9913\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9576, AUC=0.9972\n",
            "----------------------------------------\n",
            "              Feature Set                    Classifier  Accuracy        F1  \\\n",
            "0            All Features                 Decision Tree  0.969048  0.953794   \n",
            "1            All Features                           SVM  0.966667  0.947465   \n",
            "2            All Features           Logistic Regression  0.964286  0.943221   \n",
            "3            All Features                 Random Forest  0.976190  0.964753   \n",
            "4            All Features  Linear Discriminant Analysis  0.950000  0.917459   \n",
            "5            All Features              Gaussian Process  0.969048  0.953421   \n",
            "6            All Features           Stacking Classifier  0.971429  0.957125   \n",
            "7          RFECV Features                 Decision Tree  0.961905  0.943718   \n",
            "8          RFECV Features                           SVM  0.964286  0.943761   \n",
            "9          RFECV Features           Logistic Regression  0.964286  0.943221   \n",
            "10         RFECV Features                 Random Forest  0.973810  0.961305   \n",
            "11         RFECV Features  Linear Discriminant Analysis  0.950000  0.918563   \n",
            "12         RFECV Features              Gaussian Process  0.971429  0.956851   \n",
            "13         RFECV Features           Stacking Classifier  0.976190  0.963709   \n",
            "14           SFS Features                 Decision Tree  0.947619  0.921073   \n",
            "15           SFS Features                           SVM  0.969048  0.951453   \n",
            "16           SFS Features           Logistic Regression  0.961905  0.938913   \n",
            "17           SFS Features                 Random Forest  0.973810  0.960819   \n",
            "18           SFS Features  Linear Discriminant Analysis  0.945238  0.908669   \n",
            "19           SFS Features              Gaussian Process  0.971429  0.956821   \n",
            "20           SFS Features           Stacking Classifier  0.978571  0.967698   \n",
            "21           RFE Features                 Decision Tree  0.954762  0.932386   \n",
            "22           RFE Features                           SVM  0.964286  0.943157   \n",
            "23           RFE Features           Logistic Regression  0.954762  0.926206   \n",
            "24           RFE Features                 Random Forest  0.978571  0.968217   \n",
            "25           RFE Features  Linear Discriminant Analysis  0.935714  0.893295   \n",
            "26           RFE Features              Gaussian Process  0.969048  0.953147   \n",
            "27           RFE Features           Stacking Classifier  0.978571  0.967698   \n",
            "28       Genetic Features                 Decision Tree  0.964286  0.947128   \n",
            "29       Genetic Features                           SVM  0.964286  0.943761   \n",
            "30       Genetic Features           Logistic Regression  0.961905  0.939232   \n",
            "31       Genetic Features                 Random Forest  0.976190  0.964753   \n",
            "32       Genetic Features  Linear Discriminant Analysis  0.947619  0.915133   \n",
            "33       Genetic Features              Gaussian Process  0.966667  0.950193   \n",
            "34       Genetic Features           Stacking Classifier  0.976190  0.964258   \n",
            "35  Union of All Features                 Decision Tree  0.954762  0.932750   \n",
            "36  Union of All Features                           SVM  0.966667  0.947465   \n",
            "37  Union of All Features           Logistic Regression  0.964286  0.943221   \n",
            "38  Union of All Features                 Random Forest  0.978571  0.967971   \n",
            "39  Union of All Features  Linear Discriminant Analysis  0.950000  0.917459   \n",
            "40  Union of All Features              Gaussian Process  0.969048  0.953421   \n",
            "41  Union of All Features           Stacking Classifier  0.969048  0.953136   \n",
            "42       Intersection Set                 Decision Tree  0.969048  0.953309   \n",
            "43       Intersection Set                           SVM  0.966667  0.947465   \n",
            "44       Intersection Set           Logistic Regression  0.964286  0.943221   \n",
            "45       Intersection Set                 Random Forest  0.976190  0.964523   \n",
            "46       Intersection Set  Linear Discriminant Analysis  0.950000  0.917459   \n",
            "47       Intersection Set              Gaussian Process  0.969048  0.953421   \n",
            "48       Intersection Set           Stacking Classifier  0.971429  0.956796   \n",
            "49           Majority Set                 Decision Tree  0.959524  0.940065   \n",
            "50           Majority Set                           SVM  0.964286  0.943761   \n",
            "51           Majority Set           Logistic Regression  0.964286  0.943221   \n",
            "52           Majority Set                 Random Forest  0.978571  0.967971   \n",
            "53           Majority Set  Linear Discriminant Analysis  0.945238  0.910871   \n",
            "54           Majority Set              Gaussian Process  0.969048  0.952862   \n",
            "55           Majority Set           Stacking Classifier  0.971429  0.957601   \n",
            "\n",
            "         AUC                                  Selected Features  \n",
            "0   0.967857  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "1   0.995408  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "2   0.995663  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "3   0.997449  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "4   0.993622  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "5   0.992602  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "6   0.995918  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "7   0.960714  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "8   0.995408  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "9   0.995663  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "10  0.998214  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "11  0.994133  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "12  0.993367  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "13  0.991071  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "14  0.941071  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "15  0.994898  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "16  0.994643  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "17  0.996684  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "18  0.995918  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "19  0.994388  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "20  0.997704  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "21  0.951786  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "22  0.995153  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "23  0.995408  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "24  0.994898  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "25  0.994643  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "26  0.995408  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "27  0.998214  [ATFD_type, FANOUT_type, LOC_type, LOCNAMM_typ...  \n",
            "28  0.964286  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "29  0.995408  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "30  0.994388  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "31  0.996684  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "32  0.990561  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "33  0.990561  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "34  0.997449  [NOII_type, NOAM_type, NOCS_type, NOM_type, NM...  \n",
            "35  0.951786  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "36  0.995408  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "37  0.995663  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "38  0.996429  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "39  0.993622  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "40  0.992602  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "41  0.991071  [NOPA_type, NOAM_type, TCC_type, number_not_fi...  \n",
            "42  0.966071  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "43  0.995408  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "44  0.995663  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "45  0.997704  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "46  0.993622  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "47  0.992602  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "48  0.990816  [FANOUT_type, NOPA_type, NOAM_type, TCC_type, ...  \n",
            "49  0.957143  [NOAM_type, TCC_type, number_not_final_not_sta...  \n",
            "50  0.995153  [NOAM_type, TCC_type, number_not_final_not_sta...  \n",
            "51  0.995408  [NOAM_type, TCC_type, number_not_final_not_sta...  \n",
            "52  0.997959  [NOAM_type, TCC_type, number_not_final_not_sta...  \n",
            "53  0.990306  [NOAM_type, TCC_type, number_not_final_not_sta...  \n",
            "54  0.991327  [NOAM_type, TCC_type, number_not_final_not_sta...  \n",
            "55  0.997194  [NOAM_type, TCC_type, number_not_final_not_sta...  \n"
          ]
        }
      ],
      "source": [
        "# main program 12\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_god_class', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/GodClass.csv')\n",
        "\n",
        "X = data.drop('is_god_class', axis=1)\n",
        "y = data['is_god_class']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_god_class')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_god_class')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_god_class', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_god_class')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=MLPClassifier(max_iter=2000,activation='relu',solver='adam'))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2k7mlwGRIr6",
        "outputId": "02786fa5-bd3e-45e3-c189-48e9591ba703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Forward Selection Selected Features:\n",
            "NOP_method, CC_method, ATFD_method, CM_method, MAXNESTING_method, CYCLO_method, NMCS_method, MaMCL_method, NOAV_method, CFNAMM_method, CDISP_method, NOII_type, NOAM_type, NOCS_type, NOM_type, NMO_type, FANOUT_type, NOA_type, DIT_type, LOC_type, LOCNAMM_type, CFNAMM_type, NOPA_type, CBO_type, RFC_type, NOC_type, LCOM5_type\n",
            "----------------------------------------\n",
            "\n",
            "Backward Elimination Selected Features:\n",
            "NOP_method, MAXNESTING_method, NOAV_method, CINT_method, NOCS_type, NOA_type, DIT_type, LOC_type, LOCNAMM_type, CFNAMM_type, TCC_type, NOPA_type, CBO_type, RFC_type, NOC_type, LCOM5_type, WOC_type, WMCNAMM_type, AMWNAMM_type, NOCS_package, NOMNAMM_package, NOI_package, LOC_package, NOM_package, NOPK_project, NOI_project, NOM_project, NOMNAMM_project\n",
            "----------------------------------------\n",
            "\n",
            "RFECV Selected Features:\n",
            "NOP_method, NOLV_method, NOAV_method, ATLD_method, CINT_method\n",
            "----------------------------------------\n",
            "\n",
            "Genetic Algorithm Selected Features:\n",
            "NOP_method, FDP_method, CYCLO_method, NOLV_method, MaMCL_method, FANOUT_method, CINT_method, MeMCL_method, NOII_type, NOAM_type, NOCS_type, NOM_type, NMO_type, ATFD_type, FANOUT_type, NOMNAMM_type, NOA_type, NIM_type, DIT_type, LOC_type, LOCNAMM_type, TCC_type, CBO_type, WOC_type, AMWNAMM_type, NOCS_package, NOMNAMM_package, NOI_package, LOC_package, NOM_package, NOPK_project\n",
            "----------------------------------------\n",
            "\n",
            "Majority Voting Selected Features:\n",
            "NOAV_method, CINT_method, NOII_type, NOA_type, NOM_package, TCC_type, NOM_project, ATFD_method, NOCS_package, CM_method, CDISP_method, NOMNAMM_package, LOC_type, CBO_type, NOMNAMM_project, AMWNAMM_type, NOI_project, CYCLO_method, NOPK_project, MAXNESTING_method, NOI_package, CFNAMM_method, NOP_method, WOC_type, FANOUT_type, RFC_type, CC_method, NOCS_type, NOC_type, LCOM5_type, LOC_package, MaMCL_method, NOPA_type, NOAM_type, WMCNAMM_type, NMO_type, LOCNAMM_type, NOM_type, DIT_type, CFNAMM_type, NMCS_method\n",
            "========================================\n",
            "\n",
            "Union of Selected Features:\n",
            "NOAV_method, CINT_method, AMW_type, NOM_package, NOM_project, ATFD_type, CYCLO_method, LAA_method, NOPK_project, WMC_type, NOI_package, MeMCL_method, NOP_method, FDP_method, CC_method, NOCS_type, LCOM5_type, NOCS_project, LOC_package, NIM_type, LOC_method, NOAM_type, WMCNAMM_type, NMO_type, FANOUT_method, NOM_type, DIT_type, NOII_type, NOA_type, TCC_type, ATFD_method, ATLD_method, LOC_project, NOCS_package, NOMNAMM_type, CM_method, CDISP_method, NOMNAMM_package, CBO_type, NOMNAMM_project, AMWNAMM_type, NOI_project, MAXNESTING_method, CFNAMM_method, WOC_type, FANOUT_type, RFC_type, NOC_type, MaMCL_method, NOPA_type, NOLV_method, LOCNAMM_type, CFNAMM_type, LOC_type, NMCS_method\n",
            "========================================\n",
            "\n",
            "Intersection of Selected Features:\n",
            "NOAV_method, NOA_type, NOPA_type, NOP_method, NOC_type, RFC_type, LOCNAMM_type, CFNAMM_type, DIT_type, NOCS_type, MAXNESTING_method, LCOM5_type, LOC_type, CBO_type\n",
            "========================================\n",
            "\n",
            "Evaluating Models:\n",
            "Accuracy with selected features (27 features): 0.8651\n",
            "========================================\n",
            "Accuracy with selected features (28 features): 0.9127\n",
            "========================================\n",
            "Accuracy with selected features (5 features): 0.9127\n",
            "========================================\n",
            "Accuracy with selected features (31 features): 0.9048\n",
            "========================================\n",
            "Accuracy with selected features (41 features): 0.9048\n",
            "========================================\n",
            "Accuracy with selected features (55 features): 0.9206\n",
            "========================================\n",
            "Accuracy with selected features (14 features): 0.9048\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# Normalize data\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Feature Selection Algorithms\n",
        "# ------------------------------\n",
        "\n",
        "# Forward Selection (FS) with Best Performance\n",
        "def select_features_by_fs_best(data, target_name='is_long_parameters_list', estimator=LogisticRegression(max_iter=1000)):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Backward Elimination (BE) with Best Performance\n",
        "def select_features_by_be_best(data, target_name='is_long_parameters_list', estimator=LogisticRegression(max_iter=1000)):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, direction='backward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Recursive Feature Elimination with Cross-Validation (RFECV)\n",
        "def select_features_by_rfecv(data, target_name='is_long_parameters_list', estimator=LogisticRegression(max_iter=1000)):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=2, scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "\n",
        "# Genetic Algorithm\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Majority Voting, Union, and Intersection\n",
        "# ------------------------------\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Display Selected Features and Evaluate\n",
        "# ------------------------------\n",
        "\n",
        "def print_selected_features(method_name, feature_dict):\n",
        "    selected_features = [feature for feature, selected in feature_dict.items() if selected]\n",
        "    print(f\"\\n{method_name} Selected Features:\")\n",
        "    print(\", \".join(selected_features))\n",
        "    print(\"-\" * 40)\n",
        "    return selected_features\n",
        "\n",
        "def train_and_evaluate(data, selected_features, target_name='is_long_parameters_list'):\n",
        "    if not selected_features:\n",
        "        print(\"\\nNo features selected\")\n",
        "        return\n",
        "\n",
        "    X = data[selected_features]\n",
        "    y = data[target_name]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    model = DecisionTreeClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy with selected features ({len(selected_features)} features): {accuracy:.4f}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main Execution\n",
        "# ------------------------------\n",
        "\n",
        "# Run all feature selection methods\n",
        "fs_results = select_features_by_fs_best(data_normalized)\n",
        "be_results = select_features_by_be_best(data_normalized)\n",
        "rfecv_results = select_features_by_rfecv(data_normalized)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized)\n",
        "\n",
        "# Print selected features for each method\n",
        "fs_selected = print_selected_features(\"Forward Selection\", fs_results)\n",
        "be_selected = print_selected_features(\"Backward Elimination\", be_results)\n",
        "rfecv_selected = print_selected_features(\"RFECV\", rfecv_results)\n",
        "genetic_selected = print_selected_features(\"Genetic Algorithm\", genetic_results)\n",
        "\n",
        "# Combine results (Majority Voting, Union, Intersection)\n",
        "results_dicts = [fs_results, be_results]\n",
        "\n",
        "# Majority Voting\n",
        "majority_features = majority_voting_features(results_dicts)\n",
        "print(\"\\nMajority Voting Selected Features:\")\n",
        "print(\", \".join(majority_features))\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Union of Features\n",
        "union_features = list(set().union(*[set(d.keys()) for d in results_dicts if any(d.values())]))\n",
        "print(\"\\nUnion of Selected Features:\")\n",
        "print(\", \".join(union_features))\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Intersection of Features\n",
        "intersection_features = list(set.intersection(*[set([k for k, v in d.items() if v]) for d in results_dicts]))\n",
        "print(\"\\nIntersection of Selected Features:\")\n",
        "print(\", \".join(intersection_features))\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Evaluate models\n",
        "print(\"\\nEvaluating Models:\")\n",
        "train_and_evaluate(data_normalized, fs_selected)\n",
        "train_and_evaluate(data_normalized, be_selected)\n",
        "train_and_evaluate(data_normalized, rfecv_selected)\n",
        "train_and_evaluate(data_normalized, genetic_selected)\n",
        "train_and_evaluate(data_normalized, majority_features)\n",
        "train_and_evaluate(data_normalized, union_features)\n",
        "train_and_evaluate(data_normalized, intersection_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id-577sBOFIN",
        "outputId": "54a5f399-6e1e-497c-8d4c-a5b7b8b022a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Running Feature Selection Algorithms...\n",
            "\n",
            "\n",
            "Forward Selection (FS) Selected Features:\n",
            "NOP_method, CC_method, ATFD_method, CM_method, MAXNESTING_method, CYCLO_method, NMCS_method, MaMCL_method, NOAV_method, CFNAMM_method, CDISP_method, NOII_type, NOAM_type, NOCS_type, NOM_type, NMO_type, FANOUT_type, NOA_type, DIT_type, LOC_type, LOCNAMM_type, CFNAMM_type, NOPA_type, CBO_type, RFC_type, NOC_type, LCOM5_type\n",
            "----------------------------------------\n",
            "\n",
            "Backward Elimination (BE) Selected Features:\n",
            "NOP_method, MAXNESTING_method, NOAV_method, CINT_method, NOCS_type, NOA_type, DIT_type, LOC_type, LOCNAMM_type, CFNAMM_type, TCC_type, NOPA_type, CBO_type, RFC_type, NOC_type, LCOM5_type, WOC_type, WMCNAMM_type, AMWNAMM_type, NOCS_package, NOMNAMM_package, NOI_package, LOC_package, NOM_package, NOPK_project, NOI_project, NOM_project, NOMNAMM_project\n",
            "----------------------------------------\n",
            "\n",
            "Recursive Feature Elimination (RFECV) Selected Features:\n",
            "NOP_method, NOLV_method, NOAV_method, ATLD_method, CINT_method, NOA_type\n",
            "----------------------------------------\n",
            "\n",
            "Genetic Algorithm Selected Features:\n",
            "NOP_method, CM_method, MAXNESTING_method, LOC_method, CYCLO_method, NOAV_method, FANOUT_method, CFNAMM_method, CINT_method, NOAM_type, NMO_type, ATFD_type, NOMNAMM_type, NOA_type, NIM_type, DIT_type, NOPA_type, CBO_type, NOC_type, LCOM5_type, WMCNAMM_type, AMW_type, NOCS_package, NOI_package, LOC_package, NOPK_project, NOI_project\n",
            "----------------------------------------\n",
            "\n",
            "Union of Selected Features: ['NOAV_method', 'NOII_type', 'NOA_type', 'CINT_method', 'NOM_package', 'NOM_project', 'TCC_type', 'AMW_type', 'ATFD_method', 'ATLD_method', 'NOCS_package', 'NOMNAMM_type', 'CM_method', 'CDISP_method', 'NOMNAMM_package', 'ATFD_type', 'CBO_type', 'NOMNAMM_project', 'AMWNAMM_type', 'NOI_project', 'CYCLO_method', 'NOPK_project', 'MAXNESTING_method', 'NOI_package', 'CFNAMM_method', 'NOP_method', 'WOC_type', 'FANOUT_type', 'RFC_type', 'CC_method', 'NOCS_type', 'NOC_type', 'LCOM5_type', 'LOC_package', 'MaMCL_method', 'NIM_type', 'NOPA_type', 'LOC_method', 'NOAM_type', 'WMCNAMM_type', 'NOLV_method', 'NMO_type', 'LOCNAMM_type', 'FANOUT_method', 'NOM_type', 'DIT_type', 'CFNAMM_type', 'LOC_type', 'NMCS_method']\n",
            "\n",
            "Intersection of Selected Features: ['NOAV_method', 'NOP_method', 'NOA_type']\n",
            "\n",
            "Majority Voting Selected Features: ['NOAV_method', 'CINT_method', 'NOA_type', 'NOCS_package', 'CM_method', 'LOC_type', 'CBO_type', 'NOI_project', 'CYCLO_method', 'NOPK_project', 'MAXNESTING_method', 'NOI_package', 'CFNAMM_method', 'NOP_method', 'RFC_type', 'NOCS_type', 'NOC_type', 'LCOM5_type', 'LOC_package', 'NOPA_type', 'NOAM_type', 'WMCNAMM_type', 'NMO_type', 'LOCNAMM_type', 'DIT_type', 'CFNAMM_type']\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# Normalize data\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Feature Selection Algorithms\n",
        "# ------------------------------\n",
        "\n",
        "# Forward Selection (FS) with Best Performance\n",
        "def select_features_by_fs_best(data, target_name='is_long_parameters_list', estimator=LogisticRegression(max_iter=1000)):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Backward Elimination (BE) with Best Performance\n",
        "def select_features_by_be_best(data, target_name='is_long_parameters_list', estimator=LogisticRegression(max_iter=1000)):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, direction='backward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Recursive Feature Elimination with Cross-Validation (RFECV)\n",
        "def select_features_by_rfecv(data, target_name='is_long_parameters_list', estimator=LogisticRegression(max_iter=1000)):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "\n",
        "# Genetic Algorithm\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Majority Voting, Union, and Intersection\n",
        "# ------------------------------\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Display Selected Features\n",
        "# ------------------------------\n",
        "\n",
        "# ------------------------------\n",
        "# Display Selected Features\n",
        "# ------------------------------\n",
        "\n",
        "def print_selected_features(method_name, feature_dict):\n",
        "    selected_features = [feature for feature, selected in feature_dict.items() if selected]\n",
        "    print(f\"\\n{method_name} Selected Features:\")\n",
        "    print(\", \".join(selected_features))\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# اجرای الگوریتم‌های انتخاب ویژگی\n",
        "print(\"Running Feature Selection Algorithms...\\n\")\n",
        "\n",
        "# FS (Forward Selection)\n",
        "fs_results = select_features_by_fs_best(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Forward Selection (FS)\", fs_results)\n",
        "\n",
        "# BE (Backward Elimination)\n",
        "be_results = select_features_by_be_best(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Backward Elimination (BE)\", be_results)\n",
        "\n",
        "# RFECV\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Recursive Feature Elimination (RFECV)\", rfecv_results)\n",
        "\n",
        "# Genetic Algorithm\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Genetic Algorithm\", genetic_results)\n",
        "# جمع‌بندی نتایج\n",
        "results_dicts = [fs_results, be_results, rfecv_results, genetic_results]\n",
        "\n",
        "# اتحاد (Union) ویژگی‌ها\n",
        "union_features = set()\n",
        "for results in results_dicts:\n",
        "    union_features.update([feature for feature, selected in results.items() if selected])\n",
        "print(f\"\\nUnion of Selected Features: {list(union_features)}\")\n",
        "\n",
        "# اشتراک (Intersection) ویژگی‌ها\n",
        "intersection_features = set.intersection(*[set([feature for feature, selected in results.items() if selected]) for results in results_dicts])\n",
        "print(f\"\\nIntersection of Selected Features: {list(intersection_features)}\")\n",
        "\n",
        "# رأی‌گیری حداکثری (Majority Voting)\n",
        "majority_features = majority_voting_features(results_dicts)\n",
        "print(f\"\\nMajority Voting Selected Features: {majority_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lTN1vgu-sU6",
        "outputId": "b6a547b1-4af2-46fb-80e9-fc50afa34ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running Feature Selection Algorithms...\n",
            "\n",
            "\n",
            "Forward Selection (FS) Selected Features:\n",
            "NOP_method: Selected\n",
            "CC_method: Selected\n",
            "ATFD_method: Not Selected\n",
            "FDP_method: Not Selected\n",
            "CM_method: Selected\n",
            "MAXNESTING_method: Not Selected\n",
            "LOC_method: Not Selected\n",
            "CYCLO_method: Selected\n",
            "NMCS_method: Not Selected\n",
            "NOLV_method: Not Selected\n",
            "MaMCL_method: Not Selected\n",
            "NOAV_method: Selected\n",
            "LAA_method: Not Selected\n",
            "FANOUT_method: Not Selected\n",
            "CFNAMM_method: Not Selected\n",
            "ATLD_method: Not Selected\n",
            "CINT_method: Selected\n",
            "MeMCL_method: Not Selected\n",
            "CDISP_method: Not Selected\n",
            "NOII_type: Selected\n",
            "NOAM_type: Selected\n",
            "NOCS_type: Not Selected\n",
            "NOM_type: Not Selected\n",
            "NMO_type: Not Selected\n",
            "ATFD_type: Not Selected\n",
            "FANOUT_type: Not Selected\n",
            "NOMNAMM_type: Not Selected\n",
            "NOA_type: Not Selected\n",
            "NIM_type: Not Selected\n",
            "DIT_type: Not Selected\n",
            "LOC_type: Not Selected\n",
            "LOCNAMM_type: Selected\n",
            "CFNAMM_type: Not Selected\n",
            "TCC_type: Not Selected\n",
            "NOPA_type: Not Selected\n",
            "CBO_type: Not Selected\n",
            "RFC_type: Not Selected\n",
            "NOC_type: Not Selected\n",
            "WMC_type: Not Selected\n",
            "LCOM5_type: Not Selected\n",
            "WOC_type: Not Selected\n",
            "WMCNAMM_type: Not Selected\n",
            "AMW_type: Not Selected\n",
            "AMWNAMM_type: Not Selected\n",
            "NOCS_package: Not Selected\n",
            "NOMNAMM_package: Not Selected\n",
            "NOI_package: Not Selected\n",
            "LOC_package: Not Selected\n",
            "NOM_package: Not Selected\n",
            "NOPK_project: Not Selected\n",
            "NOCS_project: Not Selected\n",
            "NOI_project: Selected\n",
            "NOM_project: Not Selected\n",
            "NOMNAMM_project: Not Selected\n",
            "LOC_project: Not Selected\n",
            "----------------------------------------\n",
            "\n",
            "Backward Elimination (BE) Selected Features:\n",
            "NOP_method: Selected\n",
            "CC_method: Not Selected\n",
            "ATFD_method: Not Selected\n",
            "FDP_method: Not Selected\n",
            "CM_method: Not Selected\n",
            "MAXNESTING_method: Selected\n",
            "LOC_method: Not Selected\n",
            "CYCLO_method: Not Selected\n",
            "NMCS_method: Not Selected\n",
            "NOLV_method: Selected\n",
            "MaMCL_method: Not Selected\n",
            "NOAV_method: Not Selected\n",
            "LAA_method: Not Selected\n",
            "FANOUT_method: Not Selected\n",
            "CFNAMM_method: Not Selected\n",
            "ATLD_method: Not Selected\n",
            "CINT_method: Not Selected\n",
            "MeMCL_method: Not Selected\n",
            "CDISP_method: Not Selected\n",
            "NOII_type: Not Selected\n",
            "NOAM_type: Not Selected\n",
            "NOCS_type: Not Selected\n",
            "NOM_type: Not Selected\n",
            "NMO_type: Not Selected\n",
            "ATFD_type: Selected\n",
            "FANOUT_type: Not Selected\n",
            "NOMNAMM_type: Not Selected\n",
            "NOA_type: Not Selected\n",
            "NIM_type: Not Selected\n",
            "DIT_type: Not Selected\n",
            "LOC_type: Not Selected\n",
            "LOCNAMM_type: Not Selected\n",
            "CFNAMM_type: Not Selected\n",
            "TCC_type: Not Selected\n",
            "NOPA_type: Not Selected\n",
            "CBO_type: Not Selected\n",
            "RFC_type: Selected\n",
            "NOC_type: Not Selected\n",
            "WMC_type: Not Selected\n",
            "LCOM5_type: Selected\n",
            "WOC_type: Not Selected\n",
            "WMCNAMM_type: Not Selected\n",
            "AMW_type: Not Selected\n",
            "AMWNAMM_type: Selected\n",
            "NOCS_package: Not Selected\n",
            "NOMNAMM_package: Not Selected\n",
            "NOI_package: Not Selected\n",
            "LOC_package: Not Selected\n",
            "NOM_package: Selected\n",
            "NOPK_project: Selected\n",
            "NOCS_project: Not Selected\n",
            "NOI_project: Selected\n",
            "NOM_project: Not Selected\n",
            "NOMNAMM_project: Not Selected\n",
            "LOC_project: Not Selected\n",
            "----------------------------------------\n",
            "\n",
            "Recursive Feature Elimination (RFE) Selected Features:\n",
            "NOP_method: Selected\n",
            "CC_method: Not Selected\n",
            "ATFD_method: Not Selected\n",
            "FDP_method: Not Selected\n",
            "CM_method: Not Selected\n",
            "MAXNESTING_method: Selected\n",
            "LOC_method: Not Selected\n",
            "CYCLO_method: Not Selected\n",
            "NMCS_method: Not Selected\n",
            "NOLV_method: Selected\n",
            "MaMCL_method: Not Selected\n",
            "NOAV_method: Selected\n",
            "LAA_method: Selected\n",
            "FANOUT_method: Selected\n",
            "CFNAMM_method: Not Selected\n",
            "ATLD_method: Selected\n",
            "CINT_method: Selected\n",
            "MeMCL_method: Not Selected\n",
            "CDISP_method: Not Selected\n",
            "NOII_type: Not Selected\n",
            "NOAM_type: Not Selected\n",
            "NOCS_type: Not Selected\n",
            "NOM_type: Not Selected\n",
            "NMO_type: Not Selected\n",
            "ATFD_type: Selected\n",
            "FANOUT_type: Not Selected\n",
            "NOMNAMM_type: Not Selected\n",
            "NOA_type: Selected\n",
            "NIM_type: Not Selected\n",
            "DIT_type: Not Selected\n",
            "LOC_type: Not Selected\n",
            "LOCNAMM_type: Not Selected\n",
            "CFNAMM_type: Not Selected\n",
            "TCC_type: Not Selected\n",
            "NOPA_type: Not Selected\n",
            "CBO_type: Not Selected\n",
            "RFC_type: Not Selected\n",
            "NOC_type: Not Selected\n",
            "WMC_type: Not Selected\n",
            "LCOM5_type: Not Selected\n",
            "WOC_type: Not Selected\n",
            "WMCNAMM_type: Not Selected\n",
            "AMW_type: Not Selected\n",
            "AMWNAMM_type: Not Selected\n",
            "NOCS_package: Not Selected\n",
            "NOMNAMM_package: Not Selected\n",
            "NOI_package: Not Selected\n",
            "LOC_package: Not Selected\n",
            "NOM_package: Not Selected\n",
            "NOPK_project: Not Selected\n",
            "NOCS_project: Not Selected\n",
            "NOI_project: Not Selected\n",
            "NOM_project: Not Selected\n",
            "NOMNAMM_project: Not Selected\n",
            "LOC_project: Not Selected\n",
            "----------------------------------------\n",
            "\n",
            "Genetic Algorithm Selected Features:\n",
            "NOP_method: Selected\n",
            "CC_method: Not Selected\n",
            "ATFD_method: Selected\n",
            "FDP_method: Selected\n",
            "CM_method: Selected\n",
            "MAXNESTING_method: Not Selected\n",
            "LOC_method: Not Selected\n",
            "CYCLO_method: Not Selected\n",
            "NMCS_method: Not Selected\n",
            "NOLV_method: Not Selected\n",
            "MaMCL_method: Not Selected\n",
            "NOAV_method: Not Selected\n",
            "LAA_method: Not Selected\n",
            "FANOUT_method: Not Selected\n",
            "CFNAMM_method: Selected\n",
            "ATLD_method: Not Selected\n",
            "CINT_method: Not Selected\n",
            "MeMCL_method: Not Selected\n",
            "CDISP_method: Not Selected\n",
            "NOII_type: Not Selected\n",
            "NOAM_type: Not Selected\n",
            "NOCS_type: Not Selected\n",
            "NOM_type: Not Selected\n",
            "NMO_type: Selected\n",
            "ATFD_type: Selected\n",
            "FANOUT_type: Not Selected\n",
            "NOMNAMM_type: Not Selected\n",
            "NOA_type: Selected\n",
            "NIM_type: Selected\n",
            "DIT_type: Selected\n",
            "LOC_type: Not Selected\n",
            "LOCNAMM_type: Selected\n",
            "CFNAMM_type: Selected\n",
            "TCC_type: Not Selected\n",
            "NOPA_type: Not Selected\n",
            "CBO_type: Selected\n",
            "RFC_type: Not Selected\n",
            "NOC_type: Not Selected\n",
            "WMC_type: Not Selected\n",
            "LCOM5_type: Selected\n",
            "WOC_type: Selected\n",
            "WMCNAMM_type: Not Selected\n",
            "AMW_type: Selected\n",
            "AMWNAMM_type: Selected\n",
            "NOCS_package: Not Selected\n",
            "NOMNAMM_package: Selected\n",
            "NOI_package: Selected\n",
            "LOC_package: Selected\n",
            "NOM_package: Not Selected\n",
            "NOPK_project: Selected\n",
            "NOCS_project: Not Selected\n",
            "NOI_project: Selected\n",
            "NOM_project: Selected\n",
            "NOMNAMM_project: Not Selected\n",
            "LOC_project: Selected\n",
            "----------------------------------------\n",
            "\n",
            "Union of Selected Features: ['CC_method', 'CM_method', 'CINT_method', 'NMO_type', 'NOII_type', 'LOCNAMM_type', 'NOA_type', 'NOM_package', 'NOMNAMM_package', 'LCOM5_type', 'ATLD_method', 'LAA_method', 'LOC_package', 'LOC_project', 'AMWNAMM_type', 'AMW_type', 'NOM_project', 'NOI_package', 'WOC_type', 'FDP_method', 'DIT_type', 'NOAV_method', 'CYCLO_method', 'ATFD_method', 'CFNAMM_type', 'MAXNESTING_method', 'ATFD_type', 'NIM_type', 'RFC_type', 'CBO_type', 'NOLV_method', 'FANOUT_method', 'NOPK_project', 'CFNAMM_method', 'NOP_method', 'NOI_project', 'NOAM_type']\n",
            "\n",
            "Intersection of Selected Features: ['NOP_method']\n",
            "\n",
            "Majority Voting Selected Features: ['CM_method', 'CINT_method', 'LOCNAMM_type', 'NOA_type', 'LCOM5_type', 'AMWNAMM_type', 'NOI_project', 'NOAV_method', 'MAXNESTING_method', 'ATFD_type', 'NOLV_method', 'NOPK_project', 'NOP_method']\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# Normalize data\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Feature Selection Algorithms\n",
        "# ------------------------------\n",
        "\n",
        "# Forward Selection (FS)\n",
        "def select_features_by_fs(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select=10, direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Backward Elimination (BE)\n",
        "def select_features_by_be(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select=10, direction='backward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Recursive Feature Elimination (RFE)\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "\n",
        "# Genetic Algorithm\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Majority Voting, Union, and Intersection\n",
        "# ------------------------------\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Display Selected Features\n",
        "# ------------------------------\n",
        "\n",
        "def print_selected_features(method_name, feature_dict):\n",
        "    print(f\"\\n{method_name} Selected Features:\")\n",
        "    for feature, selected in feature_dict.items():\n",
        "        print(f\"{feature}: {'Selected' if selected else 'Not Selected'}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# اجرای الگوریتم‌های انتخاب ویژگی\n",
        "print(\"Running Feature Selection Algorithms...\\n\")\n",
        "\n",
        "# FS (Forward Selection)\n",
        "fs_results = select_features_by_fs(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Forward Selection (FS)\", fs_results)\n",
        "\n",
        "# BE (Backward Elimination)\n",
        "be_results = select_features_by_be(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Backward Elimination (BE)\", be_results)\n",
        "\n",
        "# RFE (Recursive Feature Elimination)\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features_to_select=10)\n",
        "print_selected_features(\"Recursive Feature Elimination (RFE)\", rfe_results)\n",
        "\n",
        "# Genetic Algorithm\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "print_selected_features(\"Genetic Algorithm\", genetic_results)\n",
        "\n",
        "# جمع‌بندی نتایج\n",
        "results_dicts = [fs_results, be_results, rfe_results, genetic_results]\n",
        "\n",
        "# اتحاد (Union) ویژگی‌ها\n",
        "union_features = set()\n",
        "for results in results_dicts:\n",
        "    union_features.update([feature for feature, selected in results.items() if selected])\n",
        "print(f\"\\nUnion of Selected Features: {list(union_features)}\")\n",
        "\n",
        "# اشتراک (Intersection) ویژگی‌ها\n",
        "intersection_features = set.intersection(*[set([feature for feature, selected in results.items() if selected]) for results in results_dicts])\n",
        "print(f\"\\nIntersection of Selected Features: {list(intersection_features)}\")\n",
        "\n",
        "# رأی‌گیری حداکثری (Majority Voting)\n",
        "majority_features = majority_voting_features(results_dicts)\n",
        "print(f\"\\nMajority Voting Selected Features: {majority_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niTbqUeUODK0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bciY0b7_9lvr",
        "outputId": "4b2ed20f-7be3-4670-e627-68b0c7e14888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9619, F1=0.9417, AUC=0.9536\n",
            "SVM: Accuracy=0.9667, F1=0.9475, AUC=0.9954\n",
            "Logistic Regression: Accuracy=0.9643, F1=0.9432, AUC=0.9957\n",
            "Random Forest: Accuracy=0.9786, F1=0.9682, AUC=0.9977\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9175, AUC=0.9936\n",
            "Gaussian Process: Accuracy=0.9690, F1=0.9534, AUC=0.9926\n",
            "Stacking Classifier: Accuracy=0.9690, F1=0.9529, AUC=0.9972\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# main program 12\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "    # Forward Selection (FS) implementation\n",
        "\n",
        "def select_features_by_fs(data, target_name='is_god_class', estimator=RandomForestClassifier()):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select=10, direction='forward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Backward Elimination (BE) implementation\n",
        "def select_features_by_be(data, target_name='is_god_class', estimator=RandomForestClassifier()):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select=10, direction='backward', scoring='accuracy', cv=StratifiedKFold(10))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_god_class', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/GodClass.csv')\n",
        "\n",
        "X = data.drop('is_god_class', axis=1)\n",
        "y = data['is_god_class']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "\n",
        "fs_results = select_features_by_fs(data_normalized, target_name='is_god_class')\n",
        "be_results = select_features_by_be(data_normalized, target_name='is_god_class')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_god_class', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_god_class')\n",
        "\n",
        "results_dicts = [be_results,fs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=MLPClassifier(max_iter=2000,activation='relu',solver='adam'))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "\n",
        "    \"SFS Features\": [feature for feature, selected in fs_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in be_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm_Q9CV86dHw",
        "outputId": "8601628f-8cd6-4309-ceed-2ffe0477eeef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "                      Feature Set                    Classifier  Accuracy  \\\n",
            "0                    All Features                 Decision Tree  0.964286   \n",
            "1                    All Features                           SVM  0.961905   \n",
            "2                    All Features           Logistic Regression  0.964286   \n",
            "3                    All Features                 Random Forest  0.978571   \n",
            "4                    All Features  Linear Discriminant Analysis  0.950000   \n",
            "5                    All Features              Gaussian Process  0.959524   \n",
            "6                  RFECV Features                 Decision Tree  0.961905   \n",
            "7                  RFECV Features                           SVM  0.964286   \n",
            "8                  RFECV Features           Logistic Regression  0.964286   \n",
            "9                  RFECV Features                 Random Forest  0.976190   \n",
            "10                 RFECV Features  Linear Discriminant Analysis  0.950000   \n",
            "11                 RFECV Features              Gaussian Process  0.964286   \n",
            "12     Forward Selection Features                 Decision Tree  0.964286   \n",
            "13     Forward Selection Features                           SVM  0.969048   \n",
            "14     Forward Selection Features           Logistic Regression  0.961905   \n",
            "15     Forward Selection Features                 Random Forest  0.978571   \n",
            "16     Forward Selection Features  Linear Discriminant Analysis  0.945238   \n",
            "17     Forward Selection Features              Gaussian Process  0.964286   \n",
            "18  Backward Elimination Features                 Decision Tree  0.957143   \n",
            "19  Backward Elimination Features                           SVM  0.966667   \n",
            "20  Backward Elimination Features           Logistic Regression  0.964286   \n",
            "21  Backward Elimination Features                 Random Forest  0.973810   \n",
            "22  Backward Elimination Features  Linear Discriminant Analysis  0.950000   \n",
            "23  Backward Elimination Features              Gaussian Process  0.957143   \n",
            "24                   RFE Features                 Decision Tree  0.954762   \n",
            "25                   RFE Features                           SVM  0.964286   \n",
            "26                   RFE Features           Logistic Regression  0.954762   \n",
            "27                   RFE Features                 Random Forest  0.978571   \n",
            "28                   RFE Features  Linear Discriminant Analysis  0.935714   \n",
            "29                   RFE Features              Gaussian Process  0.954762   \n",
            "30               Genetic Features                 Decision Tree  0.954762   \n",
            "31               Genetic Features                           SVM  0.964286   \n",
            "32               Genetic Features           Logistic Regression  0.954762   \n",
            "33               Genetic Features                 Random Forest  0.980952   \n",
            "34               Genetic Features  Linear Discriminant Analysis  0.952381   \n",
            "35               Genetic Features              Gaussian Process  0.954762   \n",
            "36       Majority Voting Features                 Decision Tree  0.957143   \n",
            "37       Majority Voting Features                           SVM  0.966667   \n",
            "38       Majority Voting Features           Logistic Regression  0.964286   \n",
            "39       Majority Voting Features                 Random Forest  0.978571   \n",
            "40       Majority Voting Features  Linear Discriminant Analysis  0.950000   \n",
            "41       Majority Voting Features              Gaussian Process  0.959524   \n",
            "\n",
            "          F1       AUC  \n",
            "0   0.945380  0.957143  \n",
            "1   0.938491  0.996684  \n",
            "2   0.943221  0.995408  \n",
            "3   0.967971  0.997832  \n",
            "4   0.917459  0.993622  \n",
            "5   0.934925  0.994643  \n",
            "6   0.942246  0.955357  \n",
            "7   0.943157  0.997449  \n",
            "8   0.943221  0.995663  \n",
            "9   0.964523  0.997959  \n",
            "10  0.918563  0.994133  \n",
            "11  0.942617  0.995408  \n",
            "12  0.945380  0.957143  \n",
            "13  0.952003  0.995918  \n",
            "14  0.938913  0.994643  \n",
            "15  0.968217  0.997194  \n",
            "16  0.908669  0.995918  \n",
            "17  0.943221  0.994133  \n",
            "18  0.935248  0.951786  \n",
            "19  0.947465  0.996684  \n",
            "20  0.942617  0.995408  \n",
            "21  0.960819  0.996811  \n",
            "22  0.917286  0.989031  \n",
            "23  0.930626  0.996173  \n",
            "24  0.932687  0.953571  \n",
            "25  0.944621  0.993622  \n",
            "26  0.926206  0.995408  \n",
            "27  0.968217  0.994770  \n",
            "28  0.893295  0.994643  \n",
            "29  0.926206  0.996173  \n",
            "30  0.931370  0.950000  \n",
            "31  0.943476  0.995408  \n",
            "32  0.925950  0.995153  \n",
            "33  0.971666  0.998597  \n",
            "34  0.920622  0.994133  \n",
            "35  0.926922  0.996173  \n",
            "36  0.932702  0.946429  \n",
            "37  0.947465  0.995918  \n",
            "38  0.943221  0.995663  \n",
            "39  0.967971  0.996173  \n",
            "40  0.917459  0.993878  \n",
            "41  0.934925  0.995408  \n"
          ]
        }
      ],
      "source": [
        "# main program\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "# Define the kernel for Gaussian Process Classifier\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "# RFECV feature selection\n",
        "def select_features_by_rfecv(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Forward Selection (FS) implementation\n",
        "def select_features_by_fs(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Backward Elimination (BE) implementation\n",
        "def select_features_by_be(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='backward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# RFE feature selection\n",
        "def select_features_by_rfe(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# Genetic algorithm for feature selection\n",
        "def select_features_by_custom_genetic(data, target_name='is_god_class', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    def fitness(solution, X, y):\n",
        "        if np.sum(solution) == 0:\n",
        "            return 0\n",
        "        X_selected = X[:, solution == 1]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        return accuracy_score(y_test, y_pred)\n",
        "\n",
        "    def initialize_population(pop_size, num_features):\n",
        "        return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "    def select_parents(population, fitness_scores):\n",
        "        parents = []\n",
        "        for _ in range(len(population)):\n",
        "            i, j = np.random.choice(len(population), 2, replace=False)\n",
        "            if fitness_scores[i] > fitness_scores[j]:\n",
        "                parents.append(population[i])\n",
        "            else:\n",
        "                parents.append(population[j])\n",
        "        return np.array(parents)\n",
        "\n",
        "    def crossover(parents):\n",
        "        offspring = []\n",
        "        for i in range(0, len(parents), 2):\n",
        "            if i + 1 < len(parents):\n",
        "                crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "                parent1, parent2 = parents[i], parents[i + 1]\n",
        "                child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "                child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "                offspring.extend([child1, child2])\n",
        "        return np.array(offspring)\n",
        "\n",
        "    def mutate(offspring, mutation_rate=0.01):\n",
        "        for individual in offspring:\n",
        "            for gene in range(len(individual)):\n",
        "                if random.random() < mutation_rate:\n",
        "                    individual[gene] = 1 - individual[gene]\n",
        "        return offspring\n",
        "\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "# Majority voting for feature selection\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "# Load and preprocess data\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/GodClass.csv')\n",
        "\n",
        "X = data.drop('is_god_class', axis=1)\n",
        "y = data['is_god_class']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "# Perform feature selection using different methods\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_god_class')\n",
        "fs_results = select_features_by_fs(data_normalized, target_name='is_god_class')\n",
        "be_results = select_features_by_be(data_normalized, target_name='is_god_class')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_god_class', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_god_class')\n",
        "\n",
        "results_dicts = [rfecv_results, fs_results, be_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "# Feature sets\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"Forward Selection Features\": [feature for feature, selected in fs_results.items() if selected],\n",
        "    \"Backward Elimination Features\": [feature for feature, selected in be_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Majority Voting Features\": majority_voting_features,\n",
        "}\n",
        "\n",
        "# Define classifiers\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier()),\n",
        "]\n",
        "\n",
        "# Evaluate classifiers for each feature set\n",
        "results_summary = []\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in base_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    # Aggregate results\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame and display\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkNDXi7vhbd8",
        "outputId": "5b155a58-4ef8-468a-b15d-5818e8badc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from random import random\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "# ** توابع انتخاب ویژگی **\n",
        "\n",
        "# SFS Forward\n",
        "def select_features_by_sfs_forward(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "# SFS Backward\n",
        "def select_features_by_sfs_backward(data, target_name='is_god_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='backward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = np.random.randint(2, size=(pop_size, X.shape[1]))\n",
        "\n",
        "    def fitness(solution):\n",
        "        if np.sum(solution) == 0:\n",
        "            return 0\n",
        "        X_selected = X[:, solution == 1]\n",
        "        if X_selected.shape[1] == 0:\n",
        "            return 0\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        return (y_test == y_pred).mean()\n",
        "\n",
        "    best_solution = None\n",
        "    best_fitness = -1\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual) for individual in population])\n",
        "        if np.all(fitness_scores == 0):\n",
        "            continue\n",
        "        current_best = population[np.argmax(fitness_scores)]\n",
        "        current_fitness = np.max(fitness_scores)\n",
        "        if current_fitness > best_fitness:\n",
        "            best_solution = current_best\n",
        "            best_fitness = current_fitness\n",
        "        parents = [current_best]\n",
        "        offspring = []\n",
        "        for i in range(0, len(parents), 2):\n",
        "            if i + 1 < len(parents):\n",
        "                crossover_point = np.random.randint(1, parents.shape[1] - 1)\n",
        "                parent1, parent2 = parents[i], parents[i + 1]\n",
        "                child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "                child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "                offspring.extend([child1, child2])\n",
        "        population = np.array(offspring)\n",
        "        for individual in population:\n",
        "            for gene in range(len(individual)):\n",
        "                if np.random.random() < mutation_rate:\n",
        "                    individual[gene] = 1 - individual[gene]\n",
        "\n",
        "    if best_solution is None:\n",
        "        raise ValueError(\"No valid solution found.\")\n",
        "\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "# Majority Voting\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "# ** بارگذاری و پیش‌پردازش داده‌ها **\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/GodClass.csv')\n",
        "\n",
        "X = data.drop('is_god_class', axis=1)\n",
        "y = data['is_god_class']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "# ** اجرای الگوریتم‌های انتخاب ویژگی **\n",
        "sfs_forward_results = select_features_by_sfs_forward(data_normalized, target_name='is_god_class')\n",
        "sfs_backward_results = select_features_by_sfs_backward(data_normalized, target_name='is_god_class')\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_god_class')\n",
        "\n",
        "results_dicts = [sfs_forward_results, sfs_backward_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=MLPClassifier(max_iter=2000,activation='relu',solver='adam'))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5nvrjFc6bae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxczzuNxfJI2",
        "outputId": "76b5d050-f191-4b26-a491-96173ef046f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9905, F1=0.9854, AUC=0.9875\n",
            "SVM: Accuracy=0.9738, F1=0.9603, AUC=0.9972\n",
            "Logistic Regression: Accuracy=0.9905, F1=0.9857, AUC=0.9992\n",
            "Random Forest: Accuracy=0.9905, F1=0.9860, AUC=0.9997\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9241, AUC=0.9802\n",
            "Gaussian Process: Accuracy=0.9833, F1=0.9754, AUC=0.9982\n",
            "Stacking Classifier: Accuracy=0.9952, F1=0.9928, AUC=0.9995\n",
            "----------------------------------------\n",
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.9929, F1=0.9891, AUC=0.9911\n",
            "SVM: Accuracy=0.9881, F1=0.9825, AUC=0.9990\n",
            "Logistic Regression: Accuracy=0.9905, F1=0.9862, AUC=0.9985\n",
            "Random Forest: Accuracy=0.9976, F1=0.9963, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9643, F1=0.9444, AUC=0.9972\n",
            "Gaussian Process: Accuracy=0.9857, F1=0.9791, AUC=0.9992\n",
            "Stacking Classifier: Accuracy=0.9952, F1=0.9931, AUC=0.9997\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9905, F1=0.9854, AUC=0.9875\n",
            "SVM: Accuracy=0.9857, F1=0.9780, AUC=0.9987\n",
            "Logistic Regression: Accuracy=0.9976, F1=0.9966, AUC=0.9990\n",
            "Random Forest: Accuracy=0.9952, F1=0.9928, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9619, F1=0.9410, AUC=0.9939\n",
            "Gaussian Process: Accuracy=0.9929, F1=0.9894, AUC=0.9997\n",
            "Stacking Classifier: Accuracy=0.9952, F1=0.9931, AUC=0.9997\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9929, F1=0.9891, AUC=0.9911\n",
            "SVM: Accuracy=0.9810, F1=0.9709, AUC=0.9980\n",
            "Logistic Regression: Accuracy=0.9905, F1=0.9854, AUC=0.9980\n",
            "Random Forest: Accuracy=0.9976, F1=0.9963, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9690, F1=0.9517, AUC=0.9974\n",
            "Gaussian Process: Accuracy=0.9833, F1=0.9748, AUC=0.9990\n",
            "Stacking Classifier: Accuracy=0.9952, F1=0.9931, AUC=0.9997\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9667, F1=0.9496, AUC=0.9625\n",
            "SVM: Accuracy=0.9714, F1=0.9560, AUC=0.9972\n",
            "Logistic Regression: Accuracy=0.9667, F1=0.9494, AUC=0.9903\n",
            "Random Forest: Accuracy=0.9833, F1=0.9754, AUC=0.9985\n",
            "Linear Discriminant Analysis: Accuracy=0.9476, F1=0.9201, AUC=0.9883\n",
            "Gaussian Process: Accuracy=0.9762, F1=0.9645, AUC=0.9967\n",
            "Stacking Classifier: Accuracy=0.9738, F1=0.9600, AUC=0.9972\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9929, F1=0.9891, AUC=0.9911\n",
            "SVM: Accuracy=0.9738, F1=0.9603, AUC=0.9972\n",
            "Logistic Regression: Accuracy=0.9905, F1=0.9857, AUC=0.9992\n",
            "Random Forest: Accuracy=0.9929, F1=0.9894, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9241, AUC=0.9802\n",
            "Gaussian Process: Accuracy=0.9833, F1=0.9754, AUC=0.9982\n",
            "Stacking Classifier: Accuracy=0.9929, F1=0.9894, AUC=0.9995\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9857, F1=0.9786, AUC=0.9839\n",
            "SVM: Accuracy=0.9738, F1=0.9603, AUC=0.9972\n",
            "Logistic Regression: Accuracy=0.9905, F1=0.9857, AUC=0.9992\n",
            "Random Forest: Accuracy=0.9905, F1=0.9860, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9500, F1=0.9241, AUC=0.9802\n",
            "Gaussian Process: Accuracy=0.9833, F1=0.9754, AUC=0.9982\n",
            "Stacking Classifier: Accuracy=0.9952, F1=0.9928, AUC=0.9995\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9929, F1=0.9891, AUC=0.9911\n",
            "SVM: Accuracy=0.9786, F1=0.9680, AUC=0.9985\n",
            "Logistic Regression: Accuracy=0.9881, F1=0.9827, AUC=0.9969\n",
            "Random Forest: Accuracy=0.9952, F1=0.9928, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9619, F1=0.9412, AUC=0.9949\n",
            "Gaussian Process: Accuracy=0.9857, F1=0.9791, AUC=0.9992\n",
            "Stacking Classifier: Accuracy=0.9976, F1=0.9966, AUC=0.9997\n",
            "----------------------------------------\n",
            "              Feature Set                    Classifier  Accuracy        F1  \\\n",
            "0            All Features                 Decision Tree  0.990476  0.985441   \n",
            "1            All Features                           SVM  0.973810  0.960299   \n",
            "2            All Features           Logistic Regression  0.990476  0.985705   \n",
            "3            All Features                 Random Forest  0.990476  0.985951   \n",
            "4            All Features  Linear Discriminant Analysis  0.950000  0.924144   \n",
            "5            All Features              Gaussian Process  0.983333  0.975360   \n",
            "6            All Features           Stacking Classifier  0.995238  0.992848   \n",
            "7          RFECV Features                 Decision Tree  0.992857  0.989144   \n",
            "8          RFECV Features                           SVM  0.988095  0.982512   \n",
            "9          RFECV Features           Logistic Regression  0.990476  0.986207   \n",
            "10         RFECV Features                 Random Forest  0.997619  0.996296   \n",
            "11         RFECV Features  Linear Discriminant Analysis  0.964286  0.944374   \n",
            "12         RFECV Features              Gaussian Process  0.985714  0.979064   \n",
            "13         RFECV Features           Stacking Classifier  0.995238  0.993103   \n",
            "14           SFS Features                 Decision Tree  0.990476  0.985441   \n",
            "15           SFS Features                           SVM  0.985714  0.978042   \n",
            "16           SFS Features           Logistic Regression  0.997619  0.996552   \n",
            "17           SFS Features                 Random Forest  0.995238  0.992848   \n",
            "18           SFS Features  Linear Discriminant Analysis  0.961905  0.940955   \n",
            "19           SFS Features              Gaussian Process  0.992857  0.989409   \n",
            "20           SFS Features           Stacking Classifier  0.995238  0.993103   \n",
            "21           RFE Features                 Decision Tree  0.992857  0.989144   \n",
            "22           RFE Features                           SVM  0.980952  0.970870   \n",
            "23           RFE Features           Logistic Regression  0.990476  0.985411   \n",
            "24           RFE Features                 Random Forest  0.997619  0.996296   \n",
            "25           RFE Features  Linear Discriminant Analysis  0.969048  0.951727   \n",
            "26           RFE Features              Gaussian Process  0.983333  0.974849   \n",
            "27           RFE Features           Stacking Classifier  0.995238  0.993103   \n",
            "28       Genetic Features                 Decision Tree  0.966667  0.949624   \n",
            "29       Genetic Features                           SVM  0.971429  0.955962   \n",
            "30       Genetic Features           Logistic Regression  0.966667  0.949405   \n",
            "31       Genetic Features                 Random Forest  0.983333  0.975360   \n",
            "32       Genetic Features  Linear Discriminant Analysis  0.947619  0.920140   \n",
            "33       Genetic Features              Gaussian Process  0.976190  0.964514   \n",
            "34       Genetic Features           Stacking Classifier  0.973810  0.960035   \n",
            "35  Union of All Features                 Decision Tree  0.992857  0.989144   \n",
            "36  Union of All Features                           SVM  0.973810  0.960299   \n",
            "37  Union of All Features           Logistic Regression  0.990476  0.985705   \n",
            "38  Union of All Features                 Random Forest  0.992857  0.989400   \n",
            "39  Union of All Features  Linear Discriminant Analysis  0.950000  0.924144   \n",
            "40  Union of All Features              Gaussian Process  0.983333  0.975360   \n",
            "41  Union of All Features           Stacking Classifier  0.992857  0.989400   \n",
            "42       Intersection Set                 Decision Tree  0.985714  0.978553   \n",
            "43       Intersection Set                           SVM  0.973810  0.960299   \n",
            "44       Intersection Set           Logistic Regression  0.990476  0.985705   \n",
            "45       Intersection Set                 Random Forest  0.990476  0.985951   \n",
            "46       Intersection Set  Linear Discriminant Analysis  0.950000  0.924144   \n",
            "47       Intersection Set              Gaussian Process  0.983333  0.975360   \n",
            "48       Intersection Set           Stacking Classifier  0.995238  0.992848   \n",
            "49           Majority Set                 Decision Tree  0.992857  0.989144   \n",
            "50           Majority Set                           SVM  0.978571  0.967953   \n",
            "51           Majority Set           Logistic Regression  0.988095  0.982733   \n",
            "52           Majority Set                 Random Forest  0.995238  0.992848   \n",
            "53           Majority Set  Linear Discriminant Analysis  0.961905  0.941200   \n",
            "54           Majority Set              Gaussian Process  0.985714  0.979055   \n",
            "55           Majority Set           Stacking Classifier  0.997619  0.996552   \n",
            "\n",
            "         AUC                                  Selected Features  \n",
            "0   0.987500  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "1   0.997194  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "2   0.999235  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "3   0.999745  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "4   0.980230  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "5   0.998214  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "6   0.999490  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "7   0.991071  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "8   0.998980  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "9   0.998469  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "10  1.000000  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "11  0.997194  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "12  0.999235  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "13  0.999745  [NOP_method, FDP_method, MAXNESTING_method, LO...  \n",
            "14  0.987500  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "15  0.998724  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "16  0.998980  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "17  1.000000  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "18  0.993878  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "19  0.999745  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "20  0.999745  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "21  0.991071  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "22  0.997959  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "23  0.997959  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "24  1.000000  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "25  0.997449  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "26  0.998980  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "27  0.999745  [NOP_method, MAXNESTING_method, LOC_method, CY...  \n",
            "28  0.962500  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "29  0.997194  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "30  0.990306  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "31  0.998469  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "32  0.988265  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "33  0.996684  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "34  0.997194  [ATFD_method, CM_method, MAXNESTING_method, NO...  \n",
            "35  0.991071  [num_not_final_not_static_attributes, CBO_type...  \n",
            "36  0.997194  [num_not_final_not_static_attributes, CBO_type...  \n",
            "37  0.999235  [num_not_final_not_static_attributes, CBO_type...  \n",
            "38  1.000000  [num_not_final_not_static_attributes, CBO_type...  \n",
            "39  0.980230  [num_not_final_not_static_attributes, CBO_type...  \n",
            "40  0.998214  [num_not_final_not_static_attributes, CBO_type...  \n",
            "41  0.999490  [num_not_final_not_static_attributes, CBO_type...  \n",
            "42  0.983929  [num_not_final_not_static_attributes, CBO_type...  \n",
            "43  0.997194  [num_not_final_not_static_attributes, CBO_type...  \n",
            "44  0.999235  [num_not_final_not_static_attributes, CBO_type...  \n",
            "45  1.000000  [num_not_final_not_static_attributes, CBO_type...  \n",
            "46  0.980230  [num_not_final_not_static_attributes, CBO_type...  \n",
            "47  0.998214  [num_not_final_not_static_attributes, CBO_type...  \n",
            "48  0.999490  [num_not_final_not_static_attributes, CBO_type...  \n",
            "49  0.991071  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n",
            "50  0.998469  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n",
            "51  0.996939  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n",
            "52  1.000000  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n",
            "53  0.994898  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n",
            "54  0.999235  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n",
            "55  0.999745  [NOMNAMM_package, NOAV_method, CYCLO_method, M...  \n"
          ]
        }
      ],
      "source": [
        "# main program 11\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_long_method', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_method', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_method', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_method', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongMethod.csv')\n",
        "\n",
        "X = data.drop('is_long_method', axis=1)\n",
        "y = data['is_long_method']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_long_method')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_long_method')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_long_method', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_long_method')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=MLPClassifier(max_iter=2000,activation='relu',solver='adam'))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rw1GDipqHSX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ6xR-aWWgHr",
        "outputId": "54b55038-904b-4a53-e549-697f20591657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9452, F1=0.9189, AUC=0.9411\n",
            "SVM: Accuracy=0.9452, F1=0.9172, AUC=0.9640\n",
            "Logistic Regression: Accuracy=0.9476, F1=0.9189, AUC=0.9878\n",
            "Random Forest: Accuracy=0.9548, F1=0.9337, AUC=0.9893\n",
            "Linear Discriminant Analysis: Accuracy=0.8881, F1=0.8089, AUC=0.9643\n",
            "Gaussian Process: Accuracy=0.9452, F1=0.9141, AUC=0.9847\n",
            "Stacking Classifier: Accuracy=0.9595, F1=0.9401, AUC=0.9895\n",
            "----------------------------------------\n",
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.9571, F1=0.9346, AUC=0.9500\n",
            "SVM: Accuracy=0.9595, F1=0.9382, AUC=0.9895\n",
            "Logistic Regression: Accuracy=0.9429, F1=0.9107, AUC=0.9885\n",
            "Random Forest: Accuracy=0.9643, F1=0.9467, AUC=0.9944\n",
            "Linear Discriminant Analysis: Accuracy=0.8952, F1=0.8147, AUC=0.9916\n",
            "Gaussian Process: Accuracy=0.9738, F1=0.9602, AUC=0.9918\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9571, AUC=0.9929\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9524, F1=0.9286, AUC=0.9482\n",
            "SVM: Accuracy=0.9500, F1=0.9230, AUC=0.9753\n",
            "Logistic Regression: Accuracy=0.9429, F1=0.9099, AUC=0.9903\n",
            "Random Forest: Accuracy=0.9643, F1=0.9470, AUC=0.9921\n",
            "Linear Discriminant Analysis: Accuracy=0.8976, F1=0.8246, AUC=0.9814\n",
            "Gaussian Process: Accuracy=0.9500, F1=0.9240, AUC=0.9883\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9573, AUC=0.9923\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9571, F1=0.9348, AUC=0.9518\n",
            "SVM: Accuracy=0.9595, F1=0.9382, AUC=0.9895\n",
            "Logistic Regression: Accuracy=0.9429, F1=0.9107, AUC=0.9885\n",
            "Random Forest: Accuracy=0.9690, F1=0.9536, AUC=0.9941\n",
            "Linear Discriminant Analysis: Accuracy=0.8929, F1=0.8071, AUC=0.9916\n",
            "Gaussian Process: Accuracy=0.9762, F1=0.9637, AUC=0.9939\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9571, AUC=0.9936\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9429, F1=0.9113, AUC=0.9339\n",
            "SVM: Accuracy=0.9381, F1=0.9075, AUC=0.9758\n",
            "Logistic Regression: Accuracy=0.9405, F1=0.9061, AUC=0.9872\n",
            "Random Forest: Accuracy=0.9690, F1=0.9539, AUC=0.9908\n",
            "Linear Discriminant Analysis: Accuracy=0.8929, F1=0.8149, AUC=0.9719\n",
            "Gaussian Process: Accuracy=0.9500, F1=0.9218, AUC=0.9842\n",
            "Stacking Classifier: Accuracy=0.9690, F1=0.9541, AUC=0.9918\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9429, F1=0.9138, AUC=0.9357\n",
            "SVM: Accuracy=0.9452, F1=0.9172, AUC=0.9640\n",
            "Logistic Regression: Accuracy=0.9476, F1=0.9189, AUC=0.9878\n",
            "Random Forest: Accuracy=0.9571, F1=0.9351, AUC=0.9903\n",
            "Linear Discriminant Analysis: Accuracy=0.8881, F1=0.8089, AUC=0.9643\n",
            "Gaussian Process: Accuracy=0.9452, F1=0.9141, AUC=0.9847\n",
            "Stacking Classifier: Accuracy=0.9643, F1=0.9482, AUC=0.9906\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9500, F1=0.9254, AUC=0.9464\n",
            "SVM: Accuracy=0.9452, F1=0.9172, AUC=0.9640\n",
            "Logistic Regression: Accuracy=0.9476, F1=0.9189, AUC=0.9878\n",
            "Random Forest: Accuracy=0.9643, F1=0.9468, AUC=0.9906\n",
            "Linear Discriminant Analysis: Accuracy=0.8881, F1=0.8089, AUC=0.9643\n",
            "Gaussian Process: Accuracy=0.9452, F1=0.9141, AUC=0.9847\n",
            "Stacking Classifier: Accuracy=0.9619, F1=0.9438, AUC=0.9890\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9500, F1=0.9240, AUC=0.9429\n",
            "SVM: Accuracy=0.9548, F1=0.9319, AUC=0.9911\n",
            "Logistic Regression: Accuracy=0.9500, F1=0.9232, AUC=0.9883\n",
            "Random Forest: Accuracy=0.9667, F1=0.9512, AUC=0.9911\n",
            "Linear Discriminant Analysis: Accuracy=0.8929, F1=0.8134, AUC=0.9860\n",
            "Gaussian Process: Accuracy=0.9643, F1=0.9459, AUC=0.9875\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9570, AUC=0.9911\n",
            "----------------------------------------\n",
            "              Feature Set                    Classifier  Accuracy        F1  \\\n",
            "0            All Features                 Decision Tree  0.945238  0.918919   \n",
            "1            All Features                           SVM  0.945238  0.917168   \n",
            "2            All Features           Logistic Regression  0.947619  0.918911   \n",
            "3            All Features                 Random Forest  0.954762  0.933685   \n",
            "4            All Features  Linear Discriminant Analysis  0.888095  0.808891   \n",
            "5            All Features              Gaussian Process  0.945238  0.914082   \n",
            "6            All Features           Stacking Classifier  0.959524  0.940060   \n",
            "7          RFECV Features                 Decision Tree  0.957143  0.934594   \n",
            "8          RFECV Features                           SVM  0.959524  0.938239   \n",
            "9          RFECV Features           Logistic Regression  0.942857  0.910697   \n",
            "10         RFECV Features                 Random Forest  0.964286  0.946745   \n",
            "11         RFECV Features  Linear Discriminant Analysis  0.895238  0.814710   \n",
            "12         RFECV Features              Gaussian Process  0.973810  0.960206   \n",
            "13         RFECV Features           Stacking Classifier  0.971429  0.957072   \n",
            "14           SFS Features                 Decision Tree  0.952381  0.928571   \n",
            "15           SFS Features                           SVM  0.950000  0.922966   \n",
            "16           SFS Features           Logistic Regression  0.942857  0.909854   \n",
            "17           SFS Features                 Random Forest  0.964286  0.946991   \n",
            "18           SFS Features  Linear Discriminant Analysis  0.897619  0.824587   \n",
            "19           SFS Features              Gaussian Process  0.950000  0.924008   \n",
            "20           SFS Features           Stacking Classifier  0.971429  0.957336   \n",
            "21           RFE Features                 Decision Tree  0.957143  0.934849   \n",
            "22           RFE Features                           SVM  0.959524  0.938239   \n",
            "23           RFE Features           Logistic Regression  0.942857  0.910697   \n",
            "24           RFE Features                 Random Forest  0.969048  0.953623   \n",
            "25           RFE Features  Linear Discriminant Analysis  0.892857  0.807059   \n",
            "26           RFE Features              Gaussian Process  0.976190  0.963654   \n",
            "27           RFE Features           Stacking Classifier  0.971429  0.957072   \n",
            "28       Genetic Features                 Decision Tree  0.942857  0.911292   \n",
            "29       Genetic Features                           SVM  0.938095  0.907478   \n",
            "30       Genetic Features           Logistic Regression  0.940476  0.906125   \n",
            "31       Genetic Features                 Random Forest  0.969048  0.953879   \n",
            "32       Genetic Features  Linear Discriminant Analysis  0.892857  0.814942   \n",
            "33       Genetic Features              Gaussian Process  0.950000  0.921849   \n",
            "34       Genetic Features           Stacking Classifier  0.969048  0.954100   \n",
            "35  Union of All Features                 Decision Tree  0.942857  0.913785   \n",
            "36  Union of All Features                           SVM  0.945238  0.917168   \n",
            "37  Union of All Features           Logistic Regression  0.947619  0.918911   \n",
            "38  Union of All Features                 Random Forest  0.957143  0.935105   \n",
            "39  Union of All Features  Linear Discriminant Analysis  0.888095  0.808891   \n",
            "40  Union of All Features              Gaussian Process  0.945238  0.914082   \n",
            "41  Union of All Features           Stacking Classifier  0.964286  0.948243   \n",
            "42       Intersection Set                 Decision Tree  0.950000  0.925416   \n",
            "43       Intersection Set                           SVM  0.945238  0.917168   \n",
            "44       Intersection Set           Logistic Regression  0.947619  0.918911   \n",
            "45       Intersection Set                 Random Forest  0.964286  0.946754   \n",
            "46       Intersection Set  Linear Discriminant Analysis  0.888095  0.808891   \n",
            "47       Intersection Set              Gaussian Process  0.945238  0.914082   \n",
            "48       Intersection Set           Stacking Classifier  0.961905  0.943799   \n",
            "49           Majority Set                 Decision Tree  0.950000  0.924012   \n",
            "50           Majority Set                           SVM  0.954762  0.931883   \n",
            "51           Majority Set           Logistic Regression  0.950000  0.923185   \n",
            "52           Majority Set                 Random Forest  0.966667  0.951190   \n",
            "53           Majority Set  Linear Discriminant Analysis  0.892857  0.813416   \n",
            "54           Majority Set              Gaussian Process  0.964286  0.945931   \n",
            "55           Majority Set           Stacking Classifier  0.971429  0.957042   \n",
            "\n",
            "         AUC                                  Selected Features  \n",
            "0   0.941071  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "1   0.964031  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "2   0.987755  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "3   0.989286  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "4   0.964286  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "5   0.984694  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "6   0.989541  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "7   0.950000  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "8   0.989541  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "9   0.988520  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "10  0.994388  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "11  0.991582  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "12  0.991837  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "13  0.992857  [ATFD_method, FDP_method, MAXNESTING_method, M...  \n",
            "14  0.948214  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "15  0.975255  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "16  0.990306  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "17  0.992092  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "18  0.981378  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "19  0.988265  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "20  0.992347  [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "21  0.951786  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "22  0.989541  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "23  0.988520  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "24  0.994133  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "25  0.991582  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "26  0.993878  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "27  0.993622  [ATFD_method, FDP_method, MAXNESTING_method, N...  \n",
            "28  0.933929  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "29  0.975765  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "30  0.987245  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "31  0.990816  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "32  0.971939  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "33  0.984184  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "34  0.991837  [CC_method, ATFD_method, FDP_method, LOC_metho...  \n",
            "35  0.935714  [num_not_final_not_static_attributes, CBO_type...  \n",
            "36  0.964031  [num_not_final_not_static_attributes, CBO_type...  \n",
            "37  0.987755  [num_not_final_not_static_attributes, CBO_type...  \n",
            "38  0.990306  [num_not_final_not_static_attributes, CBO_type...  \n",
            "39  0.964286  [num_not_final_not_static_attributes, CBO_type...  \n",
            "40  0.984694  [num_not_final_not_static_attributes, CBO_type...  \n",
            "41  0.990561  [num_not_final_not_static_attributes, CBO_type...  \n",
            "42  0.946429  [num_not_final_not_static_attributes, CBO_type...  \n",
            "43  0.964031  [num_not_final_not_static_attributes, CBO_type...  \n",
            "44  0.987755  [num_not_final_not_static_attributes, CBO_type...  \n",
            "45  0.990561  [num_not_final_not_static_attributes, CBO_type...  \n",
            "46  0.964286  [num_not_final_not_static_attributes, CBO_type...  \n",
            "47  0.984694  [num_not_final_not_static_attributes, CBO_type...  \n",
            "48  0.989031  [num_not_final_not_static_attributes, CBO_type...  \n",
            "49  0.942857  [NMCS_method, NOAV_method, num_static_not_fina...  \n",
            "50  0.991071  [NMCS_method, NOAV_method, num_static_not_fina...  \n",
            "51  0.988265  [NMCS_method, NOAV_method, num_static_not_fina...  \n",
            "52  0.991071  [NMCS_method, NOAV_method, num_static_not_fina...  \n",
            "53  0.985969  [NMCS_method, NOAV_method, num_static_not_fina...  \n",
            "54  0.987500  [NMCS_method, NOAV_method, num_static_not_fina...  \n",
            "55  0.991071  [NMCS_method, NOAV_method, num_static_not_fina...  \n"
          ]
        }
      ],
      "source": [
        "# main program 10\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_data_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_data_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_data_class', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_data_class', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/FeatureEnvy.csv')\n",
        "\n",
        "X = data.drop('is_feature_envy', axis=1)\n",
        "y = data['is_feature_envy']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_feature_envy')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_feature_envy')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_feature_envy', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_feature_envy')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=MLPClassifier(max_iter=2000,activation='relu',solver='adam'))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wU1FYUYXKgsa",
        "outputId": "2c98dae6-e3cc-46b2-ed6e-b5c94fafcd3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9881, F1=0.9827, AUC=0.9893\n",
            "SVM: Accuracy=0.9690, F1=0.9554, AUC=0.9916\n",
            "Logistic Regression: Accuracy=0.9762, F1=0.9654, AUC=0.9939\n",
            "Random Forest: Accuracy=0.9857, F1=0.9795, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9190, F1=0.8864, AUC=0.9788\n",
            "Gaussian Process: Accuracy=0.9738, F1=0.9608, AUC=0.9967\n",
            "Stacking Classifier: Accuracy=0.9952, F1=0.9928, AUC=1.0000\n",
            "----------------------------------------\n",
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.9381, F1=0.9061, AUC=0.9304\n",
            "SVM: Accuracy=0.9476, F1=0.9229, AUC=0.9806\n",
            "Logistic Regression: Accuracy=0.9476, F1=0.9236, AUC=0.9821\n",
            "Random Forest: Accuracy=0.9714, F1=0.9565, AUC=0.9967\n",
            "Linear Discriminant Analysis: Accuracy=0.9119, F1=0.8788, AUC=0.9661\n",
            "Gaussian Process: Accuracy=0.9643, F1=0.9468, AUC=0.9908\n",
            "Stacking Classifier: Accuracy=0.9714, F1=0.9566, AUC=0.9898\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9881, F1=0.9827, AUC=0.9893\n",
            "SVM: Accuracy=0.9738, F1=0.9605, AUC=0.9890\n",
            "Logistic Regression: Accuracy=0.9714, F1=0.9578, AUC=0.9880\n",
            "Random Forest: Accuracy=0.9833, F1=0.9758, AUC=0.9990\n",
            "Linear Discriminant Analysis: Accuracy=0.9310, F1=0.8995, AUC=0.9745\n",
            "Gaussian Process: Accuracy=0.9762, F1=0.9654, AUC=0.9962\n",
            "Stacking Classifier: Accuracy=0.9881, F1=0.9817, AUC=0.9992\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9619, F1=0.9406, AUC=0.9500\n",
            "SVM: Accuracy=0.9429, F1=0.9177, AUC=0.9786\n",
            "Logistic Regression: Accuracy=0.9500, F1=0.9266, AUC=0.9793\n",
            "Random Forest: Accuracy=0.9857, F1=0.9772, AUC=0.9990\n",
            "Linear Discriminant Analysis: Accuracy=0.9095, F1=0.8753, AUC=0.9633\n",
            "Gaussian Process: Accuracy=0.9810, F1=0.9711, AUC=0.9929\n",
            "Stacking Classifier: Accuracy=0.9786, F1=0.9675, AUC=0.9985\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9857, F1=0.9798, AUC=0.9893\n",
            "SVM: Accuracy=0.9738, F1=0.9618, AUC=0.9903\n",
            "Logistic Regression: Accuracy=0.9786, F1=0.9684, AUC=0.9901\n",
            "Random Forest: Accuracy=0.9833, F1=0.9760, AUC=0.9997\n",
            "Linear Discriminant Analysis: Accuracy=0.9119, F1=0.8792, AUC=0.9719\n",
            "Gaussian Process: Accuracy=0.9619, F1=0.9438, AUC=0.9923\n",
            "Stacking Classifier: Accuracy=0.9929, F1=0.9896, AUC=0.9992\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9881, F1=0.9827, AUC=0.9893\n",
            "SVM: Accuracy=0.9690, F1=0.9554, AUC=0.9916\n",
            "Logistic Regression: Accuracy=0.9762, F1=0.9654, AUC=0.9939\n",
            "Random Forest: Accuracy=0.9857, F1=0.9797, AUC=0.9997\n",
            "Linear Discriminant Analysis: Accuracy=0.9190, F1=0.8864, AUC=0.9788\n",
            "Gaussian Process: Accuracy=0.9738, F1=0.9608, AUC=0.9967\n",
            "Stacking Classifier: Accuracy=0.9976, F1=0.9963, AUC=1.0000\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9810, F1=0.9731, AUC=0.9839\n",
            "SVM: Accuracy=0.9690, F1=0.9554, AUC=0.9916\n",
            "Logistic Regression: Accuracy=0.9762, F1=0.9654, AUC=0.9939\n",
            "Random Forest: Accuracy=0.9881, F1=0.9832, AUC=1.0000\n",
            "Linear Discriminant Analysis: Accuracy=0.9190, F1=0.8864, AUC=0.9788\n",
            "Gaussian Process: Accuracy=0.9738, F1=0.9608, AUC=0.9967\n",
            "Stacking Classifier: Accuracy=0.9857, F1=0.9795, AUC=0.9997\n",
            "----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-197bd480e864>\u001b[0m in \u001b[0;36m<cell line: 186>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimized_classifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Probability estimates for AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \"\"\"\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0my_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mclasses_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         ret = _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0m\u001b[1;32m    275\u001b[0m                         equal_nan=equal_nan)\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# main program 9\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_data_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_data_class', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_data_class', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_data_class', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/FeatureEnvy.csv')\n",
        "\n",
        "X = data.drop('is_data_class', axis=1)\n",
        "y = data['is_data_class']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_data_class')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_data_class')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_data_class', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_data_class')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=MLPClassifier(max_iter=2000,activation='relu',solver='adam'))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdtlDK9GCPW",
        "outputId": "a0d82cfb-cd9c-494f-d920-defc10da0358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.8452, F1=0.7396, AUC=0.8132\n",
            "SVM: Accuracy=0.8595, F1=0.7488, AUC=0.9292\n",
            "KNN: Accuracy=0.7905, F1=0.6036, AUC=0.8498\n",
            "Logistic Regression: Accuracy=0.9000, F1=0.8343, AUC=0.9526\n",
            "MLP: Accuracy=0.8762, F1=0.8031, AUC=0.9355\n",
            "Linear Discriminant Analysis: Accuracy=0.8500, F1=0.7381, AUC=0.9196\n",
            "Gaussian Process: Accuracy=0.8548, F1=0.7496, AUC=0.9356\n",
            "Stacking Classifier: Accuracy=0.8905, F1=0.8197, AUC=0.9451\n",
            "----------------------------------------\n",
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.8500, F1=0.7630, AUC=0.8319\n",
            "SVM: Accuracy=0.8667, F1=0.7621, AUC=0.9438\n",
            "KNN: Accuracy=0.8571, F1=0.7488, AUC=0.9149\n",
            "Logistic Regression: Accuracy=0.8952, F1=0.8268, AUC=0.9609\n",
            "MLP: Accuracy=0.8905, F1=0.8223, AUC=0.9580\n",
            "Linear Discriminant Analysis: Accuracy=0.8714, F1=0.7801, AUC=0.9435\n",
            "Gaussian Process: Accuracy=0.8929, F1=0.8271, AUC=0.9567\n",
            "Stacking Classifier: Accuracy=0.8833, F1=0.8063, AUC=0.9466\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.8500, F1=0.7569, AUC=0.8273\n",
            "SVM: Accuracy=0.8548, F1=0.7506, AUC=0.9367\n",
            "KNN: Accuracy=0.8143, F1=0.6612, AUC=0.8918\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7990, AUC=0.9572\n",
            "MLP: Accuracy=0.8714, F1=0.7844, AUC=0.9356\n",
            "Linear Discriminant Analysis: Accuracy=0.8595, F1=0.7624, AUC=0.9383\n",
            "Gaussian Process: Accuracy=0.8690, F1=0.7791, AUC=0.9463\n",
            "Stacking Classifier: Accuracy=0.8762, F1=0.7909, AUC=0.9485\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.8405, F1=0.7450, AUC=0.8186\n",
            "SVM: Accuracy=0.8619, F1=0.7505, AUC=0.9449\n",
            "KNN: Accuracy=0.8548, F1=0.7482, AUC=0.9164\n",
            "Logistic Regression: Accuracy=0.8976, F1=0.8305, AUC=0.9614\n",
            "MLP: Accuracy=0.8881, F1=0.8168, AUC=0.9577\n",
            "Linear Discriminant Analysis: Accuracy=0.8667, F1=0.7694, AUC=0.9444\n",
            "Gaussian Process: Accuracy=0.8952, F1=0.8281, AUC=0.9561\n",
            "Stacking Classifier: Accuracy=0.8786, F1=0.7973, AUC=0.9555\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.8548, F1=0.7657, AUC=0.8287\n",
            "SVM: Accuracy=0.8524, F1=0.7272, AUC=0.9366\n",
            "KNN: Accuracy=0.7952, F1=0.6211, AUC=0.8618\n",
            "Logistic Regression: Accuracy=0.8833, F1=0.8038, AUC=0.9494\n",
            "MLP: Accuracy=0.8905, F1=0.8223, AUC=0.9425\n",
            "Linear Discriminant Analysis: Accuracy=0.8357, F1=0.7053, AUC=0.9271\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7863, AUC=0.9383\n",
            "Stacking Classifier: Accuracy=0.8762, F1=0.7915, AUC=0.9426\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.8571, F1=0.7689, AUC=0.8325\n",
            "SVM: Accuracy=0.8595, F1=0.7488, AUC=0.9292\n",
            "KNN: Accuracy=0.7905, F1=0.6036, AUC=0.8498\n",
            "Logistic Regression: Accuracy=0.9000, F1=0.8343, AUC=0.9529\n",
            "MLP: Accuracy=0.8667, F1=0.7850, AUC=0.9376\n",
            "Linear Discriminant Analysis: Accuracy=0.8500, F1=0.7381, AUC=0.9196\n",
            "Gaussian Process: Accuracy=0.8548, F1=0.7496, AUC=0.9356\n",
            "Stacking Classifier: Accuracy=0.8833, F1=0.8040, AUC=0.9411\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.8548, F1=0.7558, AUC=0.8247\n",
            "SVM: Accuracy=0.8595, F1=0.7488, AUC=0.9289\n",
            "KNN: Accuracy=0.7905, F1=0.6036, AUC=0.8498\n",
            "Logistic Regression: Accuracy=0.9000, F1=0.8343, AUC=0.9526\n",
            "MLP: Accuracy=0.8690, F1=0.7850, AUC=0.9379\n",
            "Linear Discriminant Analysis: Accuracy=0.8500, F1=0.7381, AUC=0.9196\n",
            "Gaussian Process: Accuracy=0.8548, F1=0.7496, AUC=0.9356\n",
            "Stacking Classifier: Accuracy=0.8881, F1=0.8163, AUC=0.9460\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.8548, F1=0.7700, AUC=0.8372\n",
            "SVM: Accuracy=0.8500, F1=0.7311, AUC=0.9340\n",
            "KNN: Accuracy=0.8238, F1=0.6815, AUC=0.8901\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.8120, AUC=0.9521\n",
            "MLP: Accuracy=0.8762, F1=0.7979, AUC=0.9358\n",
            "Linear Discriminant Analysis: Accuracy=0.8405, F1=0.7224, AUC=0.9318\n",
            "Gaussian Process: Accuracy=0.8667, F1=0.7741, AUC=0.9447\n",
            "Stacking Classifier: Accuracy=0.8738, F1=0.7915, AUC=0.9453\n",
            "----------------------------------------\n",
            "     Feature Set                    Classifier  Accuracy        F1       AUC  \\\n",
            "0   All Features                 Decision Tree  0.845238  0.739592  0.813249   \n",
            "1   All Features                           SVM  0.859524  0.748768  0.929192   \n",
            "2   All Features                           KNN  0.790476  0.603624  0.849845   \n",
            "3   All Features           Logistic Regression  0.900000  0.834265  0.952597   \n",
            "4   All Features                           MLP  0.876190  0.803120  0.935546   \n",
            "..           ...                           ...       ...       ...       ...   \n",
            "59  Majority Set           Logistic Regression  0.885714  0.812016  0.952092   \n",
            "60  Majority Set                           MLP  0.876190  0.797915  0.935811   \n",
            "61  Majority Set  Linear Discriminant Analysis  0.840476  0.722449  0.931782   \n",
            "62  Majority Set              Gaussian Process  0.866667  0.774136  0.944665   \n",
            "63  Majority Set           Stacking Classifier  0.873810  0.791464  0.945322   \n",
            "\n",
            "                                    Selected Features  \n",
            "0   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "1   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "2   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "3   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "4   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "..                                                ...  \n",
            "59  [LOC_method, CFNAMM_method, MAXNESTING_method,...  \n",
            "60  [LOC_method, CFNAMM_method, MAXNESTING_method,...  \n",
            "61  [LOC_method, CFNAMM_method, MAXNESTING_method,...  \n",
            "62  [LOC_method, CFNAMM_method, MAXNESTING_method,...  \n",
            "63  [LOC_method, CFNAMM_method, MAXNESTING_method,...  \n",
            "\n",
            "[64 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "# main program 8\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_switch_statements', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/SwitchStatements.csv')\n",
        "\n",
        "X = data.drop('is_switch_statements', axis=1)\n",
        "y = data['is_switch_statements']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_switch_statements')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_switch_statements')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_switch_statements', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_switch_statements')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "# Define parameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [10, 20, 30, None]},\n",
        "    \"SVM\": {\"C\": [0.1, 1, 10, 100], \"gamma\": [\"scale\", \"auto\"], \"kernel\": [\"linear\", \"rbf\"]},\n",
        "    \"KNN\": {\"n_neighbors\": [3, 5, 7], \"weights\": [\"uniform\", \"distance\"]},\n",
        "    \"Logistic Regression\": {\"C\": [0.01, 0.1, 1, 10, 100], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\", \"saga\"]},\n",
        "    \"MLP\": {\"hidden_layer_sizes\": [(50,), (100,), (50,50)], \"activation\": [\"tanh\", \"relu\"], \"solver\": [\"adam\", \"sgd\"]},\n",
        "    \"Linear Discriminant Analysis\": {\"solver\": [\"svd\", \"lsqr\", \"eigen\"]},\n",
        "    \"Gaussian Process\": {\"kernel\": [best_kernel, None]}  # Example parameter grid for GPC\n",
        "}\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "    (\"KNN\", KNeighborsClassifier()),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('MLP', MLPClassifier(max_iter=2000)),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "optimized_classifiers = []\n",
        "for name, clf in base_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grids[name], cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    grid_search.fit(X_normalized, y)\n",
        "    optimized_classifiers.append((name, grid_search.best_estimator_))\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=optimized_classifiers, final_estimator=RandomForestClassifier())\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in optimized_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_QNCVPU05ec",
        "outputId": "27ec9ab7-6582-4d35-cc3c-84b144b58a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.8429, F1=0.7445, AUC=0.8158\n",
            "SVM: Accuracy=0.8595, F1=0.7528, AUC=0.9298\n",
            "KNN: Accuracy=0.7690, F1=0.6040, AUC=0.8224\n",
            "Logistic Regression: Accuracy=0.8548, F1=0.7552, AUC=0.9345\n",
            "MLP: Accuracy=0.8738, F1=0.7926, AUC=0.9318\n",
            "Linear Discriminant Analysis: Accuracy=0.8500, F1=0.7381, AUC=0.9196\n",
            "Gaussian Process: Accuracy=0.8548, F1=0.7496, AUC=0.9356\n",
            "Stacking Classifier: Accuracy=0.8571, F1=0.7498, AUC=0.9249\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.8357, F1=0.7412, AUC=0.8172\n",
            "SVM: Accuracy=0.8929, F1=0.8229, AUC=0.9542\n",
            "KNN: Accuracy=0.8357, F1=0.7227, AUC=0.8907\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.8022, AUC=0.9556\n",
            "MLP: Accuracy=0.8905, F1=0.8242, AUC=0.9569\n",
            "Linear Discriminant Analysis: Accuracy=0.8714, F1=0.7801, AUC=0.9435\n",
            "Gaussian Process: Accuracy=0.8929, F1=0.8271, AUC=0.9567\n",
            "Stacking Classifier: Accuracy=0.8690, F1=0.7850, AUC=0.9398\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.8524, F1=0.7619, AUC=0.8315\n",
            "SVM: Accuracy=0.8667, F1=0.7669, AUC=0.9440\n",
            "KNN: Accuracy=0.8000, F1=0.6353, AUC=0.8464\n",
            "Logistic Regression: Accuracy=0.8714, F1=0.7883, AUC=0.9468\n",
            "MLP: Accuracy=0.8595, F1=0.7680, AUC=0.9346\n",
            "Linear Discriminant Analysis: Accuracy=0.8595, F1=0.7624, AUC=0.9383\n",
            "Gaussian Process: Accuracy=0.8690, F1=0.7791, AUC=0.9463\n",
            "Stacking Classifier: Accuracy=0.8762, F1=0.7924, AUC=0.9427\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.8500, F1=0.7643, AUC=0.8344\n",
            "SVM: Accuracy=0.8952, F1=0.8259, AUC=0.9556\n",
            "KNN: Accuracy=0.8500, F1=0.7330, AUC=0.8715\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.8010, AUC=0.9564\n",
            "MLP: Accuracy=0.8881, F1=0.8187, AUC=0.9585\n",
            "Linear Discriminant Analysis: Accuracy=0.8667, F1=0.7694, AUC=0.9444\n",
            "Gaussian Process: Accuracy=0.8952, F1=0.8281, AUC=0.9561\n",
            "Stacking Classifier: Accuracy=0.8857, F1=0.8145, AUC=0.9521\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.8524, F1=0.7635, AUC=0.8269\n",
            "SVM: Accuracy=0.8714, F1=0.7803, AUC=0.9401\n",
            "KNN: Accuracy=0.7905, F1=0.6377, AUC=0.8408\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7952, AUC=0.9452\n",
            "MLP: Accuracy=0.8881, F1=0.8172, AUC=0.9512\n",
            "Linear Discriminant Analysis: Accuracy=0.8571, F1=0.7509, AUC=0.9395\n",
            "Gaussian Process: Accuracy=0.8738, F1=0.7869, AUC=0.9465\n",
            "Stacking Classifier: Accuracy=0.8833, F1=0.8091, AUC=0.9463\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.8500, F1=0.7501, AUC=0.8209\n",
            "SVM: Accuracy=0.8595, F1=0.7528, AUC=0.9298\n",
            "KNN: Accuracy=0.7690, F1=0.6040, AUC=0.8224\n",
            "Logistic Regression: Accuracy=0.8548, F1=0.7552, AUC=0.9343\n",
            "MLP: Accuracy=0.8571, F1=0.7629, AUC=0.9320\n",
            "Linear Discriminant Analysis: Accuracy=0.8500, F1=0.7381, AUC=0.9196\n",
            "Gaussian Process: Accuracy=0.8548, F1=0.7496, AUC=0.9356\n",
            "Stacking Classifier: Accuracy=0.8524, F1=0.7421, AUC=0.9273\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.8548, F1=0.7588, AUC=0.8244\n",
            "SVM: Accuracy=0.8595, F1=0.7528, AUC=0.9298\n",
            "KNN: Accuracy=0.7690, F1=0.6040, AUC=0.8224\n",
            "Logistic Regression: Accuracy=0.8548, F1=0.7552, AUC=0.9351\n",
            "MLP: Accuracy=0.8738, F1=0.7953, AUC=0.9326\n",
            "Linear Discriminant Analysis: Accuracy=0.8500, F1=0.7381, AUC=0.9196\n",
            "Gaussian Process: Accuracy=0.8548, F1=0.7496, AUC=0.9356\n",
            "Stacking Classifier: Accuracy=0.8667, F1=0.7674, AUC=0.9309\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.8595, F1=0.7722, AUC=0.8365\n",
            "SVM: Accuracy=0.8667, F1=0.7719, AUC=0.9439\n",
            "KNN: Accuracy=0.8167, F1=0.6770, AUC=0.8448\n",
            "Logistic Regression: Accuracy=0.8714, F1=0.7883, AUC=0.9444\n",
            "MLP: Accuracy=0.8762, F1=0.7936, AUC=0.9449\n",
            "Linear Discriminant Analysis: Accuracy=0.8571, F1=0.7542, AUC=0.9353\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7835, AUC=0.9505\n",
            "Stacking Classifier: Accuracy=0.8548, F1=0.7641, AUC=0.9359\n",
            "----------------------------------------\n",
            "     Feature Set                    Classifier  Accuracy        F1       AUC  \\\n",
            "0   All Features                 Decision Tree  0.842857  0.744463  0.815769   \n",
            "1   All Features                           SVM  0.859524  0.752805  0.929761   \n",
            "2   All Features                           KNN  0.769048  0.604001  0.822366   \n",
            "3   All Features           Logistic Regression  0.854762  0.755226  0.934523   \n",
            "4   All Features                           MLP  0.873810  0.792551  0.931782   \n",
            "..           ...                           ...       ...       ...       ...   \n",
            "59  Majority Set           Logistic Regression  0.871429  0.788269  0.944374   \n",
            "60  Majority Set                           MLP  0.876190  0.793603  0.944892   \n",
            "61  Majority Set  Linear Discriminant Analysis  0.857143  0.754152  0.935306   \n",
            "62  Majority Set              Gaussian Process  0.871429  0.783484  0.950538   \n",
            "63  Majority Set           Stacking Classifier  0.854762  0.764072  0.935943   \n",
            "\n",
            "                                    Selected Features  \n",
            "0   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "1   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "2   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "3   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "4   [NOP_method, CC_method, ATFD_method, FDP_metho...  \n",
            "..                                                ...  \n",
            "59  [NOC_type, LOC_method, LOC_package, CINT_metho...  \n",
            "60  [NOC_type, LOC_method, LOC_package, CINT_metho...  \n",
            "61  [NOC_type, LOC_method, LOC_package, CINT_metho...  \n",
            "62  [NOC_type, LOC_method, LOC_package, CINT_metho...  \n",
            "63  [NOC_type, LOC_method, LOC_package, CINT_metho...  \n",
            "\n",
            "[64 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "#main program 7\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
        "best_kernel = C(1.0) * Matern(length_scale=0.5, nu=1.5)\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_switch_statements', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_switch_statements', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_switch_statements', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_switch_statements', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/SwitchStatements.csv')\n",
        "\n",
        "X = data.drop('is_switch_statements', axis=1)\n",
        "y = data['is_switch_statements']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_switch_statements')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_switch_statements')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_switch_statements', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_switch_statements')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(C=100, gamma='auto', kernel='rbf', probability=True)),\n",
        "    (\"KNN\", KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')),\n",
        "    ('Logistic Regression', LogisticRegression(C=100,penalty = 'l1',solver = 'saga')),\n",
        "    ('MLP', MLPClassifier(max_iter=2000)),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier(kernel=best_kernel, random_state=42)),\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=RandomForestClassifier())\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "results_summary = []\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in base_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        results_summary.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Classifier\": name,\n",
        "            \"Accuracy\": avg_accuracy,\n",
        "            \"F1\": avg_f1,\n",
        "            \"AUC\": avg_auc,\n",
        "            \"Selected Features\": features\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Convert the results summary to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "# Display the DataFrame containing the selected features and their evaluation metrics\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbORLWXft8Qz",
        "outputId": "69f3b1fa-0893-450d-9ddb-cf89d0ffe180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8766, AUC=0.9118\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9614\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9405, F1=0.9013, AUC=0.9632\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9214, F1=0.8772, AUC=0.9552\n",
            "----------------------------------------\n",
            "Results for RFECV Features:\n",
            "Decision Tree: Accuracy=0.9071, F1=0.8572, AUC=0.8982\n",
            "SVM: Accuracy=0.9190, F1=0.8567, AUC=0.9712\n",
            "KNN: Accuracy=0.9310, F1=0.8924, AUC=0.9502\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7808, AUC=0.9625\n",
            "MLP: Accuracy=0.9405, F1=0.9029, AUC=0.9587\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8255, AUC=0.9582\n",
            "Gaussian Process: Accuracy=0.8857, F1=0.7818, AUC=0.9582\n",
            "Stacking Classifier: Accuracy=0.9381, F1=0.9038, AUC=0.9605\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9167, F1=0.8775, AUC=0.9081\n",
            "SVM: Accuracy=0.9119, F1=0.8483, AUC=0.9706\n",
            "KNN: Accuracy=0.8500, F1=0.7525, AUC=0.8778\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7874, AUC=0.9676\n",
            "MLP: Accuracy=0.9333, F1=0.8905, AUC=0.9681\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8282, AUC=0.9579\n",
            "Gaussian Process: Accuracy=0.8810, F1=0.7689, AUC=0.9558\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8867, AUC=0.9684\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9214, F1=0.8784, AUC=0.9119\n",
            "SVM: Accuracy=0.9238, F1=0.8685, AUC=0.9659\n",
            "KNN: Accuracy=0.9119, F1=0.8616, AUC=0.9376\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7728, AUC=0.9667\n",
            "MLP: Accuracy=0.9405, F1=0.9034, AUC=0.9651\n",
            "Linear Discriminant Analysis: Accuracy=0.9095, F1=0.8298, AUC=0.9548\n",
            "Gaussian Process: Accuracy=0.8833, F1=0.7774, AUC=0.9603\n",
            "Stacking Classifier: Accuracy=0.9357, F1=0.8988, AUC=0.9659\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8773, AUC=0.9087\n",
            "SVM: Accuracy=0.9238, F1=0.8736, AUC=0.9711\n",
            "KNN: Accuracy=0.8000, F1=0.6804, AUC=0.8325\n",
            "Logistic Regression: Accuracy=0.8881, F1=0.7845, AUC=0.9624\n",
            "MLP: Accuracy=0.9381, F1=0.8961, AUC=0.9713\n",
            "Linear Discriminant Analysis: Accuracy=0.8952, F1=0.8064, AUC=0.9622\n",
            "Gaussian Process: Accuracy=0.8667, F1=0.7369, AUC=0.9427\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8848, AUC=0.9529\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8748, AUC=0.9083\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9614\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9429, F1=0.9033, AUC=0.9669\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9214, F1=0.8733, AUC=0.9588\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8705, AUC=0.9064\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9613\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9452, F1=0.9090, AUC=0.9716\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9429, F1=0.9087, AUC=0.9607\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9048, F1=0.8607, AUC=0.9032\n",
            "SVM: Accuracy=0.9310, F1=0.8856, AUC=0.9717\n",
            "KNN: Accuracy=0.8333, F1=0.7394, AUC=0.8867\n",
            "Logistic Regression: Accuracy=0.8738, F1=0.7607, AUC=0.9637\n",
            "MLP: Accuracy=0.9333, F1=0.8911, AUC=0.9668\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8296, AUC=0.9544\n",
            "Gaussian Process: Accuracy=0.8786, F1=0.7700, AUC=0.9551\n",
            "Stacking Classifier: Accuracy=0.9167, F1=0.8705, AUC=0.9579\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#main program 6\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def select_features_by_rfecv(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear')):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = SFS(estimator, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=StratifiedKFold(5))\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', estimator=LogisticRegression(solver='liblinear'), n_features_to_select=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        all_features.update(results.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for feature, selected in results.items():\n",
        "            if selected:\n",
        "                feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfecv_results = select_features_by_rfecv(data_normalized, target_name='is_long_parameters_list')\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list')\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features_to_select=10)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "\n",
        "results_dicts = [rfecv_results, sfs_results, rfe_results, genetic_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    common_features.update(results.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(results.keys()) for results in results_dicts])\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(C=10, gamma='scale', kernel='linear', probability=True)),\n",
        "    (\"KNN\", KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('MLP', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier()),\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=MLPClassifier(max_iter=2000))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFECV Features\": [feature for feature, selected in rfecv_results.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_results.items() if selected],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_results.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, features in feature_sets.items():\n",
        "    X_selected = X_normalized[features]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in base_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFgSdUnLnHH9",
        "outputId": "4be06f08-631a-4a29-c950-b5e9f9a60cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8679, AUC=0.9028\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9614\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9405, F1=0.9027, AUC=0.9672\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8909, AUC=0.9603\n",
            "----------------------------------------\n",
            "Results for RFE Features with 10 features:\n",
            "Decision Tree: Accuracy=0.9214, F1=0.8778, AUC=0.9117\n",
            "SVM: Accuracy=0.9238, F1=0.8685, AUC=0.9659\n",
            "KNN: Accuracy=0.9119, F1=0.8616, AUC=0.9376\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7728, AUC=0.9667\n",
            "MLP: Accuracy=0.9357, F1=0.8973, AUC=0.9654\n",
            "Linear Discriminant Analysis: Accuracy=0.9095, F1=0.8298, AUC=0.9548\n",
            "Gaussian Process: Accuracy=0.8833, F1=0.7774, AUC=0.9603\n",
            "Stacking Classifier: Accuracy=0.9381, F1=0.9020, AUC=0.9699\n",
            "----------------------------------------\n",
            "Results for RFE Features with 15 features:\n",
            "Decision Tree: Accuracy=0.9119, F1=0.8701, AUC=0.9085\n",
            "SVM: Accuracy=0.9357, F1=0.8916, AUC=0.9750\n",
            "KNN: Accuracy=0.8405, F1=0.7516, AUC=0.8942\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7835, AUC=0.9692\n",
            "MLP: Accuracy=0.9452, F1=0.9114, AUC=0.9750\n",
            "Linear Discriminant Analysis: Accuracy=0.9024, F1=0.8198, AUC=0.9650\n",
            "Gaussian Process: Accuracy=0.8786, F1=0.7690, AUC=0.9566\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8860, AUC=0.9667\n",
            "----------------------------------------\n",
            "Results for RFE Features with 20 features:\n",
            "Decision Tree: Accuracy=0.9000, F1=0.8532, AUC=0.8959\n",
            "SVM: Accuracy=0.9357, F1=0.8922, AUC=0.9768\n",
            "KNN: Accuracy=0.8500, F1=0.7643, AUC=0.8937\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7872, AUC=0.9697\n",
            "MLP: Accuracy=0.9405, F1=0.9011, AUC=0.9716\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8287, AUC=0.9665\n",
            "Gaussian Process: Accuracy=0.8762, F1=0.7660, AUC=0.9569\n",
            "Stacking Classifier: Accuracy=0.9238, F1=0.8838, AUC=0.9627\n",
            "----------------------------------------\n",
            "Results for SFS Features with 10 features:\n",
            "Decision Tree: Accuracy=0.9238, F1=0.8798, AUC=0.9121\n",
            "SVM: Accuracy=0.9167, F1=0.8528, AUC=0.9675\n",
            "KNN: Accuracy=0.8548, F1=0.7616, AUC=0.8933\n",
            "Logistic Regression: Accuracy=0.8881, F1=0.7839, AUC=0.9609\n",
            "MLP: Accuracy=0.9310, F1=0.8832, AUC=0.9670\n",
            "Linear Discriminant Analysis: Accuracy=0.9024, F1=0.8195, AUC=0.9675\n",
            "Gaussian Process: Accuracy=0.8810, F1=0.7694, AUC=0.9555\n",
            "Stacking Classifier: Accuracy=0.9167, F1=0.8666, AUC=0.9641\n",
            "----------------------------------------\n",
            "Results for SFS Features with 15 features:\n",
            "Decision Tree: Accuracy=0.9238, F1=0.8834, AUC=0.9140\n",
            "SVM: Accuracy=0.9167, F1=0.8516, AUC=0.9715\n",
            "KNN: Accuracy=0.8667, F1=0.7862, AUC=0.8963\n",
            "Logistic Regression: Accuracy=0.8881, F1=0.7839, AUC=0.9617\n",
            "MLP: Accuracy=0.9310, F1=0.8842, AUC=0.9637\n",
            "Linear Discriminant Analysis: Accuracy=0.9024, F1=0.8185, AUC=0.9652\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7511, AUC=0.9553\n",
            "Stacking Classifier: Accuracy=0.9167, F1=0.8664, AUC=0.9654\n",
            "----------------------------------------\n",
            "Results for SFS Features with 20 features:\n",
            "Decision Tree: Accuracy=0.9238, F1=0.8860, AUC=0.9154\n",
            "SVM: Accuracy=0.9095, F1=0.8382, AUC=0.9726\n",
            "KNN: Accuracy=0.8571, F1=0.7708, AUC=0.8738\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7885, AUC=0.9650\n",
            "MLP: Accuracy=0.9333, F1=0.8911, AUC=0.9690\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8282, AUC=0.9611\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7522, AUC=0.9553\n",
            "Stacking Classifier: Accuracy=0.9310, F1=0.8904, AUC=0.9757\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9024, F1=0.8519, AUC=0.8927\n",
            "SVM: Accuracy=0.9190, F1=0.8641, AUC=0.9712\n",
            "KNN: Accuracy=0.8357, F1=0.7263, AUC=0.8657\n",
            "Logistic Regression: Accuracy=0.8833, F1=0.7748, AUC=0.9602\n",
            "MLP: Accuracy=0.9405, F1=0.8991, AUC=0.9653\n",
            "Linear Discriminant Analysis: Accuracy=0.8881, F1=0.7922, AUC=0.9485\n",
            "Gaussian Process: Accuracy=0.8667, F1=0.7393, AUC=0.9448\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8857, AUC=0.9669\n",
            "----------------------------------------\n",
            "Results for New Wrapper Features with 10 features:\n",
            "Decision Tree: Accuracy=0.9071, F1=0.8599, AUC=0.8981\n",
            "SVM: Accuracy=0.9167, F1=0.8546, AUC=0.9726\n",
            "KNN: Accuracy=0.9000, F1=0.8440, AUC=0.9411\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7684, AUC=0.9586\n",
            "MLP: Accuracy=0.9476, F1=0.9151, AUC=0.9782\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8263, AUC=0.9661\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7558, AUC=0.9537\n",
            "Stacking Classifier: Accuracy=0.9333, F1=0.8951, AUC=0.9755\n",
            "----------------------------------------\n",
            "Results for New Wrapper Features with 15 features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8756, AUC=0.9068\n",
            "SVM: Accuracy=0.9167, F1=0.8564, AUC=0.9703\n",
            "KNN: Accuracy=0.8738, F1=0.7902, AUC=0.9194\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7737, AUC=0.9573\n",
            "MLP: Accuracy=0.9429, F1=0.9053, AUC=0.9684\n",
            "Linear Discriminant Analysis: Accuracy=0.9000, F1=0.8141, AUC=0.9573\n",
            "Gaussian Process: Accuracy=0.8738, F1=0.7608, AUC=0.9537\n",
            "Stacking Classifier: Accuracy=0.9310, F1=0.8914, AUC=0.9752\n",
            "----------------------------------------\n",
            "Results for New Wrapper Features with 20 features:\n",
            "Decision Tree: Accuracy=0.9214, F1=0.8816, AUC=0.9138\n",
            "SVM: Accuracy=0.9167, F1=0.8536, AUC=0.9721\n",
            "KNN: Accuracy=0.8667, F1=0.7749, AUC=0.8891\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7756, AUC=0.9552\n",
            "MLP: Accuracy=0.9476, F1=0.9119, AUC=0.9737\n",
            "Linear Discriminant Analysis: Accuracy=0.8952, F1=0.8040, AUC=0.9578\n",
            "Gaussian Process: Accuracy=0.8595, F1=0.7343, AUC=0.9456\n",
            "Stacking Classifier: Accuracy=0.9333, F1=0.8977, AUC=0.9716\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8691, AUC=0.9047\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9613\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9405, F1=0.9009, AUC=0.9678\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9071, F1=0.8570, AUC=0.9527\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9167, F1=0.8747, AUC=0.9083\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9614\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9405, F1=0.9031, AUC=0.9722\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8846, AUC=0.9614\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9119, F1=0.8666, AUC=0.9010\n",
            "SVM: Accuracy=0.9119, F1=0.8570, AUC=0.9690\n",
            "KNN: Accuracy=0.7905, F1=0.6608, AUC=0.8188\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7776, AUC=0.9661\n",
            "MLP: Accuracy=0.9381, F1=0.8990, AUC=0.9709\n",
            "Linear Discriminant Analysis: Accuracy=0.8976, F1=0.8157, AUC=0.9516\n",
            "Gaussian Process: Accuracy=0.8571, F1=0.7201, AUC=0.9328\n",
            "Stacking Classifier: Accuracy=0.9381, F1=0.9032, AUC=0.9688\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#main program 3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features_list=[10, 15, 20]):\n",
        "    results = {}\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    estimator = LogisticRegression(solver='liblinear')\n",
        "    for n_features in n_features_list:\n",
        "        selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "        selector = selector.fit(X, y)\n",
        "        results[n_features] = dict(zip(X.columns, selector.support_))\n",
        "    return results\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features_list=[10, 15, 20]):\n",
        "    results = {}\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    estimator = LogisticRegression(solver='liblinear')\n",
        "    for n_features in n_features_list:\n",
        "        selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "        selector = selector.fit(X, y)\n",
        "        results[n_features] = dict(zip(X.columns, selector.support_))\n",
        "    return results\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(results_dicts):\n",
        "    all_features = set()\n",
        "    for results in results_dicts:\n",
        "        for features in results.values():\n",
        "            all_features.update(features.keys())\n",
        "\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "    for results in results_dicts:\n",
        "        for features in results.values():\n",
        "            for feature, selected in features.items():\n",
        "                if selected:\n",
        "                    feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= len(results_dicts) // 2]\n",
        "    return majority_features\n",
        "\n",
        "def select_features_by_new_wrapper(data, target_name='is_long_parameters_list', n_features_list=[10, 15, 20]):\n",
        "    results = {}\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    estimator = RandomForestClassifier()\n",
        "    for n_features in n_features_list:\n",
        "        selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "        selector = selector.fit(X, y)\n",
        "        results[n_features] = dict(zip(X.columns, selector.support_))\n",
        "    return results\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "n_features_list = [10, 15, 20]\n",
        "\n",
        "rfe_results = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features_list=n_features_list)\n",
        "sfs_results = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list', n_features_list=n_features_list)\n",
        "genetic_results = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "new_wrapper_results = select_features_by_new_wrapper(data_normalized, target_name='is_long_parameters_list', n_features_list=n_features_list)\n",
        "\n",
        "results_dicts = [rfe_results, sfs_results, {20: genetic_results}, new_wrapper_results]\n",
        "majority_voting_features = majority_voting_features(results_dicts)\n",
        "\n",
        "common_features = set()\n",
        "for results in results_dicts:\n",
        "    for features in results.values():\n",
        "        common_features.update(features.keys())\n",
        "\n",
        "intersection_features = set.intersection(*[set(features.keys()) for results in results_dicts for features in results.values()])\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    (\"SVM\", SVC(C=10, gamma='scale', kernel='linear', probability=True)),\n",
        "    (\"KNN\", KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    ('MLP', RandomForestClassifier()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier()),\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=MLPClassifier(max_iter=2000))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"RFE Features\": {n: [feature for feature, selected in features.items() if selected] for n, features in rfe_results.items()},\n",
        "    \"SFS Features\": {n: [feature for feature, selected in features.items() if selected] for n, features in sfs_results.items()},\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_results.items() if selected],\n",
        "    \"New Wrapper Features\": {n: [feature for feature, selected in features.items() if selected] for n, features in new_wrapper_results.items()},\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, feature_set in feature_sets.items():\n",
        "    if isinstance(feature_set, dict):\n",
        "        for n_features, features in feature_set.items():\n",
        "            X_selected = X_normalized[features]\n",
        "            metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "            metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "            for train_index, test_index in skf.split(X_selected, y):\n",
        "                X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "                for name, clf in base_classifiers:\n",
        "                    clf.fit(X_train, y_train)\n",
        "                    y_pred = clf.predict(X_test)\n",
        "                    y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "                    metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "                    metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "                    metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "                # Stacking Classifier\n",
        "                stacking_classifier.fit(X_train, y_train)\n",
        "                y_pred = stacking_classifier.predict(X_test)\n",
        "                y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "                metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "                metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "                metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "            print(f\"Results for {feature_set_name} with {n_features} features:\")\n",
        "\n",
        "            for name, scores in metrics.items():\n",
        "                avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "                avg_f1 = np.mean(scores[\"f1\"])\n",
        "                avg_auc = np.mean(scores[\"auc\"])\n",
        "                print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "            print(\"-\" * 40)\n",
        "    else:\n",
        "        X_selected = X_normalized[feature_set]\n",
        "        metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "        metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "        for train_index, test_index in skf.split(X_selected, y):\n",
        "            X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "            for name, clf in base_classifiers:\n",
        "                clf.fit(X_train, y_train)\n",
        "                y_pred = clf.predict(X_test)\n",
        "                y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "                metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "                metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "                metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "            # Stacking Classifier\n",
        "            stacking_classifier.fit(X_train, y_train)\n",
        "            y_pred = stacking_classifier.predict(X_test)\n",
        "            y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "        for name, scores in metrics.items():\n",
        "            avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "            avg_f1 = np.mean(scores[\"f1\"])\n",
        "            avg_auc = np.mean(scores[\"auc\"])\n",
        "            print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "        print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC9ugE3N1njn",
        "outputId": "68b776e5-a592-48c4-a2fa-8f0898ff6c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9214, F1=0.8817, AUC=0.9137\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9614\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9405, F1=0.8996, AUC=0.9665\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9381, F1=0.9028, AUC=0.9658\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9000, F1=0.8512, AUC=0.8940\n",
            "SVM: Accuracy=0.9357, F1=0.8922, AUC=0.9768\n",
            "KNN: Accuracy=0.8500, F1=0.7643, AUC=0.8937\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7872, AUC=0.9697\n",
            "MLP: Accuracy=0.9357, F1=0.8938, AUC=0.9725\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8287, AUC=0.9665\n",
            "Gaussian Process: Accuracy=0.8762, F1=0.7660, AUC=0.9569\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8862, AUC=0.9620\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9095, F1=0.8675, AUC=0.9034\n",
            "SVM: Accuracy=0.9095, F1=0.8382, AUC=0.9726\n",
            "KNN: Accuracy=0.8571, F1=0.7708, AUC=0.8738\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7885, AUC=0.9650\n",
            "MLP: Accuracy=0.9286, F1=0.8806, AUC=0.9692\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8282, AUC=0.9611\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7522, AUC=0.9553\n",
            "Stacking Classifier: Accuracy=0.9357, F1=0.8965, AUC=0.9757\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9071, F1=0.8587, AUC=0.8976\n",
            "SVM: Accuracy=0.9333, F1=0.8914, AUC=0.9671\n",
            "KNN: Accuracy=0.8119, F1=0.6856, AUC=0.8206\n",
            "Logistic Regression: Accuracy=0.8881, F1=0.7867, AUC=0.9591\n",
            "MLP: Accuracy=0.9357, F1=0.8963, AUC=0.9703\n",
            "Linear Discriminant Analysis: Accuracy=0.9024, F1=0.8207, AUC=0.9519\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7509, AUC=0.9416\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8845, AUC=0.9697\n",
            "----------------------------------------\n",
            "Results for New Wrapper Features:\n",
            "Decision Tree: Accuracy=0.9214, F1=0.8799, AUC=0.9103\n",
            "SVM: Accuracy=0.9190, F1=0.8585, AUC=0.9709\n",
            "KNN: Accuracy=0.8595, F1=0.7696, AUC=0.8865\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7740, AUC=0.9563\n",
            "MLP: Accuracy=0.9500, F1=0.9166, AUC=0.9745\n",
            "Linear Discriminant Analysis: Accuracy=0.9000, F1=0.8136, AUC=0.9566\n",
            "Gaussian Process: Accuracy=0.8619, F1=0.7343, AUC=0.9455\n",
            "Stacking Classifier: Accuracy=0.9333, F1=0.8957, AUC=0.9758\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9119, F1=0.8668, AUC=0.9029\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9614\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9429, F1=0.9060, AUC=0.9714\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8848, AUC=0.9638\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8763, AUC=0.9083\n",
            "SVM: Accuracy=0.9048, F1=0.8485, AUC=0.9613\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "MLP: Accuracy=0.9429, F1=0.9048, AUC=0.9714\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9357, F1=0.8975, AUC=0.9631\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9238, F1=0.8837, AUC=0.9110\n",
            "SVM: Accuracy=0.9071, F1=0.8347, AUC=0.9697\n",
            "KNN: Accuracy=0.8952, F1=0.8383, AUC=0.9175\n",
            "Logistic Regression: Accuracy=0.8833, F1=0.7766, AUC=0.9578\n",
            "MLP: Accuracy=0.9405, F1=0.9036, AUC=0.9647\n",
            "Linear Discriminant Analysis: Accuracy=0.9119, F1=0.8346, AUC=0.9674\n",
            "Gaussian Process: Accuracy=0.8810, F1=0.7736, AUC=0.9532\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8878, AUC=0.9662\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#main program 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "  if np.sum(solution) == 0:\n",
        "    return 0\n",
        "  X_selected = X[:, solution == 1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "  model = LogisticRegression(max_iter=1000)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "  return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "  parents = []\n",
        "  for _ in range(len(population)):\n",
        "    i, j = np.random.choice(len(population), 2, replace=False)\n",
        "    if fitness_scores[i] > fitness_scores[j]:\n",
        "      parents.append(population[i])\n",
        "    else:\n",
        "      parents.append(population[j])\n",
        "  return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "  offspring = []\n",
        "  for i in range(0, len(parents), 2):\n",
        "    if i + 1 < len(parents):\n",
        "      crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "      parent1, parent2 = parents[i], parents[i + 1]\n",
        "      child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "      child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "      offspring.extend([child1, child2])\n",
        "  return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "  for individual in offspring:\n",
        "    for gene in range(len(individual)):\n",
        "      if random.random() < mutation_rate:\n",
        "        individual[gene] = 1 - individual[gene]\n",
        "  return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "  X = data.drop(target_name, axis=1).values\n",
        "  y = data[target_name].values\n",
        "  population = initialize_population(pop_size, X.shape[1])\n",
        "  for generation in range(n_generations):\n",
        "    fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "    parents = select_parents(population, fitness_scores)\n",
        "    offspring = crossover(parents)\n",
        "    population = mutate(offspring, mutation_rate)\n",
        "  best_solution = population[np.argmax(fitness_scores)]\n",
        "  return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(rfe_scores, sfs_scores, genetic_scores, new_wrapper_scores):\n",
        "  all_features = set(rfe_scores.keys()).union(sfs_scores.keys()).union(genetic_scores.keys()).union(new_wrapper_scores.keys())\n",
        "  feature_count = {feature: 0 for feature in all_features}\n",
        "  for feature in rfe_scores:\n",
        "    if rfe_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in sfs_scores:\n",
        "    if sfs_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in genetic_scores:\n",
        "    if genetic_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in new_wrapper_scores:\n",
        "    if new_wrapper_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  majority_features = [feature for feature, count in feature_count.items() if count >= 3]\n",
        "  return majority_features\n",
        "\n",
        "def select_features_by_new_wrapper(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = RandomForestClassifier()\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfe_scores = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "sfs_scores = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "genetic_scores = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "new_wrapper_scores = select_features_by_new_wrapper(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "\n",
        "intersection_features = set(rfe_scores.keys()).intersection(set(sfs_scores.keys()), set(genetic_scores.keys()), set(new_wrapper_scores.keys()))\n",
        "majority_voting_features = majority_voting_features(rfe_scores, sfs_scores, genetic_scores, new_wrapper_scores)\n",
        "\n",
        "common_features = set(rfe_scores.keys()).union(set(sfs_scores.keys())).union(set(genetic_scores.keys())).union(set(new_wrapper_scores.keys()))\n",
        "\n",
        "base_classifiers = [\n",
        "  (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "\n",
        "  (\"SVM\", SVC(C= 10, gamma= 'scale', kernel= 'linear', probability=True)),\n",
        "  (\"KNN\", KNeighborsClassifier(n_neighbors=3,weights='uniform',   algorithm='auto',  leaf_size=30,   p=2,   metric='minkowski', metric_params=None,    n_jobs=None)),\n",
        "  ('Logistic Regression', LogisticRegression()),\n",
        "    ('MLP', RandomForestClassifier()),\n",
        "  (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "  (\"Gaussian Process\", GaussianProcessClassifier()),\n",
        "\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=(MLPClassifier(max_iter=2000)))\n",
        "\n",
        "feature_sets = {\n",
        "  \"All Features\": X_normalized.columns.tolist(),\n",
        "  \"RFE Features\": [feature for feature, selected in rfe_scores.items() if selected],\n",
        "  \"SFS Features\": [feature for feature, selected in sfs_scores.items() if selected],\n",
        "  \"Genetic Features\": [feature for feature, selected in genetic_scores.items() if selected],\n",
        "  \"New Wrapper Features\": [feature for feature, selected in new_wrapper_scores.items() if selected],\n",
        "  \"Union of All Features\": list(common_features),\n",
        "  \"Intersection Set\": list(intersection_features),\n",
        "  \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, feature_set in feature_sets.items():\n",
        "  X_selected = X_normalized[feature_set]\n",
        "  metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "  metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "  for train_index, test_index in skf.split(X_selected, y):\n",
        "    X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    for name, clf in base_classifiers:\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_pred = clf.predict(X_test)\n",
        "      y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "      metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "      metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "      metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    # Stacking Classifier\n",
        "    stacking_classifier.fit(X_train, y_train)\n",
        "    y_pred = stacking_classifier.predict(X_test)\n",
        "    y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "    metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "    metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "  print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "  for name, scores in metrics.items():\n",
        "    avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "    avg_f1 = np.mean(scores[\"f1\"])\n",
        "    avg_auc = np.mean(scores[\"auc\"])\n",
        "    print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "  print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaxKZgy-1SPu",
        "outputId": "f1694979-33b0-4abe-baef-71b67ba49a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9095, F1=0.8619, AUC=0.8993\n",
            "Random Forest: Accuracy=0.9405, F1=0.9002, AUC=0.9693\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7667, F1=0.6352, AUC=0.7284\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9333, F1=0.8937, AUC=0.9642\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9024, F1=0.8537, AUC=0.8957\n",
            "Random Forest: Accuracy=0.9381, F1=0.8999, AUC=0.9722\n",
            "SVM: Accuracy=0.8357, F1=0.6592, AUC=0.9438\n",
            "KNN: Accuracy=0.8429, F1=0.7566, AUC=0.8185\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7872, AUC=0.9697\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8287, AUC=0.9665\n",
            "Gaussian Process: Accuracy=0.8762, F1=0.7660, AUC=0.9569\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8861, AUC=0.9756\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9262, F1=0.8901, AUC=0.9194\n",
            "Random Forest: Accuracy=0.9333, F1=0.8906, AUC=0.9672\n",
            "SVM: Accuracy=0.8167, F1=0.6005, AUC=0.9304\n",
            "KNN: Accuracy=0.8452, F1=0.7689, AUC=0.8275\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7885, AUC=0.9650\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8282, AUC=0.9611\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7522, AUC=0.9553\n",
            "Stacking Classifier: Accuracy=0.9310, F1=0.8892, AUC=0.9660\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9167, F1=0.8749, AUC=0.9083\n",
            "Random Forest: Accuracy=0.9429, F1=0.9064, AUC=0.9708\n",
            "SVM: Accuracy=0.8333, F1=0.6520, AUC=0.9424\n",
            "KNN: Accuracy=0.7762, F1=0.6535, AUC=0.7416\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7912, AUC=0.9604\n",
            "Linear Discriminant Analysis: Accuracy=0.8952, F1=0.8102, AUC=0.9547\n",
            "Gaussian Process: Accuracy=0.8571, F1=0.7091, AUC=0.9375\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8862, AUC=0.9717\n",
            "----------------------------------------\n",
            "Results for New Wrapper Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8760, AUC=0.9085\n",
            "Random Forest: Accuracy=0.9476, F1=0.9128, AUC=0.9757\n",
            "SVM: Accuracy=0.8548, F1=0.7155, AUC=0.9231\n",
            "KNN: Accuracy=0.8286, F1=0.7420, AUC=0.8037\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7689, AUC=0.9570\n",
            "Linear Discriminant Analysis: Accuracy=0.8952, F1=0.8040, AUC=0.9551\n",
            "Gaussian Process: Accuracy=0.8643, F1=0.7425, AUC=0.9463\n",
            "Stacking Classifier: Accuracy=0.9429, F1=0.9110, AUC=0.9715\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8771, AUC=0.9100\n",
            "Random Forest: Accuracy=0.9381, F1=0.8958, AUC=0.9660\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7667, F1=0.6352, AUC=0.7284\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8885, AUC=0.9660\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8695, AUC=0.9048\n",
            "Random Forest: Accuracy=0.9452, F1=0.9079, AUC=0.9716\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7667, F1=0.6352, AUC=0.7284\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9381, F1=0.9053, AUC=0.9673\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9286, F1=0.8927, AUC=0.9251\n",
            "Random Forest: Accuracy=0.9310, F1=0.8899, AUC=0.9694\n",
            "SVM: Accuracy=0.8643, F1=0.7327, AUC=0.9435\n",
            "KNN: Accuracy=0.8595, F1=0.7848, AUC=0.8406\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7920, AUC=0.9606\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8263, AUC=0.9634\n",
            "Gaussian Process: Accuracy=0.8857, F1=0.7802, AUC=0.9581\n",
            "Stacking Classifier: Accuracy=0.9143, F1=0.8655, AUC=0.9607\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#programm 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "  if np.sum(solution) == 0:\n",
        "    return 0\n",
        "  X_selected = X[:, solution == 1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "  model = LogisticRegression(max_iter=1000)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "  return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "  parents = []\n",
        "  for _ in range(len(population)):\n",
        "    i, j = np.random.choice(len(population), 2, replace=False)\n",
        "    if fitness_scores[i] > fitness_scores[j]:\n",
        "      parents.append(population[i])\n",
        "    else:\n",
        "      parents.append(population[j])\n",
        "  return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "  offspring = []\n",
        "  for i in range(0, len(parents), 2):\n",
        "    if i + 1 < len(parents):\n",
        "      crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "      parent1, parent2 = parents[i], parents[i + 1]\n",
        "      child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "      child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "      offspring.extend([child1, child2])\n",
        "  return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "  for individual in offspring:\n",
        "    for gene in range(len(individual)):\n",
        "      if random.random() < mutation_rate:\n",
        "        individual[gene] = 1 - individual[gene]\n",
        "  return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "  X = data.drop(target_name, axis=1).values\n",
        "  y = data[target_name].values\n",
        "  population = initialize_population(pop_size, X.shape[1])\n",
        "  for generation in range(n_generations):\n",
        "    fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "    parents = select_parents(population, fitness_scores)\n",
        "    offspring = crossover(parents)\n",
        "    population = mutate(offspring, mutation_rate)\n",
        "  best_solution = population[np.argmax(fitness_scores)]\n",
        "  return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(rfe_scores, sfs_scores, genetic_scores, new_wrapper_scores):\n",
        "  all_features = set(rfe_scores.keys()).union(sfs_scores.keys()).union(genetic_scores.keys()).union(new_wrapper_scores.keys())\n",
        "  feature_count = {feature: 0 for feature in all_features}\n",
        "  for feature in rfe_scores:\n",
        "    if rfe_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in sfs_scores:\n",
        "    if sfs_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in genetic_scores:\n",
        "    if genetic_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in new_wrapper_scores:\n",
        "    if new_wrapper_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  majority_features = [feature for feature, count in feature_count.items() if count >= 3]\n",
        "  return majority_features\n",
        "\n",
        "def select_features_by_new_wrapper(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = RandomForestClassifier()\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfe_scores = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "sfs_scores = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "genetic_scores = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "new_wrapper_scores = select_features_by_new_wrapper(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "\n",
        "intersection_features = set(rfe_scores.keys()).intersection(set(sfs_scores.keys()), set(genetic_scores.keys()), set(new_wrapper_scores.keys()))\n",
        "majority_voting_features = majority_voting_features(rfe_scores, sfs_scores, genetic_scores, new_wrapper_scores)\n",
        "\n",
        "common_features = set(rfe_scores.keys()).union(set(sfs_scores.keys())).union(set(genetic_scores.keys())).union(set(new_wrapper_scores.keys()))\n",
        "\n",
        "base_classifiers = [\n",
        "  (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "  ('Random Forest', RandomForestClassifier()),\n",
        "  (\"SVM\", SVC(C=0.1, gamma='scale', kernel='poly', degree=3, probability=True)),\n",
        "  (\"KNN\", KNeighborsClassifier(n_neighbors=1)),\n",
        "  ('Logistic Regression', LogisticRegression()),\n",
        "  (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "  (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=MLPClassifier(max_iter=2000))\n",
        "\n",
        "feature_sets = {\n",
        "  \"All Features\": X_normalized.columns.tolist(),\n",
        "  \"RFE Features\": [feature for feature, selected in rfe_scores.items() if selected],\n",
        "  \"SFS Features\": [feature for feature, selected in sfs_scores.items() if selected],\n",
        "  \"Genetic Features\": [feature for feature, selected in genetic_scores.items() if selected],\n",
        "  \"New Wrapper Features\": [feature for feature, selected in new_wrapper_scores.items() if selected],\n",
        "  \"Union of All Features\": list(common_features),\n",
        "  \"Intersection Set\": list(intersection_features),\n",
        "  \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, feature_set in feature_sets.items():\n",
        "  X_selected = X_normalized[feature_set]\n",
        "  metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "  metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "  for train_index, test_index in skf.split(X_selected, y):\n",
        "    X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    for name, clf in base_classifiers:\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_pred = clf.predict(X_test)\n",
        "      y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "      metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "      metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "      metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    # Stacking Classifier\n",
        "    stacking_classifier.fit(X_train, y_train)\n",
        "    y_pred = stacking_classifier.predict(X_test)\n",
        "    y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "    metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "    metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "  print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "  for name, scores in metrics.items():\n",
        "    avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "    avg_f1 = np.mean(scores[\"f1\"])\n",
        "    avg_auc = np.mean(scores[\"auc\"])\n",
        "    print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "  print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExMCyJvrqmN3",
        "outputId": "22408fae-2877-43ba-a483-5d022f37a920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9095, F1=0.8640, AUC=0.9029\n",
            "Random Forest: Accuracy=0.9381, F1=0.8980, AUC=0.9665\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9381, F1=0.9017, AUC=0.9665\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9024, F1=0.8557, AUC=0.8979\n",
            "Random Forest: Accuracy=0.9357, F1=0.8948, AUC=0.9710\n",
            "SVM: Accuracy=0.8357, F1=0.6592, AUC=0.9438\n",
            "KNN: Accuracy=0.8500, F1=0.7643, AUC=0.8937\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7872, AUC=0.9697\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8287, AUC=0.9665\n",
            "Gaussian Process: Accuracy=0.8762, F1=0.7660, AUC=0.9569\n",
            "Stacking Classifier: Accuracy=0.9238, F1=0.8802, AUC=0.9635\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9167, F1=0.8704, AUC=0.9049\n",
            "Random Forest: Accuracy=0.9452, F1=0.9097, AUC=0.9692\n",
            "SVM: Accuracy=0.8357, F1=0.6649, AUC=0.9503\n",
            "KNN: Accuracy=0.8619, F1=0.7722, AUC=0.8865\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7702, AUC=0.9604\n",
            "Linear Discriminant Analysis: Accuracy=0.9024, F1=0.8196, AUC=0.9546\n",
            "Gaussian Process: Accuracy=0.8619, F1=0.7328, AUC=0.9542\n",
            "Stacking Classifier: Accuracy=0.9405, F1=0.9072, AUC=0.9716\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9071, F1=0.8637, AUC=0.8997\n",
            "Random Forest: Accuracy=0.9429, F1=0.9069, AUC=0.9687\n",
            "SVM: Accuracy=0.8310, F1=0.6439, AUC=0.9475\n",
            "KNN: Accuracy=0.7952, F1=0.6477, AUC=0.8241\n",
            "Logistic Regression: Accuracy=0.8833, F1=0.7747, AUC=0.9624\n",
            "Linear Discriminant Analysis: Accuracy=0.9024, F1=0.8185, AUC=0.9601\n",
            "Gaussian Process: Accuracy=0.8690, F1=0.7432, AUC=0.9462\n",
            "Stacking Classifier: Accuracy=0.9167, F1=0.8632, AUC=0.9624\n",
            "----------------------------------------\n",
            "Results for New Wrapper Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8750, AUC=0.9067\n",
            "Random Forest: Accuracy=0.9452, F1=0.9090, AUC=0.9727\n",
            "SVM: Accuracy=0.8619, F1=0.7374, AUC=0.9294\n",
            "KNN: Accuracy=0.8714, F1=0.7862, AUC=0.9084\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7694, AUC=0.9562\n",
            "Linear Discriminant Analysis: Accuracy=0.8976, F1=0.8095, AUC=0.9555\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7567, AUC=0.9509\n",
            "Stacking Classifier: Accuracy=0.9357, F1=0.8986, AUC=0.9714\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8723, AUC=0.9083\n",
            "Random Forest: Accuracy=0.9405, F1=0.9005, AUC=0.9644\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9067\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9238, F1=0.8822, AUC=0.9686\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9095, F1=0.8659, AUC=0.9047\n",
            "Random Forest: Accuracy=0.9381, F1=0.8952, AUC=0.9732\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7690, F1=0.6258, AUC=0.8054\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9333, F1=0.8942, AUC=0.9635\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.8738, F1=0.8181, AUC=0.8713\n",
            "Random Forest: Accuracy=0.9333, F1=0.8931, AUC=0.9629\n",
            "SVM: Accuracy=0.8810, F1=0.7781, AUC=0.9466\n",
            "KNN: Accuracy=0.8976, F1=0.8382, AUC=0.9346\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7720, AUC=0.9665\n",
            "Linear Discriminant Analysis: Accuracy=0.9143, F1=0.8393, AUC=0.9605\n",
            "Gaussian Process: Accuracy=0.8786, F1=0.7677, AUC=0.9648\n",
            "Stacking Classifier: Accuracy=0.9310, F1=0.8874, AUC=0.9678\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#main program\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "  if np.sum(solution) == 0:\n",
        "    return 0\n",
        "  X_selected = X[:, solution == 1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "  model = LogisticRegression(max_iter=1000)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "  return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "  parents = []\n",
        "  for _ in range(len(population)):\n",
        "    i, j = np.random.choice(len(population), 2, replace=False)\n",
        "    if fitness_scores[i] > fitness_scores[j]:\n",
        "      parents.append(population[i])\n",
        "    else:\n",
        "      parents.append(population[j])\n",
        "  return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "  offspring = []\n",
        "  for i in range(0, len(parents), 2):\n",
        "    if i + 1 < len(parents):\n",
        "      crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "      parent1, parent2 = parents[i], parents[i + 1]\n",
        "      child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "      child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "      offspring.extend([child1, child2])\n",
        "  return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "  for individual in offspring:\n",
        "    for gene in range(len(individual)):\n",
        "      if random.random() < mutation_rate:\n",
        "        individual[gene] = 1 - individual[gene]\n",
        "  return offspring\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "  X = data.drop(target_name, axis=1).values\n",
        "  y = data[target_name].values\n",
        "  population = initialize_population(pop_size, X.shape[1])\n",
        "  for generation in range(n_generations):\n",
        "    fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "    parents = select_parents(population, fitness_scores)\n",
        "    offspring = crossover(parents)\n",
        "    population = mutate(offspring, mutation_rate)\n",
        "  best_solution = population[np.argmax(fitness_scores)]\n",
        "  return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "def majority_voting_features(rfe_scores, sfs_scores, genetic_scores, new_wrapper_scores):\n",
        "  all_features = set(rfe_scores.keys()).union(sfs_scores.keys()).union(genetic_scores.keys()).union(new_wrapper_scores.keys())\n",
        "  feature_count = {feature: 0 for feature in all_features}\n",
        "  for feature in rfe_scores:\n",
        "    if rfe_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in sfs_scores:\n",
        "    if sfs_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in genetic_scores:\n",
        "    if genetic_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  for feature in new_wrapper_scores:\n",
        "    if new_wrapper_scores[feature]:\n",
        "      feature_count[feature] += 1\n",
        "  majority_features = [feature for feature, count in feature_count.items() if count >= 3]\n",
        "  return majority_features\n",
        "\n",
        "def select_features_by_new_wrapper(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = RandomForestClassifier()\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "rfe_scores = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "sfs_scores = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "genetic_scores = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "new_wrapper_scores = select_features_by_new_wrapper(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "\n",
        "intersection_features = set(rfe_scores.keys()).intersection(set(sfs_scores.keys()), set(genetic_scores.keys()), set(new_wrapper_scores.keys()))\n",
        "majority_voting_features = majority_voting_features(rfe_scores, sfs_scores, genetic_scores, new_wrapper_scores)\n",
        "\n",
        "common_features = set(rfe_scores.keys()).union(set(sfs_scores.keys())).union(set(genetic_scores.keys())).union(set(new_wrapper_scores.keys()))\n",
        "\n",
        "base_classifiers = [\n",
        "  (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "  ('Random Forest', RandomForestClassifier()),\n",
        "  (\"SVM\", SVC(C=0.1, gamma='scale', kernel='poly', degree=3, probability=True)),\n",
        "  (\"KNN\", KNeighborsClassifier(n_neighbors=3,weights='uniform',   algorithm='auto',  leaf_size=30,   p=2,   metric='minkowski', metric_params=None,    n_jobs=None)),\n",
        "  ('Logistic Regression', LogisticRegression()),\n",
        "  (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "  (\"Gaussian Process\", GaussianProcessClassifier()),\n",
        "  (\"MLP\",MLPClassifier(max_iter=2000))\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=MLPClassifier(max_iter=2000))\n",
        "\n",
        "feature_sets = {\n",
        "  \"All Features\": X_normalized.columns.tolist(),\n",
        "  \"RFE Features\": [feature for feature, selected in rfe_scores.items() if selected],\n",
        "  \"SFS Features\": [feature for feature, selected in sfs_scores.items() if selected],\n",
        "  \"Genetic Features\": [feature for feature, selected in genetic_scores.items() if selected],\n",
        "  \"New Wrapper Features\": [feature for feature, selected in new_wrapper_scores.items() if selected],\n",
        "  \"Union of All Features\": list(common_features),\n",
        "  \"Intersection Set\": list(intersection_features),\n",
        "  \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, feature_set in feature_sets.items():\n",
        "  X_selected = X_normalized[feature_set]\n",
        "  metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "  metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "  for train_index, test_index in skf.split(X_selected, y):\n",
        "    X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    for name, clf in base_classifiers:\n",
        "      clf.fit(X_train, y_train)\n",
        "      y_pred = clf.predict(X_test)\n",
        "      y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "      metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "      metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "      metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    # Stacking Classifier\n",
        "    stacking_classifier.fit(X_train, y_train)\n",
        "    y_pred = stacking_classifier.predict(X_test)\n",
        "    y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "    metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "    metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "  print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "  for name, scores in metrics.items():\n",
        "    avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "    avg_f1 = np.mean(scores[\"f1\"])\n",
        "    avg_auc = np.mean(scores[\"auc\"])\n",
        "    print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "  print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VwkF_MJRaeL",
        "outputId": "ee39d757-c19e-4f8d-ecee-d1ceab5cf5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.8328912466843501\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9442970822281167\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7000000000000001, AUC: 0.9496021220159151\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.4761904761904762, AUC: 0.6657824933687002\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9549071618037135\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9018567639257294\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.631578947368421, AUC: 0.9310344827586208\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9363395225464192\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8275862068965517, AUC: 0.8925729442970822\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.9600000000000001, AUC: 0.9973474801061009\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9681697612732095\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.6666666666666666, AUC: 0.8342175066312998\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9787798408488064\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9628647214854111\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666667, AUC: 0.9336870026525199\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9708222811671088\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.875\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9477040816326531\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9387755102040817\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9183673469387755\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9285714285714286\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9413265306122449\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8647959183673469\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9566326530612245\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.989795918367347\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.45454545454545453, AUC: 0.7818877551020408\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9897959183673469\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9821428571428572\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9464285714285715\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9948979591836735\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9617346938775511\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.8775510204081634\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9770408163265306\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9795918367346939\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9719387755102041\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9107142857142857\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9693877551020409\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.56, AUC: 0.8418367346938775\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9311224489795918\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.7857142857142857, F1: 0.7096774193548386, AUC: 0.7857142857142856\n",
            "Rg - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9145408163265306\n",
            "SVM - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8290816326530611\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.5925925925925927, AUC: 0.8367346938775511\n",
            "Logistic Regression - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.8622448979591837\n",
            "Linear Discriminant Analysis - Accuracy: 0.6904761904761905, F1: 0.4799999999999999, AUC: 0.8443877551020407\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5263157894736842, AUC: 0.8214285714285714\n",
            "Stacking Classifier - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9489795918367347\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.896551724137931, AUC: 0.9285714285714286\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9400510204081632\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9846938775510204\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.5217391304347826, AUC: 0.760204081632653\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9489795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9387755102040816\n",
            "GPC - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9464285714285714\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.9362244897959183\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9821428571428571\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6153846153846153, AUC: 0.8188775510204083\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9770408163265306\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9948979591836735\n",
            "\n",
            "All Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8571428571428571\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.931122448979592\n",
            "SVM - Accuracy: 0.7619047619047619, F1: 0.5, AUC: 0.9081632653061225\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.5384615384615384, AUC: 0.7283163265306123\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9617346938775511\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9668367346938775\n",
            "GPC - Accuracy: 0.7380952380952381, F1: 0.4210526315789473, AUC: 0.8469387755102041\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9540816326530612\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9095238095238096, Average F1: 0.8649199965773935, Average AUC: 0.9011178476695718\n",
            "\n",
            "Rg - Average Accuracy: 0.9452380952380952, Average F1: 0.9090370370370371, Average AUC: 0.9675062929681155\n",
            "\n",
            "SVM - Average Accuracy: 0.8476190476190478, Average F1: 0.7017146621494448, Average AUC: 0.9481547393493207\n",
            "\n",
            "KNN - Average Accuracy: 0.7595238095238095, Average F1: 0.5832987881683532, Average AUC: 0.8063775510204081\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8690476190476192, Average F1: 0.755855072463768, Average AUC: 0.9602054349591296\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8785714285714287, Average F1: 0.7870528891398456, Average AUC: 0.951268066908461\n",
            "\n",
            "GPC - Average Accuracy: 0.819047619047619, Average F1: 0.6311970142748173, Average AUC: 0.9170843934390731\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.930952380952381, Average F1: 0.8896524216524219, Average AUC: 0.9692876089427813\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.850132625994695\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9204244031830239\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.761904761904762, AUC: 0.960212201591512\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7500000000000001, AUC: 0.8899204244031831\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9336870026525199\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9045092838196287\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.761904761904762, AUC: 0.9522546419098143\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.8249336870026526\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8275862068965517, AUC: 0.8925729442970822\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.986737400530504\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.7199999999999999, AUC: 0.9442970822281167\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6666666666666667, AUC: 0.8554376657824934\n",
            "Logistic Regression - Accuracy: 0.8571428571428571, F1: 0.7500000000000001, AUC: 0.9655172413793104\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.9166666666666666, AUC: 0.9787798408488063\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9442970822281167\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9920424403183024\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7692307692307692, AUC: 0.8214285714285715\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9400510204081634\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.8903061224489797\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.5833333333333334, AUC: 0.8341836734693878\n",
            "Logistic Regression - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9158163265306123\n",
            "Linear Discriminant Analysis - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9311224489795917\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.875\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.923469387755102\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9821428571428572\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.8533163265306123\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9770408163265306\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9770408163265306\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9642857142857143\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9821428571428572\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9209183673469388\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9974489795918368\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9948979591836735\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9107142857142857\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9719387755102041\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.8137755102040817\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9923469387755102\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9693877551020409\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9719387755102041\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.8095238095238095, F1: 0.7333333333333334, AUC: 0.8035714285714285\n",
            "Rg - Accuracy: 0.8571428571428571, F1: 0.7857142857142857, AUC: 0.9413265306122449\n",
            "SVM - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8826530612244897\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.8852040816326531\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8954081632653061\n",
            "Linear Discriminant Analysis - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8622448979591837\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8903061224489797\n",
            "Stacking Classifier - Accuracy: 0.8095238095238095, F1: 0.7142857142857143, AUC: 0.931122448979592\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.8095238095238095, F1: 0.7333333333333334, AUC: 0.8035714285714285\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9094387755102041\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9617346938775511\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.5384615384615384, AUC: 0.8086734693877551\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.923469387755102\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.943877551020408\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9183673469387755\n",
            "Stacking Classifier - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9515306122448979\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.896551724137931, AUC: 0.9285714285714286\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9948979591836735\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.8533163265306123\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9974489795918368\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 1.0\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9974489795918368\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Gain Ratio Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.8928571428571428\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9489795918367346\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9311224489795917\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.6086956521739131, AUC: 0.7793367346938775\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9107142857142858\n",
            "Linear Discriminant Analysis - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9617346938775511\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5263157894736842, AUC: 0.8852040816326531\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9464285714285714\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.8928571428571429, Average F1: 0.8395590922487475, Average AUC: 0.8831991284577491\n",
            "\n",
            "Rg - Average Accuracy: 0.9428571428571428, Average F1: 0.9054603174603175, Average AUC: 0.9646957722080873\n",
            "\n",
            "SVM - Average Accuracy: 0.854761904761905, Average F1: 0.7263214756258234, Average AUC: 0.9501448059329831\n",
            "\n",
            "KNN - Average Accuracy: 0.8, Average F1: 0.6530456853935116, Average AUC: 0.8494082579981594\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.880952380952381, Average F1: 0.7778677409546975, Average AUC: 0.9508898121582853\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8880952380952382, Average F1: 0.7933992673992674, Average AUC: 0.9523595247117415\n",
            "\n",
            "GPC - Average Accuracy: 0.8571428571428573, Average F1: 0.7241620774343887, Average AUC: 0.9394000703729768\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.930952380952381, Average F1: 0.8905549801411871, Average AUC: 0.9569527147729119\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.8156498673740052\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9204244031830239\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666667, AUC: 0.8806366047745358\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.8289124668435013\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9124668435013263\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9124668435013262\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9018567639257294\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9283819628647215\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.8333333333333334, F1: 0.7741935483870968, AUC: 0.8580901856763927\n",
            "Rg - Accuracy: 0.9523809523809523, F1: 0.9230769230769231, AUC: 0.9920424403183024\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9814323607427056\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.7824933687002653\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9867374005305041\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9655172413793103\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9761273209549071\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9920424403183024\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9336734693877552\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9005102040816326\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6153846153846153, AUC: 0.854591836734694\n",
            "Logistic Regression - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9285714285714286\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9464285714285714\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8979591836734694\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9591836734693877\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9285714285714286, AUC: 0.9464285714285715\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.979591836734694\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6666666666666666, AUC: 0.8813775510204083\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9795918367346939\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9923469387755103\n",
            "GPC - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9770408163265306\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9974489795918368\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9795918367346939\n",
            "KNN - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9387755102040816\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.989795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.989795918367347\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9846938775510203\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9285714285714286\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9897959183673469\n",
            "KNN - Accuracy: 0.9523809523809523, F1: 0.9285714285714286, AUC: 0.9642857142857143\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9897959183673469\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9668367346938777\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9872448979591837\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.8333333333333334, F1: 0.7586206896551724, AUC: 0.8214285714285714\n",
            "Rg - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9489795918367347\n",
            "SVM - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8724489795918368\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.56, AUC: 0.7589285714285714\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8954081632653061\n",
            "Linear Discriminant Analysis - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9132653061224489\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8673469387755103\n",
            "Stacking Classifier - Accuracy: 0.8333333333333334, F1: 0.7586206896551724, AUC: 0.9285714285714286\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.8333333333333334, F1: 0.7586206896551724, AUC: 0.8214285714285714\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9260204081632654\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9413265306122449\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.8290816326530612\n",
            "Logistic Regression - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.9260204081632653\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9489795918367346\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.923469387755102\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.9464285714285714\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9285714285714286, AUC: 0.9464285714285715\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9897959183673469\n",
            "KNN - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9375\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9923469387755103\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "\n",
            "Mutual Information Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8571428571428571\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9681122448979591\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9489795918367347\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8073979591836735\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9158163265306123\n",
            "Linear Discriminant Analysis - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9591836734693877\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8852040816326531\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.951530612244898\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.8928571428571429, Average F1: 0.8392727504852088, Average AUC: 0.8816597195907541\n",
            "\n",
            "Rg - Average Accuracy: 0.9428571428571428, Average F1: 0.9053447293447295, Average AUC: 0.9689252557787041\n",
            "\n",
            "SVM - Average Accuracy: 0.8690476190476192, Average F1: 0.7529243510113075, Average AUC: 0.9464109781843775\n",
            "\n",
            "KNN - Average Accuracy: 0.8357142857142857, Average F1: 0.7107400425661294, Average AUC: 0.858334461105397\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8738095238095239, Average F1: 0.7619296645383602, Average AUC: 0.9519102203215504\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.9023809523809524, Average F1: 0.8187505827505828, Average AUC: 0.9589718778758188\n",
            "\n",
            "GPC - Average Accuracy: 0.8642857142857144, Average F1: 0.7413399209486166, Average AUC: 0.9393290207329615\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.9357142857142857, Average F1: 0.8984137931034484, Average AUC: 0.9703587668489148\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.8885941644562334\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9124668435013263\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.761904761904762, AUC: 0.9575596816976127\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.5833333333333334, AUC: 0.7970822281167109\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9549071618037135\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9177718832891246\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.761904761904762, AUC: 0.9310344827586208\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.9098143236074271\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.896551724137931, AUC: 0.9482758620689655\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.8799999999999999, AUC: 0.986737400530504\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9787798408488063\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.5833333333333334, AUC: 0.7572944297082229\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9761273209549072\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7500000000000001, AUC: 0.9734748010610079\n",
            "GPC - Accuracy: 0.8571428571428571, F1: 0.7272727272727274, AUC: 0.9389920424403182\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.896551724137931, AUC: 0.9946949602122016\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.875\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9387755102040817\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9438775510204082\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.6666666666666666, AUC: 0.8571428571428572\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9336734693877551\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9413265306122449\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8979591836734694\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9183673469387755\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9974489795918368\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9821428571428572\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.45454545454545453, AUC: 0.7946428571428572\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9897959183673469\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.989795918367347\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9719387755102041\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9795918367346939\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8954081632653061\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9923469387755103\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9872448979591837\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9821428571428571\n",
            "Stacking Classifier - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 1.0\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 0.9999999999999999\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9617346938775511\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.56, AUC: 0.8303571428571429\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9846938775510203\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9617346938775511\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.8928571428571429\n",
            "Rg - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.9413265306122449\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8214285714285714\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6428571428571429, AUC: 0.7959183673469388\n",
            "Logistic Regression - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8826530612244897\n",
            "Linear Discriminant Analysis - Accuracy: 0.7380952380952381, F1: 0.5217391304347826, AUC: 0.8520408163265306\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5263157894736842, AUC: 0.8494897959183674\n",
            "Stacking Classifier - Accuracy: 0.8333333333333334, F1: 0.7586206896551724, AUC: 0.9413265306122449\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9107142857142857\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9375\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9897959183673469\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6153846153846153, AUC: 0.7997448979591837\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9438775510204082\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9464285714285714\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9540816326530612\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.8724489795918366\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9770408163265306\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.64, AUC: 0.8737244897959183\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 1.0\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9821428571428572\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "Spearman Correlation Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9553571428571429\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9132653061224489\n",
            "KNN - Accuracy: 0.6904761904761905, F1: 0.4799999999999999, AUC: 0.673469387755102\n",
            "Logistic Regression - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9515306122448979\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.971938775510204\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8673469387755102\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9770408163265306\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9357142857142857, Average F1: 0.8996960127994612, Average AUC: 0.9283298597953771\n",
            "\n",
            "Rg - Average Accuracy: 0.9380952380952381, Average F1: 0.8949743589743591, Average AUC: 0.9669612407297137\n",
            "\n",
            "SVM - Average Accuracy: 0.8690476190476192, Average F1: 0.7459058912102391, Average AUC: 0.9505217073566827\n",
            "\n",
            "KNN - Average Accuracy: 0.7571428571428571, Average F1: 0.5826120546120546, Average AUC: 0.8074784821090241\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8785714285714287, Average F1: 0.773734526343222, Average AUC: 0.962491203377903\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8880952380952382, Average F1: 0.7988670315192056, Average AUC: 0.9564716072105235\n",
            "\n",
            "GPC - Average Accuracy: 0.8476190476190478, Average F1: 0.6967798944000316, Average AUC: 0.9336863259892816\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.9238095238095239, Average F1: 0.8811857746340506, Average AUC: 0.9613692957289016\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.8328912466843501\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9230769230769231\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9655172413793104\n",
            "KNN - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9336870026525199\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9681697612732095\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9416445623342176\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9549071618037136\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.8806366047745358\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.8125000000000001, AUC: 0.896551724137931\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9946949602122015\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9920424403183024\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7857142857142856, AUC: 0.8461538461538463\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9814323607427056\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.9166666666666666, AUC: 0.9973474801061009\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9628647214854111\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8387096774193548, AUC: 0.9920424403183024\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.8928571428571428\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9642857142857143\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9311224489795917\n",
            "KNN - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8941326530612245\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9336734693877552\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9387755102040816\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9081632653061225\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9668367346938775\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9948979591836735\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6923076923076924, AUC: 0.923469387755102\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9846938775510204\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9846938775510204\n",
            "Stacking Classifier - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 1.0\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 0.9999999999999999\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9872448979591837\n",
            "KNN - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9744897959183674\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9897959183673469\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9923469387755103\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9948979591836735\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9196428571428571\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 1.0\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9948979591836735\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.8333333333333334, F1: 0.7741935483870968, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.7857142857142857, F1: 0.6666666666666666, AUC: 0.9183673469387755\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8647959183673468\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.7333333333333334, AUC: 0.8061224489795918\n",
            "Logistic Regression - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8826530612244897\n",
            "Linear Discriminant Analysis - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.8596938775510204\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8494897959183674\n",
            "Stacking Classifier - Accuracy: 0.8333333333333334, F1: 0.7586206896551724, AUC: 0.9336734693877551\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8571428571428571\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9566326530612246\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9719387755102041\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.5714285714285714, AUC: 0.8048469387755102\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9693877551020408\n",
            "Linear Discriminant Analysis - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9566326530612245\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9566326530612245\n",
            "Stacking Classifier - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9821428571428571\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9974489795918368\n",
            "KNN - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9502551020408163\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "RFE Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8275862068965518, AUC: 0.8750000000000001\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9617346938775511\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9387755102040816\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6153846153846153, AUC: 0.8762755102040816\n",
            "Logistic Regression - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9693877551020409\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9821428571428572\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8979591836734694\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9285714285714285\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9142857142857144, Average F1: 0.8744992005995899, Average AUC: 0.9122300113679425\n",
            "\n",
            "Rg - Average Accuracy: 0.9357142857142857, Average F1: 0.8935555555555557, Average AUC: 0.9718792291452389\n",
            "\n",
            "SVM - Average Accuracy: 0.8761904761904763, Average F1: 0.7656666666666666, Average AUC: 0.9638682130677205\n",
            "\n",
            "KNN - Average Accuracy: 0.8380952380952381, Average F1: 0.7453724053724053, Average AUC: 0.8929075542683919\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8785714285714287, Average F1: 0.7745651739564783, Average AUC: 0.9694500081199589\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8976190476190476, Average F1: 0.8145719063545152, Average AUC: 0.9645624695501542\n",
            "\n",
            "GPC - Average Accuracy: 0.8714285714285716, Average F1: 0.7566429512516468, Average AUC: 0.9501955556758512\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.930952380952381, Average F1: 0.8944898890505119, Average AUC: 0.9683903534888756\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.850132625994695\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9084880636604775\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9177718832891246\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7199999999999999, AUC: 0.8143236074270558\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9389920424403182\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9283819628647215\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9336870026525199\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9442970822281167\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9098143236074271\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9893899204244032\n",
            "SVM - Accuracy: 0.9523809523809523, F1: 0.9166666666666666, AUC: 0.9973474801061009\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.8832891246684351\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.986737400530504\n",
            "Linear Discriminant Analysis - Accuracy: 0.9761904761904762, F1: 0.9600000000000001, AUC: 0.9973474801061009\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.986737400530504\n",
            "Stacking Classifier - Accuracy: 0.9523809523809523, F1: 0.9230769230769231, AUC: 0.9893899204244031\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.8928571428571428\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9770408163265306\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9285714285714286\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6923076923076924, AUC: 0.8711734693877551\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9438775510204082\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9413265306122449\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9158163265306123\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9591836734693878\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9744897959183674\n",
            "KNN - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.8839285714285714\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9821428571428572\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9846938775510204\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9744897959183674\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9974489795918368\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9795918367346939\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9477040816326531\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9872448979591837\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9846938775510203\n",
            "GPC - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.989795918367347\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9285714285714286\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 1.0\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.8660714285714286\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9974489795918368\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9872448979591837\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.8333333333333334, F1: 0.7741935483870968, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.9336734693877551\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8928571428571429\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.7333333333333334, AUC: 0.8596938775510204\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9285714285714285\n",
            "Linear Discriminant Analysis - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8979591836734694\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9209183673469388\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9234693877551021\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8571428571428571\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9553571428571429\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9438775510204082\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6666666666666666, AUC: 0.8341836734693877\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9489795918367346\n",
            "Linear Discriminant Analysis - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9209183673469388\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9438775510204082\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9591836734693877\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9897959183673469\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7999999999999999, AUC: 0.9247448979591837\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9974489795918368\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9948979591836735\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "SFS Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8275862068965518, AUC: 0.8750000000000001\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9591836734693878\n",
            "SVM - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9489795918367347\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6666666666666666, AUC: 0.8775510204081634\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9566326530612246\n",
            "Linear Discriminant Analysis - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9540816326530612\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9158163265306123\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9515306122448979\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9261904761904762, Average F1: 0.8876814350318243, Average AUC: 0.9152804092459265\n",
            "\n",
            "Rg - Average Accuracy: 0.9333333333333333, Average F1: 0.888021978021978, Average AUC: 0.9723133086125697\n",
            "\n",
            "SVM - Average Accuracy: 0.8857142857142858, Average F1: 0.7820671936758893, Average AUC: 0.957328262870135\n",
            "\n",
            "KNN - Average Accuracy: 0.8357142857142856, Average F1: 0.7271130029390899, Average AUC: 0.8762663752503654\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8904761904761905, Average F1: 0.7872919833789398, Average AUC: 0.9668076381746331\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.9023809523809524, Average F1: 0.8221570120700555, Average AUC: 0.9604300871542251\n",
            "\n",
            "GPC - Average Accuracy: 0.8833333333333334, Average F1: 0.7735382588426065, Average AUC: 0.9563281546040165\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.9404761904761905, Average F1: 0.8995527065527066, Average AUC: 0.9724503329183133\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8713527851458887\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9137931034482759\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.761904761904762, AUC: 0.883289124668435\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.608695652173913, AUC: 0.8183023872679045\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9018567639257294\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.920424403183024\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.8912466843501327\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.8461538461538463\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.927055702917772\n",
            "Rg - Accuracy: 0.9523809523809523, F1: 0.9230769230769231, AUC: 0.9960212201591512\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 1.0\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.8554376657824934\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9973474801061009\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.9166666666666666, AUC: 0.9814323607427056\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9920424403183025\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.8125000000000001, AUC: 0.9814323607427056\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.8571428571428572\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9553571428571429\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.903061224489796\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7857142857142857, AUC: 0.8698979591836734\n",
            "Logistic Regression - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9362244897959184\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9744897959183674\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.8903061224489797\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9617346938775511\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8666666666666666, AUC: 0.9107142857142858\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9974489795918368\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9846938775510204\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.6086956521739131, AUC: 0.8316326530612244\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9846938775510204\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9795918367346939\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9744897959183674\n",
            "Stacking Classifier - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9693877551020408\n",
            "KNN - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9757653061224489\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.989795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9617346938775511\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9872448979591837\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9897959183673469\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.9094387755102041\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.951530612244898\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9872448979591837\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7999999999999999, AUC: 0.8571428571428572\n",
            "Rg - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.9489795918367347\n",
            "SVM - Accuracy: 0.7380952380952381, F1: 0.5217391304347826, AUC: 0.8545918367346939\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.689655172413793, AUC: 0.8571428571428571\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8545918367346939\n",
            "Linear Discriminant Analysis - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.8571428571428572\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8418367346938775\n",
            "Stacking Classifier - Accuracy: 0.8095238095238095, F1: 0.6923076923076924, AUC: 0.9107142857142857\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.8928571428571429\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9477040816326531\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9464285714285714\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.56, AUC: 0.802295918367347\n",
            "Logistic Regression - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9311224489795918\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.923469387755102\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9362244897959184\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9285714285714286\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "KNN - Accuracy: 0.8571428571428571, F1: 0.7857142857142857, AUC: 0.9081632653061225\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9974489795918368\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "genetic Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9528061224489797\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9591836734693878\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.5833333333333334, AUC: 0.8558673469387755\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9540816326530612\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9591836734693877\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9362244897959183\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9362244897959184\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9214285714285715, Average F1: 0.8791881043605182, Average AUC: 0.9119837059492232\n",
            "\n",
            "Rg - Average Accuracy: 0.9404761904761905, Average F1: 0.8998223794085863, Average AUC: 0.9712110241974774\n",
            "\n",
            "SVM - Average Accuracy: 0.8666666666666668, Average F1: 0.7538083945040468, Average AUC: 0.9490431981811293\n",
            "\n",
            "KNN - Average Accuracy: 0.8166666666666667, Average F1: 0.6917460555436568, Average AUC: 0.868394413468305\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8785714285714287, Average F1: 0.7672049255092734, Average AUC: 0.953951036648081\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8952380952380953, Average F1: 0.8098197696023783, Average AUC: 0.9503897580252261\n",
            "\n",
            "GPC - Average Accuracy: 0.8761904761904763, Average F1: 0.7610856389986824, Average AUC: 0.9434309532831702\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.9214285714285715, Average F1: 0.874121794871795, Average AUC: 0.955972906403941\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.850132625994695\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9190981432360743\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7000000000000001, AUC: 0.9496021220159151\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.4761904761904762, AUC: 0.6657824933687002\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9549071618037135\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9018567639257294\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.631578947368421, AUC: 0.9310344827586208\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9310344827586208\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8275862068965517, AUC: 0.8925729442970822\n",
            "Rg - Accuracy: 0.9523809523809523, F1: 0.9166666666666666, AUC: 0.980106100795756\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9681697612732095\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.6666666666666666, AUC: 0.8342175066312998\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9787798408488064\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9628647214854111\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666667, AUC: 0.9336870026525199\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9681697612732095\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.8928571428571429\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9426020408163265\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9387755102040817\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9183673469387755\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9285714285714286\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9413265306122449\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8647959183673469\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.9183673469387755\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.989795918367347\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.45454545454545453, AUC: 0.7818877551020408\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9897959183673469\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9821428571428572\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9464285714285715\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9617346938775511\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.8775510204081634\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9770408163265306\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9795918367346939\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9719387755102041\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9107142857142857\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9693877551020409\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.56, AUC: 0.8418367346938775\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9311224489795918\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7857142857142857, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.8095238095238095, F1: 0.7333333333333334, AUC: 0.9081632653061225\n",
            "SVM - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8290816326530611\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.5925925925925927, AUC: 0.8367346938775511\n",
            "Logistic Regression - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.8622448979591837\n",
            "Linear Discriminant Analysis - Accuracy: 0.6904761904761905, F1: 0.4799999999999999, AUC: 0.8443877551020407\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5263157894736842, AUC: 0.8214285714285714\n",
            "Stacking Classifier - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9183673469387756\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9285714285714285\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9846938775510204\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.5217391304347826, AUC: 0.760204081632653\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9489795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9387755102040816\n",
            "GPC - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9464285714285714\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.7857142857142857, AUC: 0.9438775510204082\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9821428571428571\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6153846153846153, AUC: 0.8188775510204083\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9770408163265306\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9821428571428572\n",
            "\n",
            "Union of All Features:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9107142857142857\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.951530612244898\n",
            "SVM - Accuracy: 0.7619047619047619, F1: 0.5, AUC: 0.9081632653061225\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.5384615384615384, AUC: 0.7283163265306123\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9617346938775511\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9668367346938775\n",
            "GPC - Accuracy: 0.7380952380952381, F1: 0.4210526315789473, AUC: 0.8469387755102041\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9464285714285714\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9261904761904762, Average F1: 0.8877850757161102, Average AUC: 0.9189134141720349\n",
            "\n",
            "Rg - Average Accuracy: 0.9380952380952381, Average F1: 0.9002592592592593, Average AUC: 0.9630071590970605\n",
            "\n",
            "SVM - Average Accuracy: 0.8476190476190478, Average F1: 0.7017146621494448, Average AUC: 0.9481547393493207\n",
            "\n",
            "KNN - Average Accuracy: 0.7595238095238095, Average F1: 0.5832987881683532, Average AUC: 0.8063775510204081\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8690476190476192, Average F1: 0.755855072463768, Average AUC: 0.9602054349591296\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8785714285714287, Average F1: 0.7870528891398456, Average AUC: 0.951268066908461\n",
            "\n",
            "GPC - Average Accuracy: 0.819047619047619, Average F1: 0.6311970142748173, Average AUC: 0.9170843934390731\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.9214285714285715, Average F1: 0.8778744754606823, Average AUC: 0.9608387917501219\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.8328912466843501\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9283819628647216\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7000000000000001, AUC: 0.9496021220159151\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.4761904761904762, AUC: 0.6657824933687002\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9549071618037135\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9018567639257294\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.631578947368421, AUC: 0.9310344827586208\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.7692307692307693, AUC: 0.9363395225464191\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8275862068965517, AUC: 0.8925729442970822\n",
            "Rg - Accuracy: 0.9761904761904762, F1: 0.9600000000000001, AUC: 0.9893899204244032\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9681697612732095\n",
            "KNN - Accuracy: 0.7857142857142857, F1: 0.6666666666666666, AUC: 0.8342175066312998\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9787798408488064\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9628647214854111\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666667, AUC: 0.9336870026525199\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9708222811671088\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.8928571428571429\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9668367346938775\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9387755102040817\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9183673469387755\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.9285714285714286\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9413265306122449\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8647959183673469\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.9617346938775511\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.9333333333333333, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.989795918367347\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.45454545454545453, AUC: 0.7818877551020408\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9897959183673469\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8571428571428571, AUC: 0.9821428571428572\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9464285714285715\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.989795918367347\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9617346938775511\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.8775510204081634\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9770408163265306\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9795918367346939\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9719387755102041\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9107142857142857\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9693877551020409\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.56, AUC: 0.8418367346938775\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9311224489795918\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.7857142857142857, F1: 0.689655172413793, AUC: 0.7678571428571429\n",
            "Rg - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9145408163265307\n",
            "SVM - Accuracy: 0.7857142857142857, F1: 0.5714285714285714, AUC: 0.8290816326530611\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.5925925925925927, AUC: 0.8367346938775511\n",
            "Logistic Regression - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.8622448979591837\n",
            "Linear Discriminant Analysis - Accuracy: 0.6904761904761905, F1: 0.4799999999999999, AUC: 0.8443877551020407\n",
            "GPC - Accuracy: 0.7857142857142857, F1: 0.5263157894736842, AUC: 0.8214285714285714\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.9591836734693877\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.9285714285714286, F1: 0.896551724137931, AUC: 0.9285714285714286\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9451530612244898\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9846938775510204\n",
            "KNN - Accuracy: 0.7380952380952381, F1: 0.5217391304347826, AUC: 0.760204081632653\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9489795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9387755102040816\n",
            "GPC - Accuracy: 0.8571428571428571, F1: 0.75, AUC: 0.9464285714285714\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.913265306122449\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9642857142857143\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9821428571428571\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6153846153846153, AUC: 0.8188775510204083\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 1.0\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9770408163265306\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.962962962962963, AUC: 0.9923469387755103\n",
            "\n",
            "Instertion Set:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7692307692307692, AUC: 0.8214285714285715\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.951530612244898\n",
            "SVM - Accuracy: 0.7619047619047619, F1: 0.5, AUC: 0.9081632653061225\n",
            "KNN - Accuracy: 0.7142857142857143, F1: 0.5384615384615384, AUC: 0.7283163265306123\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9617346938775511\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9668367346938775\n",
            "GPC - Accuracy: 0.7380952380952381, F1: 0.4210526315789473, AUC: 0.8469387755102041\n",
            "Stacking Classifier - Accuracy: 0.8571428571428571, F1: 0.7692307692307692, AUC: 0.9030612244897959\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9047619047619048, Average F1: 0.8560099925617166, Average AUC: 0.895760704812429\n",
            "\n",
            "Rg - Average Accuracy: 0.9523809523809523, Average F1: 0.9186666666666667, Average AUC: 0.9695833107778921\n",
            "\n",
            "SVM - Average Accuracy: 0.8476190476190478, Average F1: 0.7017146621494448, Average AUC: 0.9481547393493207\n",
            "\n",
            "KNN - Average Accuracy: 0.7595238095238095, Average F1: 0.5832987881683532, Average AUC: 0.8063775510204081\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8690476190476192, Average F1: 0.755855072463768, Average AUC: 0.9602054349591296\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.8785714285714287, Average F1: 0.7870528891398456, Average AUC: 0.951268066908461\n",
            "\n",
            "GPC - Average Accuracy: 0.819047619047619, Average F1: 0.6311970142748173, Average AUC: 0.9170843934390731\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.9214285714285715, Average F1: 0.8762831038693107, Average AUC: 0.962654955881557\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.850132625994695\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9058355437665783\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.8992042440318303\n",
            "KNN - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.8435013262599469\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9257294429708223\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9363395225464191\n",
            "GPC - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9257294429708223\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9522546419098143\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8387096774193548, AUC: 0.9137931034482758\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9893899204244032\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.8695652173913044, AUC: 0.9946949602122016\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7199999999999999, AUC: 0.8514588859416446\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8181818181818181, AUC: 0.9920424403183025\n",
            "Linear Discriminant Analysis - Accuracy: 0.9761904761904762, F1: 0.9600000000000001, AUC: 0.9787798408488064\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.7826086956521738, AUC: 0.9787798408488063\n",
            "Stacking Classifier - Accuracy: 0.9285714285714286, F1: 0.896551724137931, AUC: 0.9893899204244032\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.8571428571428572\n",
            "Rg - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9770408163265306\n",
            "SVM - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9362244897959183\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.8852040816326532\n",
            "Logistic Regression - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9285714285714286\n",
            "Linear Discriminant Analysis - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9515306122448979\n",
            "GPC - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9209183673469388\n",
            "Stacking Classifier - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9668367346938775\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.979591836734694\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.7200000000000001, AUC: 0.8992346938775511\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.989795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.9285714285714286, F1: 0.888888888888889, AUC: 0.9897959183673469\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9821428571428572\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9821428571428572\n",
            "KNN - Accuracy: 0.9523809523809523, F1: 0.9285714285714286, AUC: 0.9757653061224489\n",
            "Logistic Regression - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9974489795918368\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9872448979591837\n",
            "GPC - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9285714285714286\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 1.0\n",
            "KNN - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9821428571428571\n",
            "Logistic Regression - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 1.0\n",
            "Linear Discriminant Analysis - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9846938775510203\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 1.0\n",
            "Stacking Classifier - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.8571428571428571, F1: 0.7857142857142857, AUC: 0.8392857142857143\n",
            "Rg - Accuracy: 0.8571428571428571, F1: 0.7692307692307692, AUC: 0.9362244897959184\n",
            "SVM - Accuracy: 0.7619047619047619, F1: 0.5454545454545454, AUC: 0.854591836734694\n",
            "KNN - Accuracy: 0.7619047619047619, F1: 0.6428571428571429, AUC: 0.8048469387755102\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9081632653061225\n",
            "Linear Discriminant Analysis - Accuracy: 0.8095238095238095, F1: 0.6363636363636364, AUC: 0.8954081632653061\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.8877551020408163\n",
            "Stacking Classifier - Accuracy: 0.8333333333333334, F1: 0.7407407407407408, AUC: 0.9617346938775511\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8571428571428571\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9323979591836734\n",
            "SVM - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9336734693877551\n",
            "KNN - Accuracy: 0.8095238095238095, F1: 0.6923076923076924, AUC: 0.8839285714285714\n",
            "Logistic Regression - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9413265306122449\n",
            "Linear Discriminant Analysis - Accuracy: 0.8809523809523809, F1: 0.782608695652174, AUC: 0.9413265306122449\n",
            "GPC - Accuracy: 0.8809523809523809, F1: 0.8, AUC: 0.9336734693877551\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8461538461538461, AUC: 0.9413265306122449\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 0.9821428571428572\n",
            "Rg - Accuracy: 1.0, F1: 1.0, AUC: 1.0\n",
            "SVM - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9795918367346939\n",
            "KNN - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9528061224489796\n",
            "Logistic Regression - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9948979591836735\n",
            "Linear Discriminant Analysis - Accuracy: 0.9523809523809523, F1: 0.923076923076923, AUC: 0.9948979591836735\n",
            "GPC - Accuracy: 0.9285714285714286, F1: 0.88, AUC: 0.9948979591836735\n",
            "Stacking Classifier - Accuracy: 0.9761904761904762, F1: 0.9655172413793104, AUC: 1.0\n",
            "\n",
            "magority set:\n",
            "Decision Tree - Accuracy: 0.8809523809523809, F1: 0.8148148148148148, AUC: 0.8571428571428571\n",
            "Rg - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9234693877551019\n",
            "SVM - Accuracy: 0.8333333333333334, F1: 0.6666666666666666, AUC: 0.9642857142857142\n",
            "KNN - Accuracy: 0.8333333333333334, F1: 0.6956521739130435, AUC: 0.9107142857142858\n",
            "Logistic Regression - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9489795918367347\n",
            "Linear Discriminant Analysis - Accuracy: 0.8571428571428571, F1: 0.7272727272727273, AUC: 0.9693877551020409\n",
            "GPC - Accuracy: 0.8095238095238095, F1: 0.6, AUC: 0.9234693877551021\n",
            "Stacking Classifier - Accuracy: 0.9047619047619048, F1: 0.8333333333333333, AUC: 0.9489795918367347\n",
            "\n",
            "Decision Tree - Average Accuracy: 0.9190476190476191, Average F1: 0.8741498331932146, Average AUC: 0.9067497158014401\n",
            "\n",
            "Rg - Average Accuracy: 0.9428571428571428, Average F1: 0.9050940170940172, Average AUC: 0.9664358117252206\n",
            "\n",
            "SVM - Average Accuracy: 0.8857142857142858, Average F1: 0.7891971892841458, Average AUC: 0.9524001245060358\n",
            "\n",
            "KNN - Average Accuracy: 0.8595238095238097, Average F1: 0.7709359947620816, Average AUC: 0.898960306934445\n",
            "\n",
            "Logistic Regression - Average Accuracy: 0.8904761904761905, Average F1: 0.7942886388973345, Average AUC: 0.9626955556758514\n",
            "\n",
            "Linear Discriminant Analysis - Average Accuracy: 0.9023809523809524, Average F1: 0.8239838518968954, Average AUC: 0.9629405077680939\n",
            "\n",
            "GPC - Average Accuracy: 0.880952380952381, Average F1: 0.7762128306476132, Average AUC: 0.9542264385860445\n",
            "\n",
            "Stacking Classifier - Average Accuracy: 0.930952380952381, Average F1: 0.8893967973278318, Average AUC: 0.9760522113354625\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "// 1\n",
        "def entropy(target_col):\n",
        "  elements, counts = np.unique(target_col, return_counts=True)\n",
        "  entropy = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
        "  return entropy\n",
        "\n",
        "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
        "  total_entropy = entropy(data[target_name])\n",
        "  vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "  Weighted_Entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "  Information_Gain = total_entropy - Weighted_Entropy\n",
        "  return Information_Gain\n",
        "\n",
        "def split_info(data, split_attribute_name):\n",
        "  vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "  split_info = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(vals))])\n",
        "  return split_info\n",
        "\n",
        "def gain_ratio(data, split_attribute_name, target_name=\"class\"):\n",
        "  gain = InfoGain(data, split_attribute_name, target_name)\n",
        "  split = split_info(data, split_attribute_name)\n",
        "  gain_ratio = gain / split\n",
        "  return gain_ratio\n",
        "\n",
        "def select_features_by_gain_ratio(data, target_name=\"class\"):\n",
        "  features = data.columns.drop(target_name)\n",
        "  scores = {feature: gain_ratio(data, feature, target_name) for feature in features}\n",
        "  return scores\n",
        "\n",
        "def select_features_by_mutual_info(data, target_name=\"class\"):\n",
        "  features = data.columns.drop(target_name)\n",
        "  scores = dict(zip(features, mutual_info_classif(data[features], data[target_name])))\n",
        "  return scores\n",
        "\n",
        "#عریف تابع برای محاسبه ضریب همبستگی Spearman\n",
        "def spearman_corr(x, y):\n",
        "  x_rank = x.rank()\n",
        "  y_rank = y.rank()\n",
        "  return ((x_rank - x_rank.mean()) * (y_rank - y_rank.mean())).mean() / (x_rank.std() * y_rank.std())\n",
        "\n",
        "def select_features_by_spearman(data, target_name=\"class\"):\n",
        "  features = data.columns.drop(target_name)\n",
        "  scores = {feature: spearman_corr(data[feature], data[target_name]) for feature in features}\n",
        "  return scores\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features=10):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "#SFS با Logistic Regression\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features=10):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "  if np.sum(solution) == 0:\n",
        "    return 0\n",
        "\n",
        "\n",
        "  X_selected = X[:, solution == 1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "  model = LogisticRegression(max_iter=1000)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "def initialize_population(pop_size, num_features):\n",
        "  return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "#magority feature selection\n",
        "def majority_voting_features(rfe_scores, sfs_scores, genetic_scores):\n",
        "    all_features = set(rfe_scores.keys()).union(sfs_scores.keys()).union(genetic_scores.keys())\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "\n",
        "    for feature in rfe_scores:\n",
        "        if rfe_scores[feature]:\n",
        "            feature_count[feature] += 1\n",
        "    for feature in sfs_scores:\n",
        "        if sfs_scores[feature]:\n",
        "            feature_count[feature] += 1\n",
        "    for feature in genetic_scores:\n",
        "        if genetic_scores[feature]:\n",
        "            feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= 2]\n",
        "    return majority_features\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "\n",
        "\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "\n",
        "gain_ratio_scores = select_features_by_gain_ratio(data_normalized, target_name='is_long_parameters_list')\n",
        "\n",
        "mutual_info_scores = select_features_by_mutual_info(data_normalized, target_name='is_long_parameters_list')\n",
        "\n",
        "\n",
        "spearman_scores = select_features_by_spearman(data_normalized, target_name='is_long_parameters_list')\n",
        "rfe_scores = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features=25)\n",
        "sfs_scores = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list', n_features=25)\n",
        "genetic_scores = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "intersection_features = set(rfe_scores.keys()).intersection(set(sfs_scores.keys()), set(genetic_scores.keys()))\n",
        "majority_voting_features = majority_voting_features(rfe_scores, sfs_scores, genetic_scores)\n",
        "\n",
        "\n",
        "common_features = set(gain_ratio_scores.keys()).union(set(mutual_info_scores.keys())).union(set(spearman_scores.keys())).union(set(rfe_scores.keys())).union(set(sfs_scores.keys())).union(set(genetic_scores.keys()))\n",
        "\n",
        "\n",
        "base_classifiers = [\n",
        "(\"Decision Tree\", DecisionTreeClassifier()),\n",
        "('Rg',RandomForestClassifier()),\n",
        "(\"SVM\", SVC(probability=True)),\n",
        "(\"KNN\", KNeighborsClassifier()),\n",
        "('Logistic Regression', LogisticRegression()),\n",
        "(\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "(\"GPC\",GaussianProcessClassifier()),\n",
        "\n",
        "]\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=MLPClassifier(max_iter=2000))\n",
        "\n",
        "\n",
        "feature_sets = {\n",
        "\"All Features\": X_normalized.columns.tolist(), # استفاده از تمامی ویژگی‌ها\n",
        "\"Gain Ratio Features\": [feature for feature, score in gain_ratio_scores.items() if score > 0.04],\n",
        "\"Mutual Information Features\": [feature for feature, score in mutual_info_scores.items() if score > 0.04],\n",
        "\"Spearman Correlation Features\": [feature for feature, score in spearman_scores.items() if score > 0.04],\n",
        "\"RFE Features\": [feature for feature, selected in rfe_scores.items() if selected],\n",
        "\"SFS Features\": [feature for feature, selected in sfs_scores.items() if selected],\n",
        "\"genetic Features\": [feature for feature, selected in genetic_scores.items() if selected],\n",
        "\"Union of All Features\": list(common_features),\n",
        "\"Instertion Set\":list(intersection_features),\n",
        "\"magority set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "\n",
        "n_splits = 7\n",
        "\n",
        "# StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# تعریف تعداد تکرار (fold)\n",
        "n_splits = 10\n",
        "\n",
        "# تعریف StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# اعمال الگوریتم‌های طبقه‌بندی بر روی مجموعه‌های ویژگی\n",
        "for feature_set_name, feature_set in feature_sets.items():\n",
        "    X_selected = X_normalized[feature_set]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        print(f\"\\n{feature_set_name}:\")\n",
        "        for base_classifier_name, base_classifier in base_classifiers:\n",
        "            base_classifier.fit(X_train, y_train)\n",
        "            y_pred = base_classifier.predict(X_test)\n",
        "            y_pred_prob = base_classifier.predict_proba(X_test)[:, 1]\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "            auc = roc_auc_score(y_test, y_pred_prob)\n",
        "            metrics[base_classifier_name][\"accuracy\"].append(accuracy)\n",
        "            metrics[base_classifier_name][\"f1\"].append(f1)\n",
        "            metrics[base_classifier_name][\"auc\"].append(auc)\n",
        "            print(f\"{base_classifier_name} - Accuracy: {accuracy}, F1: {f1}, AUC: {auc}\")\n",
        "\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_pred_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_pred_prob)\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy)\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1)\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(auc)\n",
        "        print(f\"Stacking Classifier - Accuracy: {accuracy}, F1: {f1}, AUC: {auc}\")\n",
        "\n",
        "    for classifier_name, metric_values in metrics.items():\n",
        "        avg_accuracy = np.mean(metric_values[\"accuracy\"])\n",
        "        avg_f1 = np.mean(metric_values[\"f1\"])\n",
        "        avg_auc = np.mean(metric_values[\"auc\"])\n",
        "        print(f\"\\n{classifier_name} - Average Accuracy: {avg_accuracy}, Average F1: {avg_f1}, Average AUC: {avg_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntNMd8LJA42g",
        "outputId": "e16e236f-15bc-41fa-c583-c9b398020dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results for All Features:\n",
            "Decision Tree: Accuracy=0.9190, F1=0.8756, AUC=0.9083\n",
            "Random Forest: Accuracy=0.9452, F1=0.9098, AUC=0.9728\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9067\n",
            "KNN: Accuracy=0.7667, F1=0.6352, AUC=0.7284\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8887, AUC=0.9717\n",
            "----------------------------------------\n",
            "Results for Gain Ratio Features:\n",
            "Decision Tree: Accuracy=0.8976, F1=0.8440, AUC=0.8852\n",
            "Random Forest: Accuracy=0.9381, F1=0.8976, AUC=0.9653\n",
            "SVM: Accuracy=0.8286, F1=0.6491, AUC=0.9261\n",
            "KNN: Accuracy=0.8214, F1=0.7114, AUC=0.7836\n",
            "Logistic Regression: Accuracy=0.8810, F1=0.7779, AUC=0.9509\n",
            "Linear Discriminant Analysis: Accuracy=0.8881, F1=0.7934, AUC=0.9524\n",
            "Gaussian Process: Accuracy=0.8571, F1=0.7242, AUC=0.9394\n",
            "Stacking Classifier: Accuracy=0.9190, F1=0.8737, AUC=0.9608\n",
            "----------------------------------------\n",
            "Results for Mutual Information Features:\n",
            "Decision Tree: Accuracy=0.9024, F1=0.8526, AUC=0.8904\n",
            "Random Forest: Accuracy=0.9381, F1=0.8965, AUC=0.9679\n",
            "SVM: Accuracy=0.8619, F1=0.7357, AUC=0.9308\n",
            "KNN: Accuracy=0.8405, F1=0.7460, AUC=0.8104\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7856, AUC=0.9596\n",
            "Linear Discriminant Analysis: Accuracy=0.9048, F1=0.8239, AUC=0.9582\n",
            "Gaussian Process: Accuracy=0.8738, F1=0.7611, AUC=0.9447\n",
            "Stacking Classifier: Accuracy=0.9286, F1=0.8909, AUC=0.9706\n",
            "----------------------------------------\n",
            "Results for Spearman Correlation Features:\n",
            "Decision Tree: Accuracy=0.9405, F1=0.9082, AUC=0.9337\n",
            "Random Forest: Accuracy=0.9357, F1=0.8922, AUC=0.9679\n",
            "SVM: Accuracy=0.8238, F1=0.6314, AUC=0.9199\n",
            "KNN: Accuracy=0.7833, F1=0.6579, AUC=0.7462\n",
            "Logistic Regression: Accuracy=0.8786, F1=0.7737, AUC=0.9625\n",
            "Linear Discriminant Analysis: Accuracy=0.8881, F1=0.7989, AUC=0.9565\n",
            "Gaussian Process: Accuracy=0.8476, F1=0.6968, AUC=0.9337\n",
            "Stacking Classifier: Accuracy=0.9238, F1=0.8805, AUC=0.9640\n",
            "----------------------------------------\n",
            "Results for RFE Features:\n",
            "Decision Tree: Accuracy=0.9048, F1=0.8604, AUC=0.8994\n",
            "Random Forest: Accuracy=0.9405, F1=0.9019, AUC=0.9720\n",
            "SVM: Accuracy=0.8357, F1=0.6592, AUC=0.9438\n",
            "KNN: Accuracy=0.8429, F1=0.7566, AUC=0.8185\n",
            "Logistic Regression: Accuracy=0.8857, F1=0.7872, AUC=0.9697\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8287, AUC=0.9665\n",
            "Gaussian Process: Accuracy=0.8762, F1=0.7660, AUC=0.9569\n",
            "Stacking Classifier: Accuracy=0.9190, F1=0.8769, AUC=0.9771\n",
            "----------------------------------------\n",
            "Results for SFS Features:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8751, AUC=0.9104\n",
            "Random Forest: Accuracy=0.9310, F1=0.8838, AUC=0.9693\n",
            "SVM: Accuracy=0.8167, F1=0.6005, AUC=0.9304\n",
            "KNN: Accuracy=0.8452, F1=0.7689, AUC=0.8275\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7885, AUC=0.9650\n",
            "Linear Discriminant Analysis: Accuracy=0.9071, F1=0.8282, AUC=0.9611\n",
            "Gaussian Process: Accuracy=0.8714, F1=0.7522, AUC=0.9553\n",
            "Stacking Classifier: Accuracy=0.9310, F1=0.8881, AUC=0.9618\n",
            "----------------------------------------\n",
            "Results for Genetic Features:\n",
            "Decision Tree: Accuracy=0.9429, F1=0.9131, AUC=0.9389\n",
            "Random Forest: Accuracy=0.9452, F1=0.9108, AUC=0.9820\n",
            "SVM: Accuracy=0.8214, F1=0.6117, AUC=0.9307\n",
            "KNN: Accuracy=0.8452, F1=0.7564, AUC=0.8174\n",
            "Logistic Regression: Accuracy=0.8833, F1=0.7770, AUC=0.9689\n",
            "Linear Discriminant Analysis: Accuracy=0.8976, F1=0.8116, AUC=0.9673\n",
            "Gaussian Process: Accuracy=0.8595, F1=0.7234, AUC=0.9508\n",
            "Stacking Classifier: Accuracy=0.9310, F1=0.8904, AUC=0.9686\n",
            "----------------------------------------\n",
            "Results for Union of All Features:\n",
            "Decision Tree: Accuracy=0.9143, F1=0.8692, AUC=0.9047\n",
            "Random Forest: Accuracy=0.9500, F1=0.9159, AUC=0.9706\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7667, F1=0.6352, AUC=0.7284\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9262, F1=0.8870, AUC=0.9732\n",
            "----------------------------------------\n",
            "Results for Intersection Set:\n",
            "Decision Tree: Accuracy=0.9167, F1=0.8718, AUC=0.9064\n",
            "Random Forest: Accuracy=0.9429, F1=0.9055, AUC=0.9714\n",
            "SVM: Accuracy=0.7976, F1=0.5445, AUC=0.9068\n",
            "KNN: Accuracy=0.7667, F1=0.6352, AUC=0.7284\n",
            "Logistic Regression: Accuracy=0.8690, F1=0.7559, AUC=0.9602\n",
            "Linear Discriminant Analysis: Accuracy=0.8786, F1=0.7871, AUC=0.9513\n",
            "Gaussian Process: Accuracy=0.8190, F1=0.6312, AUC=0.9171\n",
            "Stacking Classifier: Accuracy=0.9357, F1=0.8979, AUC=0.9732\n",
            "----------------------------------------\n",
            "Results for Majority Set:\n",
            "Decision Tree: Accuracy=0.9238, F1=0.8856, AUC=0.9160\n",
            "Random Forest: Accuracy=0.9357, F1=0.8955, AUC=0.9763\n",
            "SVM: Accuracy=0.8262, F1=0.6257, AUC=0.9294\n",
            "KNN: Accuracy=0.8643, F1=0.7852, AUC=0.8399\n",
            "Logistic Regression: Accuracy=0.8905, F1=0.7921, AUC=0.9697\n",
            "Linear Discriminant Analysis: Accuracy=0.9048, F1=0.8248, AUC=0.9692\n",
            "Gaussian Process: Accuracy=0.8762, F1=0.7638, AUC=0.9618\n",
            "Stacking Classifier: Accuracy=0.9119, F1=0.8637, AUC=0.9602\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def entropy(target_col):\n",
        "  elements, counts = np.unique(target_col, return_counts=True)\n",
        "  entropy = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
        "  return entropy\n",
        "\n",
        "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
        "  total_entropy = entropy(data[target_name])\n",
        "  vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "  Weighted_Entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "  Information_Gain = total_entropy - Weighted_Entropy\n",
        "  return Information_Gain\n",
        "\n",
        "def split_info(data, split_attribute_name):\n",
        "  vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "  split_info = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(vals))])\n",
        "  return split_info\n",
        "\n",
        "def gain_ratio(data, split_attribute_name, target_name=\"class\"):\n",
        "  gain = InfoGain(data, split_attribute_name, target_name)\n",
        "  split = split_info(data, split_attribute_name)\n",
        "  gain_ratio = gain / split\n",
        "  return gain_ratio\n",
        "\n",
        "def select_features_by_gain_ratio(data, target_name=\"class\"):\n",
        "  features = data.columns.drop(target_name)\n",
        "  scores = {feature: gain_ratio(data, feature, target_name) for feature in features}\n",
        "  return scores\n",
        "\n",
        "def select_features_by_mutual_info(data, target_name=\"class\"):\n",
        "  features = data.columns.drop(target_name)\n",
        "  scores = dict(zip(features, mutual_info_classif(data[features], data[target_name])))\n",
        "  return scores\n",
        "\n",
        "#عریف تابع برای محاسبه ضریب همبستگی Spearman\n",
        "def spearman_corr(x, y):\n",
        "  x_rank = x.rank()\n",
        "  y_rank = y.rank()\n",
        "  return ((x_rank - x_rank.mean()) * (y_rank - y_rank.mean())).mean() / (x_rank.std() * y_rank.std())\n",
        "\n",
        "def select_features_by_spearman(data, target_name=\"class\"):\n",
        "  features = data.columns.drop(target_name)\n",
        "  scores = {feature: spearman_corr(data[feature], data[target_name]) for feature in features}\n",
        "  return scores\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "#SFS با Logistic Regression\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features=20):\n",
        "  X = data.drop(target_name, axis=1)\n",
        "  y = data[target_name]\n",
        "  estimator = LogisticRegression(solver='liblinear')\n",
        "  selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "  selector = selector.fit(X, y)\n",
        "  return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "  if np.sum(solution) == 0:\n",
        "    return 0\n",
        "\n",
        "\n",
        "  X_selected = X[:, solution == 1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "  model = LogisticRegression(max_iter=1000)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "def initialize_population(pop_size, num_features):\n",
        "  return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores):\n",
        "    parents = []\n",
        "    for _ in range(len(population)):\n",
        "        i, j = np.random.choice(len(population), 2, replace=False)\n",
        "        if fitness_scores[i] > fitness_scores[j]:\n",
        "            parents.append(population[i])\n",
        "        else:\n",
        "            parents.append(population[j])\n",
        "    return np.array(parents)\n",
        "\n",
        "def crossover(parents):\n",
        "    offspring = []\n",
        "    for i in range(0, len(parents), 2):\n",
        "        if i + 1 < len(parents):\n",
        "            crossover_point = random.randint(1, parents.shape[1] - 1)\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "            child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "            offspring.extend([child1, child2])\n",
        "    return np.array(offspring)\n",
        "\n",
        "def mutate(offspring, mutation_rate=0.01):\n",
        "    for individual in offspring:\n",
        "        for gene in range(len(individual)):\n",
        "            if random.random() < mutation_rate:\n",
        "                individual[gene] = 1 - individual[gene]\n",
        "    return offspring\n",
        "\n",
        "\n",
        "def select_features_by_custom_genetic(data, target_name='is_long_parameters_list', pop_size=20, n_generations=50, mutation_rate=0.01):\n",
        "    X = data.drop(target_name, axis=1).values\n",
        "    y = data[target_name].values\n",
        "\n",
        "    population = initialize_population(pop_size, X.shape[1])\n",
        "\n",
        "    for generation in range(n_generations):\n",
        "        fitness_scores = np.array([fitness(individual, X, y) for individual in population])\n",
        "        parents = select_parents(population, fitness_scores)\n",
        "        offspring = crossover(parents)\n",
        "        population = mutate(offspring, mutation_rate)\n",
        "\n",
        "    best_solution = population[np.argmax(fitness_scores)]\n",
        "    return dict(zip(data.drop(target_name, axis=1).columns, best_solution))\n",
        "\n",
        "#magority feature selection\n",
        "def majority_voting_features(rfe_scores, sfs_scores, genetic_scores):\n",
        "    all_features = set(rfe_scores.keys()).union(sfs_scores.keys()).union(genetic_scores.keys())\n",
        "    feature_count = {feature: 0 for feature in all_features}\n",
        "\n",
        "    for feature in rfe_scores:\n",
        "        if rfe_scores[feature]:\n",
        "            feature_count[feature] += 1\n",
        "    for feature in sfs_scores:\n",
        "        if sfs_scores[feature]:\n",
        "            feature_count[feature] += 1\n",
        "    for feature in genetic_scores:\n",
        "        if genetic_scores[feature]:\n",
        "            feature_count[feature] += 1\n",
        "\n",
        "    majority_features = [feature for feature, count in feature_count.items() if count >= 2]\n",
        "    return majority_features\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "X = data.drop('is_long_parameters_list', axis=1)\n",
        "y = data['is_long_parameters_list']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_normalized.fillna(X_normalized.mean(), inplace=True)\n",
        "data_normalized = pd.concat([X_normalized, y], axis=1)\n",
        "\n",
        "gain_ratio_scores = select_features_by_gain_ratio(data_normalized, target_name='is_long_parameters_list')\n",
        "mutual_info_scores = select_features_by_mutual_info(data_normalized, target_name='is_long_parameters_list')\n",
        "spearman_scores = select_features_by_spearman(data_normalized, target_name='is_long_parameters_list')\n",
        "rfe_scores = select_features_by_rfe(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "sfs_scores = select_features_by_sfs(data_normalized, target_name='is_long_parameters_list', n_features=20)\n",
        "genetic_scores = select_features_by_custom_genetic(data_normalized, target_name='is_long_parameters_list')\n",
        "intersection_features = set(rfe_scores.keys()).intersection(set(sfs_scores.keys()), set(genetic_scores.keys()))\n",
        "majority_voting_features = majority_voting_features(rfe_scores, sfs_scores, genetic_scores)\n",
        "\n",
        "common_features = set(gain_ratio_scores.keys()).union(set(mutual_info_scores.keys())).union(set(spearman_scores.keys())).union(set(rfe_scores.keys())).union(set(sfs_scores.keys())).union(set(genetic_scores.keys()))\n",
        "\n",
        "base_classifiers = [\n",
        "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    (\"SVM\", SVC(C=0.1, gamma='scale', kernel='poly', degree=3,probability=True)),\n",
        "    (\"KNN\", KNeighborsClassifier(n_neighbors=1)),\n",
        "    ('Logistic Regression', LogisticRegression()),\n",
        "    (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n",
        "    (\"Gaussian Process\", GaussianProcessClassifier())\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=MLPClassifier(max_iter=2000))\n",
        "\n",
        "feature_sets = {\n",
        "    \"All Features\": X_normalized.columns.tolist(),\n",
        "    \"Gain Ratio Features\": [feature for feature, score in gain_ratio_scores.items() if score > 0.04],\n",
        "    \"Mutual Information Features\": [feature for feature, score in mutual_info_scores.items() if score > 0.04],\n",
        "    \"Spearman Correlation Features\": [feature for feature, score in spearman_scores.items() if score > 0.04],\n",
        "    \"RFE Features\": [feature for feature, selected in rfe_scores.items() if selected],\n",
        "    \"SFS Features\": [feature for feature, selected in sfs_scores.items() if selected],\n",
        "    \"Genetic Features\": [feature for feature, selected in genetic_scores.items() if selected],\n",
        "    \"Union of All Features\": list(common_features),\n",
        "    \"Intersection Set\": list(intersection_features),\n",
        "    \"Majority Set\": list(majority_voting_features)\n",
        "}\n",
        "\n",
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for feature_set_name, feature_set in feature_sets.items():\n",
        "    X_selected = X_normalized[feature_set]\n",
        "    metrics = {name: {\"accuracy\": [], \"f1\": [], \"auc\": []} for name, _ in base_classifiers}\n",
        "    metrics[\"Stacking Classifier\"] = {\"accuracy\": [], \"f1\": [], \"auc\": []}\n",
        "\n",
        "    for train_index, test_index in skf.split(X_selected, y):\n",
        "        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for name, clf in base_classifiers:\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC\n",
        "\n",
        "            metrics[name][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "            metrics[name][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "            metrics[name][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "        # Stacking Classifier\n",
        "        stacking_classifier.fit(X_train, y_train)\n",
        "        y_pred = stacking_classifier.predict(X_test)\n",
        "        y_prob = stacking_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        metrics[\"Stacking Classifier\"][\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"f1\"].append(f1_score(y_test, y_pred))\n",
        "        metrics[\"Stacking Classifier\"][\"auc\"].append(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "    print(f\"Results for {feature_set_name}:\")\n",
        "\n",
        "    for name, scores in metrics.items():\n",
        "        avg_accuracy = np.mean(scores[\"accuracy\"])\n",
        "        avg_f1 = np.mean(scores[\"f1\"])\n",
        "        avg_auc = np.mean(scores[\"auc\"])\n",
        "        print(f\"{name}: Accuracy={avg_accuracy:.4f}, F1={avg_f1:.4f}, AUC={avg_auc:.4f}\")\n",
        "\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g72dBTEKIPgN",
        "outputId": "92eb8149-d7b8-444b-b466-7f3558c9d3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Gain Ratio Selected Features:\n",
            "['NOP_method', 'CC_method', 'ATFD_method', 'FDP_method', 'CM_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'NMCS_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'LAA_method', 'FANOUT_method', 'CFNAMM_method', 'ATLD_method', 'CINT_method', 'MeMCL_method', 'CDISP_method', 'NOII_type', 'NOAM_type', 'NOCS_type', 'NOM_type', 'NMO_type', 'ATFD_type', 'FANOUT_type', 'NOMNAMM_type', 'NOA_type', 'NIM_type', 'DIT_type', 'LOC_type', 'LOCNAMM_type', 'CFNAMM_type', 'TCC_type', 'NOPA_type', 'CBO_type', 'RFC_type', 'NOC_type', 'WMC_type', 'LCOM5_type', 'WOC_type', 'WMCNAMM_type', 'AMW_type', 'AMWNAMM_type', 'NOCS_package', 'NOMNAMM_package', 'NOI_package', 'LOC_package', 'NOM_package', 'NOPK_project', 'NOCS_project', 'NOI_project', 'NOM_project', 'NOMNAMM_project', 'LOC_project']\n",
            "Mutual Information Selected Features:\n",
            "['NOP_method', 'CC_method', 'ATFD_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'LAA_method', 'FANOUT_method', 'CFNAMM_method', 'ATLD_method', 'CINT_method', 'CDISP_method', 'NOII_type', 'NOAM_type', 'NOM_type', 'NMO_type', 'ATFD_type', 'FANOUT_type', 'NOMNAMM_type', 'NOA_type', 'NIM_type', 'DIT_type', 'LOC_type', 'LOCNAMM_type', 'CFNAMM_type', 'TCC_type', 'NOPA_type', 'CBO_type', 'RFC_type', 'NOC_type', 'WMC_type', 'LCOM5_type', 'WOC_type', 'WMCNAMM_type', 'AMW_type', 'AMWNAMM_type', 'NOCS_package', 'NOMNAMM_package', 'NOI_package', 'LOC_package', 'NOM_package', 'NOPK_project', 'NOCS_project', 'NOM_project', 'NOMNAMM_project', 'LOC_project']\n",
            "Spearman Correlation Selected Features:\n",
            "['NOP_method', 'CC_method', 'ATFD_method', 'FDP_method', 'CM_method', 'MAXNESTING_method', 'LOC_method', 'CYCLO_method', 'NMCS_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'LAA_method', 'FANOUT_method', 'CFNAMM_method', 'ATLD_method', 'CINT_method', 'MeMCL_method', 'CDISP_method', 'NOII_type', 'NOAM_type', 'NOCS_type', 'NOM_type', 'NMO_type', 'ATFD_type', 'FANOUT_type', 'NOMNAMM_type', 'NOA_type', 'NIM_type', 'DIT_type', 'LOC_type', 'LOCNAMM_type', 'CFNAMM_type', 'TCC_type', 'NOPA_type', 'CBO_type', 'RFC_type', 'NOC_type', 'WMC_type', 'LCOM5_type', 'WOC_type', 'WMCNAMM_type', 'AMW_type', 'AMWNAMM_type', 'NOCS_package', 'NOMNAMM_package', 'NOI_package', 'LOC_package', 'NOM_package', 'NOPK_project', 'NOCS_project', 'NOI_project', 'NOM_project', 'NOMNAMM_project', 'LOC_project']\n",
            "RFE Selected Features:\n",
            "['NOP_method', 'MAXNESTING_method', 'NOLV_method', 'NOAV_method', 'LAA_method', 'FANOUT_method', 'ATLD_method', 'CINT_method', 'ATFD_type', 'NOA_type']\n",
            "SFS Selected Features:\n",
            "['NOP_method', 'CC_method', 'ATFD_method', 'CM_method', 'NMCS_method', 'MaMCL_method', 'NOAV_method', 'CDISP_method', 'LOC_package', 'NOI_project']\n",
            "Genetic Algorithm Selected Features:\n",
            "['NOP_method', 'FDP_method', 'MAXNESTING_method', 'NOLV_method', 'MaMCL_method', 'NOAV_method', 'LAA_method', 'FANOUT_method', 'CFNAMM_method', 'MeMCL_method', 'NOII_type', 'NOCS_type', 'NMO_type', 'FANOUT_type', 'NIM_type', 'LOC_type', 'LOCNAMM_type', 'TCC_type', 'NOPA_type', 'CBO_type', 'NOC_type', 'LCOM5_type', 'AMWNAMM_type', 'NOMNAMM_package', 'LOC_package', 'NOPK_project', 'NOM_project']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def entropy(target_col):\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    entropy = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    Weighted_Entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "\n",
        "def split_info(data, split_attribute_name):\n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    split_info = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(vals))])\n",
        "    return split_info\n",
        "\n",
        "def gain_ratio(data, split_attribute_name, target_name=\"class\"):\n",
        "    gain = InfoGain(data, split_attribute_name, target_name)\n",
        "    split = split_info(data, split_attribute_name)\n",
        "    gain_ratio = gain / split\n",
        "    return gain_ratio\n",
        "\n",
        "def select_features_by_gain_ratio(data, target_name=\"class\"):\n",
        "    features = data.columns.drop(target_name)\n",
        "    scores = {feature: gain_ratio(data, feature, target_name) for feature in features}\n",
        "    return scores\n",
        "\n",
        "def select_features_by_mutual_info(data, target_name=\"class\"):\n",
        "    features = data.columns.drop(target_name)\n",
        "    scores = dict(zip(features, mutual_info_classif(data[features], data[target_name])))\n",
        "    return scores\n",
        "\n",
        "def spearman_corr(x, y):\n",
        "    x_rank = x.rank()\n",
        "    y_rank = y.rank()\n",
        "    return ((x_rank - x_rank.mean()) * (y_rank - y_rank.mean())).mean() / (x_rank.std() * y_rank.std())\n",
        "\n",
        "def select_features_by_spearman(data, target_name=\"class\"):\n",
        "    features = data.columns.drop(target_name)\n",
        "    scores = {feature: spearman_corr(data[feature], data[target_name]) for feature in features}\n",
        "    return scores\n",
        "\n",
        "def select_features_by_rfe(data, target_name='is_long_parameters_list', n_features=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    estimator = LogisticRegression(solver='liblinear')\n",
        "    selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def select_features_by_sfs(data, target_name='is_long_parameters_list', n_features=10):\n",
        "    X = data.drop(target_name, axis=1)\n",
        "    y = data[target_name]\n",
        "    estimator = LogisticRegression(solver='liblinear')\n",
        "    selector = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "    selector = selector.fit(X, y)\n",
        "    return dict(zip(X.columns, selector.support_))\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size,num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores, num_parents):\n",
        "    parents = np.zeros((num_parents, population.shape[1]))\n",
        "    for parent_num in range(num_parents):\n",
        "        max_fitness_idx = np.argmax(fitness_scores)\n",
        "        parents[parent_num, :] = population[max_fitness_idx, :]\n",
        "        fitness_scores[max_fitness_idx] = -np.inf\n",
        "    return parents\n",
        "\n",
        "def crossover(parents, offspring_size):\n",
        "    offspring = np.zeros(offspring_size)\n",
        "    crossover_point = np.uint8(offspring_size[1] / 2)\n",
        "    for k in range(offspring_size[0]):\n",
        "        parent1_idx = k % parents.shape[0]\n",
        "        parent2_idx = (k + 1) % parents.shape[0]\n",
        "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
        "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
        "    return offspring\n",
        "\n",
        "def mutation(offspring_crossover, mutation_rate):\n",
        "    for idx in range(offspring_crossover.shape[0]):\n",
        "        for gene_idx in range(offspring_crossover.shape[1]):\n",
        "            if random.random() < mutation_rate:\n",
        "                offspring_crossover[idx, gene_idx] = 1 - offspring_crossover[idx, gene_idx]\n",
        "    return offspring_crossover\n",
        "\n",
        "def genetic_algorithm(X, y, pop_size, num_generations, num_parents_mating, mutation_rate):\n",
        "    num_features = X.shape[1]\n",
        "    population = initialize_population(pop_size, num_features)\n",
        "    for generation in range(num_generations):\n",
        "        fitness_scores = np.array([fitness(ind, X, y) for ind in population])\n",
        "        parents = select_parents(population, fitness_scores, num_parents_mating)\n",
        "        offspring_crossover = crossover(parents, (pop_size - parents.shape[0], num_features))\n",
        "        offspring_mutation = mutation(offspring_crossover, mutation_rate)\n",
        "        population[0:parents.shape[0], :] = parents\n",
        "        population[parents.shape[0]:, :] = offspring_mutation\n",
        "    fitness_scores = np.array([fitness(ind, X, y) for ind in population])\n",
        "    best_solution_idx = np.argmax(fitness_scores)\n",
        "    best_solution = population[best_solution_idx]\n",
        "    return best_solution\n",
        "\n",
        "# Load data\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# Preprocess data\n",
        "scaler = MinMaxScaler()\n",
        "data[data.columns[:-1]] = scaler.fit_transform(data[data.columns[:-1]])\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop('is_long_parameters_list', axis=1).values\n",
        "y = data['is_long_parameters_list'].values\n",
        "\n",
        "# Select features using different algorithms\n",
        "gain_ratio_scores = select_features_by_gain_ratio(data, target_name='is_long_parameters_list')\n",
        "mutual_info_scores = select_features_by_mutual_info(data, target_name='is_long_parameters_list')\n",
        "spearman_scores = select_features_by_spearman(data, target_name='is_long_parameters_list')\n",
        "rfe_scores = select_features_by_rfe(data, target_name='is_long_parameters_list')\n",
        "sfs_scores = select_features_by_sfs(data, target_name='is_long_parameters_list')\n",
        "\n",
        "# Display selected features\n",
        "print(\"Gain Ratio Selected Features:\")\n",
        "print([feature for feature, selected in gain_ratio_scores.items() if selected])\n",
        "\n",
        "print(\"Mutual Information Selected Features:\")\n",
        "print([feature for feature, selected in mutual_info_scores.items() if selected])\n",
        "\n",
        "print(\"Spearman Correlation Selected Features:\")\n",
        "print([feature for feature, selected in spearman_scores.items() if selected])\n",
        "\n",
        "print(\"RFE Selected Features:\")\n",
        "print([feature for feature, selected in rfe_scores.items() if selected])\n",
        "\n",
        "print(\"SFS Selected Features:\")\n",
        "print([feature for feature, selected in sfs_scores.items() if selected])\n",
        "\n",
        "# Genetic Algorithm for feature selection\n",
        "best_solution = genetic_algorithm(X, y, pop_size=20, num_generations=50, num_parents_mating=10, mutation_rate=0.1)\n",
        "selected_features = [feature for feature, selected in zip(data.columns[:-1], best_solution) if selected]\n",
        "\n",
        "print(\"Genetic Algorithm Selected Features:\")\n",
        "print(selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TEAFVXX9Xtc",
        "outputId": "bc328b08-7e06-4f31-8ba1-0bdac386ce1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "n_features_to_select must be either 'auto', 'warn', None, an integer in [1, n_features - 1] representing the absolute number of features, or a float in (0, 1] representing a percentage of features to select. Got 55",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-de6d6509b9b9>\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mspearman_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_features_by_spearman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'is_long_parameters_list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m# Select features using SFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m \u001b[0msfs_selected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_features_by_sfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SFS Selected Features:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfs_selected_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-de6d6509b9b9>\u001b[0m in \u001b[0;36mselect_features_by_sfs\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0msfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_to_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mselected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_sequential.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_to_select\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_to_select\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_to_select_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_to_select\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_to_select\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_features_to_select must be either 'auto', 'warn', None, an integer in [1, n_features - 1] representing the absolute number of features, or a float in (0, 1] representing a percentage of features to select. Got 55"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "def entropy(target_col):\n",
        "    elements, counts = np.unique(target_col, return_counts=True)\n",
        "    entropy = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    Weighted_Entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "\n",
        "def split_info(data, split_attribute_name):\n",
        "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
        "    split_info = -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(vals))])\n",
        "    return split_info\n",
        "\n",
        "def gain_ratio(data, split_attribute_name, target_name=\"class\"):\n",
        "    gain = InfoGain(data, split_attribute_name, target_name)\n",
        "    split = split_info(data, split_attribute_name)\n",
        "    gain_ratio = gain / split\n",
        "    return gain_ratio\n",
        "\n",
        "def select_features_by_gain_ratio(data, target_name=\"class\"):\n",
        "    features = data.columns.drop(target_name)\n",
        "    scores = {feature: gain_ratio(data, feature, target_name) for feature in features}\n",
        "    return scores\n",
        "\n",
        "def select_features_by_mutual_info(data, target_name=\"class\"):\n",
        "    features = data.columns.drop(target_name)\n",
        "    scores = dict(zip(features, mutual_info_classif(data[features], data[target_name])))\n",
        "    return scores\n",
        "\n",
        "def spearman_corr(x, y):\n",
        "    x_rank = x.rank()\n",
        "    y_rank = y.rank()\n",
        "    return ((x_rank - x_rank.mean()) * (y_rank - y_rank.mean())).mean() / (x_rank.std() * y_rank.std())\n",
        "\n",
        "def select_features_by_spearman(data, target_name=\"class\"):\n",
        "    features = data.columns.drop(target_name)\n",
        "    scores = {feature: spearman_corr(data[feature], data[target_name]) for feature in features}\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "def fitness(solution, X, y):\n",
        "    if np.sum(solution) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, solution == 1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def select_parents(population, fitness_scores, num_parents):\n",
        "    parents = np.zeros((num_parents, population.shape[1]))\n",
        "    for parent_num in range(num_parents):\n",
        "        max_fitness_idx = np.argmax(fitness_scores)\n",
        "        parents[parent_num, :] = population[max_fitness_idx, :]\n",
        "        fitness_scores[max_fitness_idx] = -np.inf\n",
        "    return parents\n",
        "\n",
        "\n",
        "def crossover(parents, offspring_size):\n",
        "    offspring = np.zeros(offspring_size)\n",
        "    crossover_point = np.uint8(offspring_size[1] / 2)\n",
        "    for k in range(offspring_size[0]):\n",
        "        parent1_idx = k % parents.shape[0]\n",
        "        parent2_idx = (k + 1) % parents.shape[0]\n",
        "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
        "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
        "    return offspring\n",
        "\n",
        "def mutation(offspring_crossover, mutation_rate):\n",
        "    for idx in range(offspring_crossover.shape[0]):\n",
        "        for gene_idx in range(offspring_crossover.shape[1]):\n",
        "            if random.random() < mutation_rate:\n",
        "                offspring_crossover[idx, gene_idx] = 1 - offspring_crossover[idx, gene_idx]\n",
        "    return offspring_crossover\n",
        "\n",
        "def genetic_algorithm(X, y, pop_size, num_generations, num_parents_mating, mutation_rate):\n",
        "    num_features = X.shape[1]\n",
        "    population = initialize_population(pop_size, num_features)\n",
        "    for generation in range(num_generations):\n",
        "        fitness_scores = np.array([fitness(ind, X, y) for ind in population])\n",
        "        parents = select_parents(population, fitness_scores, num_parents_mating)\n",
        "        offspring_crossover = crossover(parents, (pop_size - parents.shape[0], num_features))\n",
        "        offspring_mutation = mutation(offspring_crossover, mutation_rate)\n",
        "        population[0:parents.shape[0], :] = parents\n",
        "        population[parents.shape[0]:, :] = offspring_mutation\n",
        "    fitness_scores = np.array([fitness(ind, X, y) for ind in population])\n",
        "    best_solution_idx = np.argmax(fitness_scores)\n",
        "    best_solution = population[best_solution_idx]\n",
        "    return best_solution\n",
        "\n",
        "def select_features_by_sfs(X, y):\n",
        "    best_score = 0\n",
        "    best_features = None\n",
        "    for n_features in range(1, X.shape[1] + 1):\n",
        "        estimator = LogisticRegression(solver='liblinear')\n",
        "        sfs = SFS(estimator, n_features_to_select=n_features, direction='forward')\n",
        "        sfs = sfs.fit(X, y)\n",
        "        selected_features = X[:, sfs.get_support()]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.3, random_state=42)\n",
        "        estimator.fit(X_train, y_train)\n",
        "        y_pred = estimator.predict(X_test)\n",
        "        score = accuracy_score(y_test, y_pred)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_features = sfs.get_support()\n",
        "    return best_features\n",
        "\n",
        "def select_features_by_rfe(X, y):\n",
        "    best_score = 0\n",
        "    best_features = None\n",
        "    for n_features in range(1, X.shape[1] + 1):\n",
        "        estimator = LogisticRegression(solver='liblinear')\n",
        "        rfe = RFE(estimator, n_features_to_select=n_features)\n",
        "        rfe = rfe.fit(X, y)\n",
        "        selected_features = X[:, rfe.get_support()]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.3, random_state=42)\n",
        "        estimator.fit(X_train, y_train)\n",
        "        y_pred = estimator.predict(X_test)\n",
        "        score = accuracy_score(y_test, y_pred)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_features = rfe.get_support()\n",
        "    return best_features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "drive.mount('/content/drive')\n",
        "Path = '/content/drive/My Drive/colab'\n",
        "data = pd.read_csv(Path + '/LongParameterList.csv')\n",
        "\n",
        "# Preprocess data\n",
        "scaler = MinMaxScaler()\n",
        "data[data.columns[:-1]] = scaler.fit_transform(data[data.columns[:-1]])\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop('is_long_parameters_list', axis=1).values\n",
        "y = data['is_long_parameters_list'].values\n",
        "\n",
        "# Select features using different algorithms\n",
        "gain_ratio_scores = select_features_by_gain_ratio(data, target_name='is_long_parameters_list')\n",
        "mutual_info_scores = select_features_by_mutual_info(data, target_name='is_long_parameters_list')\n",
        "spearman_scores = select_features_by_spearman(data, target_name='is_long_parameters_list')\n",
        "# Select features using SFS\n",
        "sfs_selected_features = select_features_by_sfs(X,y)\n",
        "print(\"SFS Selected Features:\")\n",
        "print([feature for feature, selected in zip(data.columns[:-1], sfs_selected_features) if selected])\n",
        "\n",
        "# Select features using RFE\n",
        "rfe_selected_features = select_features_by_rfe(X,y)\n",
        "print(\"RFE Selected Features:\")\n",
        "print([feature for feature, selected in zip(data.columns[:-1], rfe_selected_features) if selected])\n",
        "\n",
        "# Display selected features\n",
        "print(\"Gain Ratio Selected Features:\")\n",
        "print([feature for feature, selected in gain_ratio_scores.items() if selected])\n",
        "\n",
        "print(\"Mutual Information Selected Features:\")\n",
        "print([feature for feature, selected in mutual_info_scores.items() if selected])\n",
        "\n",
        "print(\"Spearman Correlation Selected Features:\")\n",
        "print([feature for feature, selected in spearman_scores.items() if selected])\n",
        "\n",
        "\n",
        "\n",
        "# Genetic Algorithm for feature selection\n",
        "#best_solution = genetic_algorithm(X, y, pop_size=20, num_generations=50, num_parents_mating=10, mutation_rate=0.1)\n",
        "#selected_features = [feature for feature, selected in zip(data.columns[:-1], best_solution) if selected]\n",
        "\n",
        "#print(\"Genetic Algorithm Selected Features:\")\n",
        "#print(selected_features)\n",
        "\n",
        "# Genetic Algorithm for feature selection\n",
        "best_solution = genetic_algorithm(X, y, pop_size=20, num_generations=50, num_parents_mating=10, mutation_rate=0.1)\n",
        "selected_features = [feature for feature, selected in zip(data.columns[:-1], best_solution) if selected]\n",
        "\n",
        "print(\"Genetic Algorithm Selected Features:\")\n",
        "print(selected_features)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQHmAwYqSlF6yMGjBx90kJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}